{"cells":[{"cell_type":"markdown","metadata":{"id":"SRjlA-8T-pOQ"},"source":["---\n","# Tutorial Objectives\n","\n","In this tutorial we will learn how to track objects in 3D on the KITTI dataset. We will build off of our object dector from part 1 and use each obejct detection to update the tracks.\n","\n","For more information a readme for the KITTI data can be found [here](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT), and a paper that details the data collection and coordinate systems can be found [here](http://www.cvlibs.net/publications/Geiger2013IJRR.pdf).\n","\n","\n","<br>"]},{"cell_type":"markdown","source":["## Get the data"],"metadata":{"id":"b8BfTqJ7_BXe"}},{"cell_type":"code","source":["## Kitti raw 데이터셋 다운로드 및 압축 해제\n","\n","!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip\n","!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_calib.zip\n","\n","!jar xf 2011_10_03_drive_0047_sync.zip\n","!jar xf 2011_10_03_calib.zip"],"metadata":{"id":"K69AK4e7_FOl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723526287573,"user_tz":-540,"elapsed":127348,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"a159033d-825c-4fa3-9337-8c6e33d49aa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-13 05:16:00--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip\n","Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.135.240, 52.219.169.181, 3.5.137.128, ...\n","Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.135.240|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3103291675 (2.9G) [application/zip]\n","Saving to: ‘2011_10_03_drive_0047_sync.zip’\n","\n","2011_10_03_drive_00 100%[===================>]   2.89G  35.9MB/s    in 1m 42s  \n","\n","2024-08-13 05:17:42 (28.9 MB/s) - ‘2011_10_03_drive_0047_sync.zip’ saved [3103291675/3103291675]\n","\n","--2024-08-13 05:17:42--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_calib.zip\n","Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.134.84, 52.219.170.161, 52.219.75.107, ...\n","Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.134.84|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4075 (4.0K) [application/zip]\n","Saving to: ‘2011_10_03_calib.zip’\n","\n","2011_10_03_calib.zi 100%[===================>]   3.98K  --.-KB/s    in 0s      \n","\n","2024-08-13 05:17:43 (211 MB/s) - ‘2011_10_03_calib.zip’ saved [4075/4075]\n","\n"]}]},{"cell_type":"markdown","source":["## Base Library Import"],"metadata":{"id":"lXqKyrfp_Jh5"}},{"cell_type":"code","source":["import os\n","from glob import glob\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams[\"figure.figsize\"] = (20, 10)"],"metadata":{"id":"lgM4JrRc_I-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Utility functions"],"metadata":{"id":"lMDnx_XP_ON7"}},{"cell_type":"code","source":["!wget https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py\n","from kitti_utils import *\n","\n","!wget https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_detection_utils.py\n","from kitti_detection_utils import *"],"metadata":{"id":"gs5psD0dRRJN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723526903998,"user_tz":-540,"elapsed":2022,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"76a9fa54-fb03-4bb3-f160-113ea9906053"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-13 05:28:21--  https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py\n","Resolving github.com (github.com)... 140.82.112.4\n","Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_utils.py [following]\n","--2024-08-13 05:28:22--  https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9759 (9.5K) [text/plain]\n","Saving to: ‘kitti_utils.py’\n","\n","\rkitti_utils.py        0%[                    ]       0  --.-KB/s               \rkitti_utils.py      100%[===================>]   9.53K  --.-KB/s    in 0s      \n","\n","2024-08-13 05:28:22 (62.0 MB/s) - ‘kitti_utils.py’ saved [9759/9759]\n","\n","--2024-08-13 05:28:23--  https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_detection_utils.py\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_detection_utils.py [following]\n","--2024-08-13 05:28:23--  https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_detection_utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5405 (5.3K) [text/plain]\n","Saving to: ‘kitti_detection_utils.py’\n","\n","kitti_detection_uti 100%[===================>]   5.28K  --.-KB/s    in 0s      \n","\n","2024-08-13 05:28:23 (56.6 MB/s) - ‘kitti_detection_utils.py’ saved [5405/5405]\n","\n"]}]},{"cell_type":"markdown","source":["## Get Data Paths"],"metadata":{"id":"OwE2dxKP_WAH"}},{"cell_type":"code","source":["DATA_PATH = r'2011_10_03/2011_10_03_drive_0047_sync'\n","\n","# get RGB camera data\n","left_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_02/data/*.png')))\n","right_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_03/data/*.png')))\n","\n","# get LiDAR data\n","bin_paths = sorted(glob(os.path.join(DATA_PATH, 'velodyne_points/data/*.bin')))\n","\n","# get GPS/IMU data\n","oxts_paths = sorted(glob(os.path.join(DATA_PATH, r'oxts/data**/*.txt')))\n","\n","print(f\"Number of left images: {len(left_image_paths)}\")\n","print(f\"Number of right images: {len(right_image_paths)}\")\n","print(f\"Number of LiDAR point clouds: {len(bin_paths)}\")\n","print(f\"Number of GPS/IMU frames: {len(oxts_paths)}\")"],"metadata":{"id":"XDH7Nhn5_Vdd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723526910964,"user_tz":-540,"elapsed":300,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"f4f80d2f-56a6-42e8-be22-9a3020c6f423"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of left images: 837\n","Number of right images: 837\n","Number of LiDAR point clouds: 837\n","Number of GPS/IMU frames: 837\n"]}]},{"cell_type":"markdown","source":["Since we will be using a Kalman Filter for 3D space, we will need to use the time stamp data. We will be fusing data from Camera 2 and the LiDAR, both of those time stamps are slightly misaligned, but they time delta is generally negligable for this purpose. We will opt to use the Camera 2 time stamps for out Kalman Filter."],"metadata":{"id":"PtNrfYsXKcYX"}},{"cell_type":"code","source":["get_total_seconds = lambda hms: hms[0]*60*60 + hms[1]*60 + hms[2]\n","\n","\n","def timestamps2seconds(timestamp_path):\n","    ''' Reads in timestamp path and returns total seconds (does not account for day rollover '''\n","    timestamps = pd.read_csv(timestamp_path,\n","                             header=None)\n","    timestamps = timestamps.squeeze('columns').astype(object) \\\n","                                          .apply(lambda x: x.split(' ')[1])\n","\n","    # Get Hours, Minutes, and Seconds\n","    hours = timestamps.apply(lambda x: x.split(':')[0]).astype(np.float64)\n","    minutes = timestamps.apply(lambda x: x.split(':')[1]).astype(np.float64)\n","    seconds = timestamps.apply(lambda x: x.split(':')[2]).astype(np.float64)\n","\n","    hms_vals = np.vstack((hours, minutes, seconds)).T\n","\n","    total_seconds = np.array(list(map(get_total_seconds, hms_vals)))\n","\n","    return total_seconds"],"metadata":{"id":"mSxlhb25KzJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cam2_total_seconds = timestamps2seconds(os.path.join(DATA_PATH, r'image_02/timestamps.txt'))\n","print(f'Number of time stamps: {len(cam2_total_seconds)}')"],"metadata":{"id":"2kdBQOAZKz3F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723527055429,"user_tz":-540,"elapsed":328,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"394dfb67-6db0-4e09-ebdf-06b813bd1d73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of time stamps: 837\n"]}]},{"cell_type":"markdown","source":["## Get Camera Transformation Matrices"],"metadata":{"id":"EYRASHTc_a-B"}},{"cell_type":"code","source":["with open('2011_10_03/calib_cam_to_cam.txt','r') as f:\n","    calib = f.readlines()\n","\n","# get projection matrices (rectified left camera --> left camera (u,v,z))\n","# Rectified 2번 카메라 좌표계에서 2번 카메라 (u,v,z) 좌표계로의 변환\n","P_rect2_cam2 = np.array([float(x) for x in calib[25].strip().split(' ')[1:]]).reshape((3,4))\n","\n","\n","# get rectified rotation matrices (left camera --> rectified left camera) #R_rect_02\n","# 2번 카메라 좌표계로부터 Rectified 2번 카메라 좌표계로의 회전 변환\n","R_ref2_rect2 = np.array([float(x) for x in calib[24].strip().split(' ')[1:]]).reshape((3, 3,))\n","R_ref2_rect2 = np.insert(R_ref2_rect2, 3, values=[0,0,0], axis=0)\n","R_ref2_rect2 = np.insert(R_ref2_rect2, 3, values=[0,0,0,1], axis=1)\n","\n","\n","# get rigid transformation from Camera 0 (ref) to Camera 2 #R_02 #T_02\n","# 0번 카메라 좌표계로부터 2번 카메라 좌표계로의 변환\n","R_2 = np.array([float(x) for x in calib[21].strip().split(' ')[1:]]).reshape((3,3))\n","t_2 = np.array([float(x) for x in calib[22].strip().split(' ')[1:]]).reshape((3,1))\n","T_ref0_ref2 = np.insert(np.hstack((R_2, t_2)), 3, values=[0,0,0,1], axis=0)"],"metadata":{"id":"FxmHiWjH_ZqT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get LiDAR and IMU Transformation matrices"],"metadata":{"id":"Ymr3IGeH_kuA"}},{"cell_type":"code","source":["# 라이다 좌표계로부터 0번 카메라 좌표계로의 변환\n","T_velo_ref0 = get_rigid_transformation(r'2011_10_03/calib_velo_to_cam.txt')\n","\n","# IMU 좌표계로부터 라이다 좌표계로의 변환\n","T_imu_velo = get_rigid_transformation(r'2011_10_03/calib_imu_to_velo.txt')"],"metadata":{"id":"cRlNyw53_j5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Get LiDAR ⬌ Camera2 Rotation matrices\n","\n"],"metadata":{"id":"Vz-T9MBnAjsq"}},{"cell_type":"code","source":["# transform from velo (LiDAR) to left color camera (shape 3x4)\n","\n","### TODO Ref0 카메라 좌표계에 대한 LiDAR 좌표 변환(T_velo_ref0) 를 안다고 할 때, 이 LiDAR 좌표를 카메라 2 (u,v,z) 좌표계에 맞추시오 ###\n","T_velo_cam2 =\n","\n","# homogeneous transform from left color camera to velo (LiDAR) (shape: 4x4)\n","T_cam2_velo = np.linalg.inv(np.insert(T_velo_cam2, 3, values=[0,0,0,1], axis=0))"],"metadata":{"id":"ba_C6VjVAX1i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Get IMU ⬌ Camera2 Rotation matrices\n"],"metadata":{"id":"nJcbv-AxBJxV"}},{"cell_type":"code","source":["### TODO IMU 좌표계에 대한 LiDAR 좌표 변환(T_imu_velo) 를 안다고 할 때, 이 LiDAR 좌표를 카메라 2 (u,v,z) 좌표계에 맞추시오 ###\n","T_imu_cam2 =\n","\n","# homogeneous transform from left color camera to IMU (shape: 4x4)\n","T_cam2_imu = np.linalg.inv(np.insert(T_imu_cam2, 3, values=[0,0,0,1], axis=0))"],"metadata":{"id":"che3QAEjAhkZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Get Object Detection pipeline**"],"metadata":{"id":"1ptISKMrBxP6"}},{"cell_type":"code","source":["# 2D object detector 로 YOLOv5 사용\n","\n","!git clone https://github.com/ultralytics/yolov5\n","!pip install -r yolov5/requirements.txt  #Install whatever is needed"],"metadata":{"id":"uagV1pAQAi0N","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1571dc8a-0356-446e-9380-29c768eb1d60","executionInfo":{"status":"ok","timestamp":1723527942681,"user_tz":-540,"elapsed":90584,"user":{"displayName":"이지현","userId":"13892748972160252839"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 16843, done.\u001b[K\n","remote: Counting objects: 100% (18/18), done.\u001b[K\n","remote: Compressing objects: 100% (18/18), done.\u001b[K\n","remote: Total 16843 (delta 4), reused 10 (delta 0), pack-reused 16825\u001b[K\n","Receiving objects: 100% (16843/16843), 15.58 MiB | 19.04 MiB/s, done.\n","Resolving deltas: 100% (11548/11548), done.\n","Collecting gitpython>=3.1.30 (from -r yolov5/requirements.txt (line 5))\n","  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 6)) (3.7.1)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 7)) (1.26.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 8)) (4.10.0.84)\n","Collecting pillow>=10.3.0 (from -r yolov5/requirements.txt (line 9))\n","  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 10)) (5.9.5)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 11)) (6.0.2)\n","Requirement already satisfied: requests>=2.32.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 12)) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 13)) (1.13.1)\n","Collecting thop>=0.1.1 (from -r yolov5/requirements.txt (line 14))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 15)) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 16)) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 17)) (4.66.5)\n","Collecting ultralytics>=8.2.34 (from -r yolov5/requirements.txt (line 18))\n","  Downloading ultralytics-8.2.76-py3-none-any.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 27)) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 28)) (0.13.1)\n","Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 42)) (71.0.4)\n","Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r yolov5/requirements.txt (line 5))\n","  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r yolov5/requirements.txt (line 12)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r yolov5/requirements.txt (line 12)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r yolov5/requirements.txt (line 12)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r yolov5/requirements.txt (line 12)) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->-r yolov5/requirements.txt (line 15))\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r yolov5/requirements.txt (line 18)) (9.0.0)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r yolov5/requirements.txt (line 18))\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r yolov5/requirements.txt (line 27)) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r yolov5/requirements.txt (line 27)) (2024.1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r yolov5/requirements.txt (line 5))\n","  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (1.3.0)\n","Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading ultralytics-8.2.76-py3-none-any.whl (865 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: smmap, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, ultralytics-thop, thop, ultralytics\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","Successfully installed gitdb-4.0.11 gitpython-3.1.43 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pillow-10.4.0 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.2.76 ultralytics-thop-2.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]},"id":"e6d1594967d440acbbfede12e9e6e128"}},"metadata":{}}]},{"cell_type":"code","source":["## YOLOv5 제공 체크포인트 다운로드\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"],"metadata":{"id":"bzz2r4qgB7SL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723527951851,"user_tz":-540,"elapsed":9176,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"381f182f-7318-449d-a0a9-d98e70cd55dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","YOLOv5 🚀 2024-8-13 Python-3.10.12 torch-2.3.1+cu121 CPU\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100%|██████████| 14.1M/14.1M [00:00<00:00, 247MB/s]\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","Adding AutoShape... \n"]}]},{"cell_type":"code","source":["# set confidence and IOU thresholds\n","model.conf = 0.25  # confidence threshold (0-1), default: 0.25\n","model.iou = 0.25  # NMS IoU threshold (0-1), default: 0.45"],"metadata":{"id":"eX4ENFYUB9aA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/ultralytics/yolov5/raw/master/data/coco.yaml"],"metadata":{"id":"Bhn2asA6FH8f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723527978051,"user_tz":-540,"elapsed":317,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"090bc70c-fe1c-4a9f-b867-a7bff3dbc7e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-13 05:46:17--  https://github.com/ultralytics/yolov5/raw/master/data/coco.yaml\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/ultralytics/yolov5/master/data/coco.yaml [following]\n","--2024-08-13 05:46:17--  https://raw.githubusercontent.com/ultralytics/yolov5/master/data/coco.yaml\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2489 (2.4K) [text/plain]\n","Saving to: ‘coco.yaml’\n","\n","\rcoco.yaml             0%[                    ]       0  --.-KB/s               \rcoco.yaml           100%[===================>]   2.43K  --.-KB/s    in 0s      \n","\n","2024-08-13 05:46:17 (34.1 MB/s) - ‘coco.yaml’ saved [2489/2489]\n","\n"]}]},{"cell_type":"code","source":["import yaml\n","\n","with open('coco.yaml', \"r\") as stream:\n","    try:\n","        classes = yaml.safe_load(stream)['names']\n","    except yaml.YAMLError as exc:\n","        print(exc)"],"metadata":{"id":"KytLXcfaFLmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_color(idx):\n","    '''returns a random color seeded from the input index '''\n","    blue = idx*5 % 256\n","    green = idx*15 % 256\n","    red = idx*25 % 256\n","    return (red, green, blue)\n","\n","\n","def draw_boxes(image_in, boxes, categories, color, mot_mode=False):\n","    image = image_in.copy()\n","    h, w, _ = image.shape\n","    for i, box in enumerate(boxes):\n","        label = classes[int(categories[i])]\n","        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, thickness=2)\n","        cv2.putText(image, str(label), (int(box[0]), int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=2)\n","    return image"],"metadata":{"id":"k66vOf7GE7hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install filterpy"],"metadata":{"id":"CJ9hZo_HIMgN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723528038504,"user_tz":-540,"elapsed":7462,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"49dd64d4-5c2c-4631-efd5-fba66a0a12fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting filterpy\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.13.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n","Building wheels for collected packages: filterpy\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=543e57a36d980df67fc9207f45127996fe0930a8ae0fd3fce62f8e06a69ccc6f\n","  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n","Successfully built filterpy\n","Installing collected packages: filterpy\n","Successfully installed filterpy-1.4.5\n"]}]},{"cell_type":"code","source":["from filterpy.kalman import KalmanFilter\n","from scipy.linalg import block_diag\n","from filterpy.common import Q_discrete_white_noise\n","\n","def constantVelocity_KF(R_std, Q_std, dt):\n","    ''' creates and instantiates a filterpy 2D Kalman Filter object\n","        with a 3D constant velocity model\n","        Inputs:\n","            R_Std (float) - measurement uncertainty\n","            Q_std (float) - Proccess noise covariance\n","            dt (float) - System Time Step\n","        Outputs:\n","            kf (filterpy object) 3D Kalman Filter object\n","    '''\n","\n","    kf = KalmanFilter(dim_x=6, dim_z=3)\n","    kf.x = np.array([10., 0., 0., 0., 0., 0.]) # initial state 10m in front of ego\n","    kf.P = np.eye(6)*1000\n","    q = Q_discrete_white_noise(dim=3, dt=dt, var=Q_std**2)\n","    kf.Q = block_diag(q, q) # Process noise covariance\n","    kf.R = np.eye(3) * R_std**2 # measurement uncertainty/noise\n","    kf.H = np.array([[1, 0, 0, 0, 0, 0],  # measurement function\n","                     [0, 0, 1, 0, 0, 0],\n","                     [0, 0, 0, 0, 1, 0]])\n","\n","    kf.F = np.array([[1, dt, 0, 0,  0, 0 ],     # state transition matrix\n","                     [0, 1,  0, 0,  0, 0 ],\n","                     [0, 0,  1, dt, 0, 0 ],\n","                     [0, 0,  0, 1,  0, 0 ],\n","                     [0, 0,  0, 0,  1, dt],\n","                     [0, 0,  0, 0,  0, 1 ]])\n","\n","    return kf"],"metadata":{"id":"TU5kLSWOIT45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.median(np.diff(cam2_total_seconds))"],"metadata":{"id":"PDVphLIAVG1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723528451881,"user_tz":-540,"elapsed":323,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"4d30e372-0939-4cfc-f447-4acd47acdb6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.10361651199491462"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# define measurement noise std dev and process noise std dev\n","R_std = 0.1 # measurement uncertainty\n","Q_std = 0.35 #  process noise\n","dt = np.median(np.diff(cam2_total_seconds))\n","\n","kf = constantVelocity_KF(R_std, Q_std, dt)"],"metadata":{"id":"ID8B8lCgQjzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(kf.x)\n","kf.predict() ## 상태값 예측\n","print(kf.x)\n","kf.update((17, 16, 0.003)) ## 예측한 상태값과 Measurement 로 상태 업데이트\n","print(kf.x, 'update')\n","\n","kf.predict()\n","print(kf.x)\n","kf.update((17.5, 15, 0.003))\n","print(kf.x, 'update')\n","\n","kf.predict()\n","print(kf.x)\n","kf.update((17.8, 15, 0.003))\n","print(kf.x, 'update')\n","\n","kf.predict()\n","print(kf.x)\n","kf.update((18, 14.9, 0.003))\n","print(kf.x, 'update')\n","\n","kf.predict()\n","print(kf.x)\n","kf.update((18.1, 14.07, 0.003))\n","print(kf.x, 'update')\n","\n","kf.predict()\n","print(kf.x)"],"metadata":{"id":"BtqUlInrRSt8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723528545873,"user_tz":-540,"elapsed":318,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"85abbfb8-b271-48e7-a0c7-28459748f047"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[         10           0           0           0           0           0]\n","[         10           0           0           0           0           0]\n","[         17      0.7178          16        1.64       0.003  0.00030758] update\n","[     17.074      0.7178       16.17        1.64   0.0030318  0.00030758]\n","[       17.5      4.8177      15.001     -9.5001       0.003  8.5309e-07] update\n","[     17.999      4.8177      14.017     -9.5001   0.0030001  8.5309e-07]\n","[     17.833      3.8794      14.968     -4.7638       0.003  3.7355e-07] update\n","[     18.235      3.8794      14.474     -4.7638   0.0030001  3.7355e-07]\n","[      18.07      3.2113       14.88     -3.3779   0.0030026  3.6326e-05] update\n","[     18.403      3.2113       14.53     -3.3779   0.0030063  3.6326e-05]\n","[     18.221       2.607      14.094     -4.3504   0.0029945 -6.5808e-05] update\n","[     18.492       2.607      13.643     -4.3504   0.0029877 -6.5808e-05]\n"]}]},{"cell_type":"markdown","source":["## **Set up tracking pipeline**\n","\n","The tracking will be a 3D real world extension of the [SORT algorithm](https://arxiv.org/pdf/1602.00763.pdf). \\\n","Instead of tracking bounding box location and aspect, we will **simple track the (x, y, z) locations of each detected object**. Even though we can esily neglect the z-axis, we will include it to keep our coordinate trasnformations easier.\n","\n","In our **Kalman Filter we will use a constant velocity model** with a random accleration assumption.\n","\n","The tracking pipeline will use the object detection methods from YOLOv5 as a backbone.\n","\n","The **L2 distance between object (x,y,z) centers will be used as a cost.**\n","\n","The Hungarian Algorithm (linear_sum_assignemnt in Python) will be used to match old tracks with new updates and determine if tracks are not updated.\n","\n","Even though an autonomous vehicle does not need bounding boxes, we will still implement them in our tracker.\n","We will **need to rotate the updated IMU (x,y,z) to the Camera (u,v,z)** which will serve as the new location for the bounding box."],"metadata":{"id":"EaYb58QpIKCV"}},{"cell_type":"code","source":["from scipy.optimize import linear_sum_assignment\n","\n","## 두 Object center를 입력으로 받아, center의 L2 distance를 도출하는 함수\n","def total_cost(center1, center2):\n","    ''' Return L2 distance between object centers '''\n","\n","    ### TODO 두 center 간의 L2 distance를 도출하세요 ###\n","    return\n","\n","\n","def associate(old_centers, new_centers, dist_thresh=1):\n","    \"\"\"\n","    Inputs:\n","        old_centers - former center locations (at time 0)\n","        new_centers - new center locations (at time 1)\n","        dist_thresh - distance threshold to declare tracks matched or unmatched\n","    Outputs:\n","       matches - Matched tracks\n","       unmatched_detections - Unmatched Detections\n","       unmatched_trackers - Unmatched Tracks\n","\n","    \"\"\"\n","    if (len(new_centers) == 0) and (len(old_centers) == 0):\n","        return [], [], []\n","    elif(len(old_centers)==0):\n","        return [], new_centers, []\n","    elif(len(new_centers)==0):\n","        return [], [], old_centers\n","\n","    # distances는 Object center 간의 L2 distance를 저장\n","    distances = np.zeros((len(old_centers),len(new_centers)),dtype=np.float32)\n","\n","    ### TODO 이중 for loop 를 통해 모든 old_center들과 new_center들 간의 L2 distance를 distances 변수에 저장하세요 ###\n","\n","\n","\n","    # Hungarian Algorithm (L2 distance metric을 cost로 사용)\n","    ## Hungarian Algorithm은 costmap 에서 전체 cost가 가장 작아지는 방향으로 매칭되는 행과 열을 반환\n","    row_ind, col_ind = linear_sum_assignment(distances)\n","    hungarian_matrix = np.array(list(zip(row_ind, col_ind)))\n","\n","    matches, unmatched_detections, unmatched_tracks = [], [], []\n","\n","    # Hungarian Matrix를 순회하며 Hungarian algorithm이 매칭시킨 요소 탐색\n","    for h in hungarian_matrix:\n","        #### TODO 매칭된 요소 간 거리가 threshold distance 이상이면 해당 old_center와 new_center를 unmatched에 추가####\n","\n","\n","        #### TODO 그렇지 않으면 매칭에 추가 ####\n","\n","\n","    if(len(matches)==0):\n","        matches = np.empty((0,2), dtype=int)\n","    else:\n","        matches = np.concatenate(matches, axis=0)\n","\n","    # old_center들을 순회하며, 매칭된 탐지가 없으면 unmatched_tracks에 추가\n","    for t, trk in enumerate(old_centers):\n","        if(t not in hungarian_matrix[:,0]):\n","            unmatched_tracks.append(trk)\n","\n","    # new_center들을 순회하며, 매칭된 트랙이 없으면 unmatched_new_centers에 추가\n","    for d, det in enumerate(new_centers):\n","        if(d not in hungarian_matrix[:,1]):\n","            unmatched_detections.append(det)\n","\n","    return matches, unmatched_detections, unmatched_tracks\n"],"metadata":{"id":"X5Wpx6tGB-6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["REF_IMAGE = cv2.imread(left_image_paths[0])\n","\n","def xyz2uvz(xyz):\n","    ''' converts IMU xyz to Cam 2 UVZ '''\n","    xyzw = np.hstack((xyz, 1))[:, None]\n","    uvz = xyzw2camera(xyzw, T_imu_cam2, REF_IMAGE)\n","\n","    return uvz"],"metadata":{"id":"bZwmT-s7M4nZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Track():\n","    def __init__(self, idx, xyz, color, bbox=None, cat=0, kf=None):\n","        '''\n","          idx - track index\n","          xyz - track center in IMU (x, y, z) coordinates\n","          color - color for the object and it's bounding box\n","          bbox - (1x4 array) bounding box (x,y) coordiantes\n","               We will only care about the bounding box until the object has\n","               been matched\n","          cat - track category\n","          kf - KalmanFilter Object for tracking\n","          age - track age, number of frames track has been observed\n","          hit_streak - simultaneuos hits\n","          unmatched_age - number of frames track has not been observed\n","          fov - (_Bool) indicates that the object is approaching edge of FOV\n","          '''\n","        # set inputs\n","        self.idx = idx\n","        self.xyz = xyz\n","        self.color = color\n","        self.bbox = bbox\n","        self.cat = cat\n","        self.kf = kf\n","\n","        # everything else\n","        self.uvz = None # uvz image location of object\n","        self.hit_streak = 1\n","        self.age = 0 # number of frames track has not been observed\n","        self.fov = 0 # indicates that the object is approaching edge of FOV\n","        self.history = [xyz] # initialize track (x,y,z) history\n","\n","\n","    def update(self, new_xyz):\n","        ''' Update function for a given track object '''\n","\n","        # update Kalman Filter State with new observation\n","        self.kf.update(new_xyz)\n","\n","        # update track file position\n","        self.xyz = new_xyz\n","\n","        # update uvz image location\n","        self.uvz = xyz2uvz(self.xyz)\n","\n","        # reset age\n","        self.age = 0\n","\n","        # update history\n","        self.history.append(self.xyz)\n","\n","\n","    def predict(self):\n","        ''' predicts new track location and updates it's bounding box '''\n","        # predict new location\n","        self.kf.predict()\n","\n","        # update track state after KF prediction\n","        self.xyz = self.kf.x[::2] # get positions only\n","\n","        # update bounding box in image coordinate\n","        if isinstance(self.bbox, np.ndarray):\n","            if self.uvz.all():\n","                self.update_bbox(self.uvz)\n","\n","\n","    def update_bbox(self, new_uvz):\n","        '''updates bounding box to new location based on new uvz center coordinates\n","          from the IMU. To be used during Kalman Filter updates.\n","          '''\n","        width = self.bbox[2] - self.bbox[0]\n","        height = self.bbox[3] - self.bbox[1]\n","\n","        x1 = np.round(new_uvz[0] - width/2.).astype(int)\n","        x2 = np.round(new_uvz[0] + width/2.).astype(int)\n","\n","        y1 = np.round(new_uvz[1] - height/2.).astype(int)\n","        y2 = np.round(new_uvz[1] + height/2.).astype(int)\n","\n","        self.bbox = np.array([x1, y1, x2, y2]).squeeze()"],"metadata":{"id":"tbW0M4cSbZqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Tracker():\n","    def __init__(self,  max_age=3, min_hits=2, dist_threshold=1):\n","        '''\n","            Inputs:\n","                max_age - max number of updates for a track to be declared lost\n","                min_hits - minnumber of hits/matches for a track to be declared\n","                dist_threshold - min L2 distance for a track to be considered\n","            Outputs:\n","                ret_tracks - returned tracks list after min hits and age out\n","            '''\n","        self.max_age = max_age\n","        self.min_hits = min_hits\n","        self.dist_threshold = dist_threshold\n","        self.tracks = []\n","        self.track_idx = 0 # unique track index\n","        self.frame_count = 0 # frames\n","\n","    def update(self, detections, det_info):\n","        ''' Update the tracks with new detections\n","            Inputs:\n","                detections - Nx3 array of (x, y, z) object locations\n","                det_info - Nx6 array of bounding boxes, confidence, and category\n","                           for each detection\n","            Outputs:\n","                ret_tracks - list of new track objects that are filtered\n","                    out based on min hits required for a track to be shown\n","            '''\n","        # unpack detection info\n","        bboxes = det_info[:, :4]\n","        categories = det_info[:, 5]\n","\n","        # increment frame count\n","        self.frame_count += 1\n","\n","        # get current track centers for association\n","        tracks_xyz = [trk.xyz for trk in self.tracks]\n","\n","        # associate new detections with current tracks (Hungarian)\n","        matches, unmatched_dets, unmatched_trks = associate(tracks_xyz,\n","                                                            detections,\n","                                                            self.dist_threshold)\n","\n","        # get returnable tracks list\n","        ret_tracks = []\n","\n","        # update matches\n","        for (old_idx, new_idx) in matches:\n","\n","            # get new detection coordinates\n","            new_xyz = detections[new_idx]\n","\n","            #### TODO update KF with new observation ####\n","\n","\n","            # update bounding box with new detection\n","            self.tracks[old_idx].bbox = np.round(bboxes[new_idx]).astype(int)\n","\n","            #### TODO predict KF ####\n","\n","\n","            # update hit streak\n","            self.tracks[old_idx].hit_streak += 1\n","\n","            # add category to track file\n","            self.tracks[old_idx].cat = categories[new_idx]\n","\n","            #### TODO if hit_streak is big enough, append to ret_tracks ####\n","\n","\n","\n","        # get indexes of tracks to delete\n","        to_del = []\n","\n","        # handle unmatched observations\n","        for old_xyz in unmatched_trks:\n","\n","            # get old track index\n","            trk_idx = np.where(tracks_xyz == old_xyz)[0][0]\n","\n","            # increment track age\n","            self.tracks[trk_idx].age += 1\n","\n","            # reset hit streak\n","            self.tracks[trk_idx].hit_streak = 0\n","\n","            #### TODO track age 가 max_age보다 넘는다면 to_del list에 추가 ####\n","\n","\n","            #### TODO track age 가 max_age보다 작다면 계속 prediction 진행 ####\n","\n","\n","                # add to new tracks if old enough and has been matched\n","                # only matched tracks have a bbox\n","                if (len(self.tracks[trk_idx].history) >= self.min_hits) \\\n","                    and (isinstance(self.tracks[trk_idx].bbox, np.ndarray)):\n","                    ret_tracks.append(self.tracks[trk_idx])\n","\n","        # delete tracks\n","        self.tracks[:] = [trk for i, trk in enumerate(self.tracks)\n","                                                      if i not in to_del]\n","\n","        # get new observations\n","        for new_xyz in unmatched_dets:\n","\n","            # get new track\n","            # add to the index to get brighter colors\n","            color = get_color(self.track_idx + np.random.randint(25, 100))\n","\n","            #### TODO 새로운 bbox observation에 대해 새로운 kf 할당, track 생성 ####\n","\n","\n","            # incerement unique track indexes\n","            self.track_idx += 1\n","\n","            # add to tracks list\n","            self.tracks.append(track)\n","\n","        return ret_tracks\n"],"metadata":{"id":"hJzdVYPDTsUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Tracking Pipeline**"],"metadata":{"id":"ct1RG6Z9EGjq"}},{"cell_type":"code","source":["#### Utils for get_detections (Object Detector)####\n","\n","def project_velobin2uvz(bin_path, T_uvz_velo, image, remove_plane=True):\n","    ''' Projects LiDAR point cloud onto the image coordinate frame (u, v, z)\n","        '''\n","\n","    # get homogeneous LiDAR points from bin file\n","    xyzw = bin2xyzw(bin_path, remove_plane)\n","\n","    # project velo (x, z, y, w) onto camera (u, v, z) coordinates\n","    velo_uvz = xyzw2camera(xyzw, T_uvz_velo, image, remove_outliers=True)\n","\n","    return velo_uvz\n","\n","def get_uvz_centers(image, velo_uvz, bboxes, draw=True):\n","    ''' Obtains detected object centers projected to uvz camera coordinates.\n","        Starts by associating LiDAR uvz coordinates to detected object centers,\n","        once a match is found, the coordiantes are transformed to the uvz\n","        camera reference and added to the bboxes array.\n","\n","        NOTE: The image is modified in place so there is no need to return it.\n","\n","        Inputs:\n","          image - input image for detection\n","          velo_uvz - LiDAR coordinates projected to camera reference\n","          bboxes - xyxy bounding boxes form detections from yolov5 model output\n","          draw - (_Bool) draw measured depths on image\n","        Outputs:\n","          bboxes_out - modified array containing the object centers projected\n","                       to uvz image coordinates\n","        '''\n","\n","    # unpack LiDAR camera coordinates\n","    u, v, z = velo_uvz\n","\n","    # get new output\n","    bboxes_out = np.zeros((bboxes.shape[0], bboxes.shape[1] + 3))\n","    bboxes_out[:, :bboxes.shape[1]] = bboxes\n","\n","    # iterate through all detected bounding boxes\n","    for i, bbox in enumerate(bboxes):\n","        pt1 = torch.round(bbox[0:2]).to(torch.int).numpy()\n","        pt2 = torch.round(bbox[2:4]).to(torch.int).numpy()\n","\n","        # get center location of the object on the image\n","        obj_x_center = (pt1[1] + pt2[1]) / 2\n","        obj_y_center = (pt1[0] + pt2[0]) / 2\n","\n","        # now get the closest LiDAR points to the center\n","        center_delta = np.abs(np.array((v, u))\n","                              - np.array([[obj_x_center, obj_y_center]]).T)\n","\n","        # choose coordinate pair with the smallest L2 norm\n","        min_loc = np.argmin(np.linalg.norm(center_delta, axis=0))\n","\n","        # get LiDAR location in image/camera space\n","        velo_depth = z[min_loc]; # LiDAR depth in camera space\n","        uvz_location = np.array([u[min_loc], v[min_loc], velo_depth])\n","\n","        # add velo projections (u, v, z) to bboxes_out\n","        bboxes_out[i, -3:] = uvz_location\n","\n","        # draw depth on image at center of each bounding box\n","        # This is depth as perceived by the camera\n","        if draw:\n","            object_center = (np.round(obj_y_center).astype(int),\n","                             np.round(obj_x_center).astype(int))\n","            cv2.putText(image,\n","                        '{0:.2f} m'.format(velo_depth),\n","                        object_center, # top left\n","                        cv2.FONT_HERSHEY_SIMPLEX,\n","                        0.5, # font scale\n","                        (255, 0, 0), 2, cv2.LINE_AA)\n","\n","    return bboxes_out\n","\n","def get_detection_coordinates(image, bin_path, model, T_velo_cam, draw_boxes=True, draw_depth=True):\n","    ''' Obtains detections for the input image, along with the coordinates of\n","        the detected object centers. The coordinate obtained are:\n","            - Camera with depth --> uvz\n","            - LiDAR/velo --> xyz\n","            - GPS/IMU --> xyz\n","        Inputs:\n","            image - rgb image to run detection on\n","            bin_path - path to LiDAR bin file\n","            T_velo_cam - transformation from LiDAR to camera## (u,v,z) space\n","            model - detection model (this functions assumes a yolo5 model)\n","                  - any detector can be used as long as it has the following attributes:\n","                    show, xyxy\n","        Output:\n","            bboxes - array of detected bounding boxes, confidences, classes,\n","            velo_uv - LiDAR points porjected to camera uvz coordinate frame\n","            coordinates - array of all object center coordinates in the frames\n","                          listed above\n","        '''\n","    ## 1. compute detections in the left image\n","    detections = model(image)\n","\n","    # draw boxes on image\n","    if draw_boxes:\n","        detections.show()\n","\n","    # get bounding box locations (x1,y1), (x2,y2) Prob, class\n","    bboxes = detections.xyxy[0].cpu() # remove from GPU\n","\n","    # get LiDAR points and transform them to image/camera space\n","    velo_uvz = project_velobin2uvz(bin_path,\n","                                   T_velo_cam,\n","                                   image,\n","                                   remove_plane=True)\n","\n","    # get uvz centers for detected objects\n","    bboxes = get_uvz_centers(image,\n","                             velo_uvz,\n","                             bboxes,\n","                             draw=draw_depth)\n","\n","    return bboxes, velo_uvz"],"metadata":{"id":"bAeRimyNmDk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_detections(image, bin_path, model, T_velo_cam, T_cam_imu):\n","     ''' Obtains (x,y,z) locations referenced to IMU and image bounding boxes\n","         Camera reference specified by image and Transformation Matrices,\n","         Can use any camera as the reference\n","         '''\n","     # get detections and object centers in uvz\n","     bboxes, velo_uvz = get_detection_coordinates(image,\n","                                                  bin_path,\n","                                                  model,\n","                                                  T_velo_cam,\n","                                                  draw_boxes=False,\n","                                                  draw_depth=False)\n","\n","     # get transformed coordinates of object centers\n","     uvz = bboxes[:, -3:]\n","\n","     # transform to IMU (u,v,z)\n","     imu_xyz = transform_uvz(uvz, T_cam_imu)\n","\n","     return imu_xyz, bboxes, velo_uvz"],"metadata":{"id":"Pnwzaoh5JCi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initiate the tracker\n","tracker = Tracker(max_age=5, min_hits=2, dist_threshold=1.75)\n","\n","# results\n","result_video = []\n","\n","# iterate through all frames\n","for index in range(len(left_image_paths)):\n","    left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n","    bin_path = bin_paths[index]\n","\n","    # get detections from Camera-LiDAR fusion\n","    imu_xyz, bboxes, velo_uvz = get_detections(left_image, bin_path, model, T_velo_cam2, T_cam2_imu)\n","\n","    # get tracks from Tracker by taking new observations\n","    new_tracks = tracker.update(imu_xyz, bboxes[:, :6])\n","\n","    # draw projected velo on blank image\n","    velo_image = np.zeros_like(left_image)\n","\n","    # draw boxes and labels on new tracks\n","    for trk in new_tracks:\n","\n","        if trk.uvz.all():\n","\n","            # draw bounding box on the tracked object\n","            xy1 = (trk.bbox[0], trk.bbox[1])\n","            xy2 = (trk.bbox[2], trk.bbox[3])\n","            cv2.rectangle(left_image, xy1, xy2, trk.color, 2);\n","\n","            # draw track label on image (cat_idx)\n","            label = str(classes[int(trk.cat)] + '_' + str(trk.idx))\n","            cv2.putText(left_image, label, (trk.bbox[0] - 10, trk.bbox[1] - 5),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, trk.color, 2, cv2.LINE_AA);\n","\n","\n","            # draw points on velo image\n","            velo_box = velo_uvz[:, ((velo_uvz[0, :] <= trk.bbox[2]) & (velo_uvz[0, :] >= trk.bbox[0]))\n","                                  & ((velo_uvz[1, :] <= trk.bbox[3]) & (velo_uvz[1, :] >= trk.bbox[1]))]\n","\n","            draw_velo_on_image(velo_box, velo_image)\n","\n","\n","            # draw slant range on left image and velo image\n","            object_center = (np.round(trk.uvz[0][0]).astype(int),\n","                            np.round(trk.uvz[1][0]).astype(int))\n","            cv2.putText(left_image,\n","                        '{0:.2f} m'.format(np.linalg.norm(trk.xyz)),\n","                        object_center, # top left\n","                        cv2.FONT_HERSHEY_SIMPLEX,\n","                        0.5, # font scale\n","                        (255, 0, 0), 2, cv2.LINE_AA);\n","            cv2.putText(velo_image,\n","                        '{0:.2f} m'.format(np.linalg.norm(trk.xyz)),\n","                        object_center, # top left\n","                        cv2.FONT_HERSHEY_SIMPLEX,\n","                        0.5, # font scale\n","                        (255, 0, 0), 2, cv2.LINE_AA);\n","\n","    # get stacked frame\n","    stacked = np.vstack((left_image, velo_image))\n","\n","    # append to results\n","    result_video.append(stacked)"],"metadata":{"id":"jeAZA7JbDrRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Store results in a video"],"metadata":{"id":"q9f7xroC_2ge"}},{"cell_type":"code","source":["# camera 2 frames per second\n","cam2_fps = 1/np.median(np.diff(cam2_total_seconds))\n","cam2_fps"],"metadata":{"id":"ZU-dZhHeCPUH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723531744537,"user_tz":-540,"elapsed":4,"user":{"displayName":"이지현","userId":"13892748972160252839"}},"outputId":"3aa447e3-778e-4648-a0a0-a2fd357dec1d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.650971459539951"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["h, w, _ = stacked.shape\n","out = cv2.VideoWriter('track_vid_stacked_with_filter_1.mp4',\n","                      cv2.VideoWriter_fourcc(*'MP4V'),\n","                      cam2_fps,\n","                      (w,h))\n","\n","for img in result_video:\n","    out.write(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","out.release()"],"metadata":{"id":"PvmE0pp75Y5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BFdMKcCsd-RL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.8 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"}},"colab":{"provenance":[{"file_id":"https://github.com/itberrios/CV_tracking/blob/main/kitti_tracker/2_kitti_tracking.ipynb","timestamp":1722441549677}]}},"nbformat":4,"nbformat_minor":0}
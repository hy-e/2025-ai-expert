{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYXIHngPoI2W"
   },
   "source": [
    "# Image Segmentation using Fully Convolutional Network (FCN)\n",
    "---\n",
    "TA : Jaehoon Yoo (wogns98@kaist.ac.kr) Chanhyuk Lee (chan3684@kaist.ac.kr)\n",
    "\n",
    "---\n",
    "## Instructions\n",
    "- In this assignment, we will perform semantic segmentation on PASCAL VOC 2011 dataset which contains 20 object categories. We use the Semantic Boundaries Dataset (SBD) as it contains more segmentation labels than the original dataset.\n",
    "- To this end, you need to implement necessary network components, load and fine-tune the pretrained network, and report segmentation performance on the validation set.\n",
    "- Fill in the section marked **Px.x** with the appropriate code. **You can only modify inside those areas, and not the skeleton code.**\n",
    "- To begin, you should download this ipynb file into your own Google drive clicking `make a copy(사본만들기)`. Find the copy in your drive, change their name to `Segmentation_FCN.ipynb`, if their names were changed to e.g. `Copy of Segmentation_FCN.ipyb` or `Segmentation_FCN.ipynb의 사본`.\n",
    "- Also, you have to download a pretrained VGG network [here](https://drive.google.com/file/d/1HbA8D08-dgglvB9nDCpLYwhgaDOOmPBP/view?usp=sharing). Upload it to your drive, place it in the same directory with your Colab notebook.\n",
    "- <font color=\"red\">You'll be training large models. We recommend you to create at least **6GB** of space available on your Google drive to run everything properly.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1ZGXqrvlc_O"
   },
   "source": [
    "---\n",
    "## Prerequisite: Mount your gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a0HEy2Tok-2u"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tsq4xR-liMH"
   },
   "source": [
    "---\n",
    "## Prerequisite: Setup the `root` directory properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SHzfVbfmloDz"
   },
   "outputs": [],
   "source": [
    "# Specify the directory path where `Segmentation.ipynb` exists.\n",
    "# For example, if you saved `Segmentation.ipynb` in `/gdrive/MyDrive/Segmentation_FCN` directory,\n",
    "# then set root = '/gdrive/MyDrive/Segmentation_FCN'\n",
    "root = '/gdrive/MyDrive/Segmentation_FCN'\n",
    "root = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XDVC3lJl1_H"
   },
   "source": [
    "---\n",
    "# Basic settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jIiyjEGlwJ8"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eGlC_yM9lvue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n",
      "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-ktz0ve40\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-ktz0ve40\n",
      "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pydensecrf\n",
      "  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pydensecrf: filename=pydensecrf-1.0-cp310-cp310-linux_x86_64.whl size=3405241 sha256=77a27e90a12003e98bb10b31dbb98521ba598a85b690b019b50f2bbdb8e1fffc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ehxtiqr6/wheels/01/5b/61/87443ed3bf03dd2940375cf2f8b6fba88efece935465e490b0\n",
      "Successfully built pydensecrf\n",
      "Installing collected packages: pydensecrf\n",
      "Successfully installed pydensecrf-1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "from easydict import EasyDict as edict\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import VOCSegmentation, SBDataset\n",
    "from torchvision.datasets.vision import StandardTransform\n",
    "from torchvision.models.vgg import VGG, vgg16, make_layers\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W27cdKyEmJy1"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dczwhJI1l8Kl"
   },
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "cfg = {'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 1\n",
    "args.lr = 1e-4\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 5e-4\n",
    "args.epoch = 2\n",
    "args.tensorboard = True\n",
    "args.gpu = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iiGFJlxmCXW"
   },
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2b3_oX_2mDw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 874816), started 0:15:24 ago. (Use '!kill 874816' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-28ae0114ddb33362\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-28ae0114ddb33362\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup tensorboard.\n",
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"/gdrive/MyDrive/{str(result_dir).replace('/gdrive/MyDrive/', '')}\" --samples_per_plugin images=100\n",
    "else:\n",
    "    writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQZvsEeDmeZt"
   },
   "source": [
    "---\n",
    "# Utility functions\n",
    "\n",
    "Here are some utility functions that we will use throughout this assignment. You don't have to modify any of these.  \n",
    "**Conditional Random Field (CRF)** is a technique to further improve segmentation performance, mainly focusing on better localization. Details can be found in the [DeepLab](https://arxiv.org/abs/1606.00915) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LUOvr2A_mf4Q"
   },
   "outputs": [],
   "source": [
    "def get_parameters(model, bias=False):\n",
    "    \"\"\" Extracts weight and bias parameters from a model. \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if bias:\n",
    "                yield m.bias\n",
    "            else:\n",
    "                yield m.weight\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # weight is frozen because it is just a bilinear upsampling\n",
    "            if bias:\n",
    "                assert m.bias is None\n",
    "\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\" Make a 2D bilinear kernel suitable for upsampling\n",
    "        https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n",
    "    \"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n",
    "\n",
    "\n",
    "class toLongTensor:\n",
    "    \"\"\" Convert a byte tensor to a long tensor \"\"\"\n",
    "    def __call__(self, img):\n",
    "        output = torch.from_numpy(np.array(img).astype(np.int32)).long()\n",
    "        output[output == 255] = 21\n",
    "        return output\n",
    "\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(label_trues, label_preds, n_class):\n",
    "    \"\"\" Returns overall accuracy and mean IoU \"\"\"\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        iou = np.diag(hist) / (\n",
    "            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
    "        )\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    return acc, mean_iou\n",
    "\n",
    "\n",
    "class Colorize(object):\n",
    "    \"\"\" Colorize the segmentation labels \"\"\"\n",
    "    def __init__(self, n=35, cmap=None):\n",
    "        if cmap is None:\n",
    "            raise NotImplementedError()\n",
    "            self.cmap = labelcolormap(n)\n",
    "        else:\n",
    "            self.cmap = cmap\n",
    "        self.cmap = self.cmap[:n]\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        if len(x.size()) > 3 and x.size(1) > 1:\n",
    "            # if x has a shape of [B, C, H, W],\n",
    "            # where B and C denote a batch size and the number of semantic classe\n",
    "            # then translate it into a shape of [B, 1, H, W]\n",
    "            x = x.argmax(dim=1, keepdim=True).float()\n",
    "        assert (len(x.shape) == 4) and (x.size(1) == 1), 'x should have a shape of [B, 1, H, W]'\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        if (x.dtype == torch.float) and (x.max() < 2):\n",
    "            x = x.mul(255).long()\n",
    "        color_images = []\n",
    "        gray_image_shape = x.shape[1:]\n",
    "        for gray_image in x:\n",
    "            color_image = torch.ByteTensor(3, *gray_image_shape[1:]).fill_(0)\n",
    "            for label, cmap in enumerate(self.cmap):\n",
    "                mask = (label == gray_image[0]).cpu()\n",
    "                color_image[0][mask] = cmap[0]\n",
    "                color_image[1][mask] = cmap[1]\n",
    "                color_image[2][mask] = cmap[2]\n",
    "            color_images.append(color_image)\n",
    "        color_images = torch.stack(color_images)\n",
    "        return color_images\n",
    "\n",
    "\n",
    "def uint82bin(n, count=8):\n",
    "    \"\"\" Returns the binary of integer n, count refers to amount of bits \"\"\"\n",
    "    return ''.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n",
    "\n",
    "\n",
    "def get_color_map():\n",
    "    \"\"\" Returns N color map \"\"\"\n",
    "    N=25\n",
    "    color_map = np.zeros((N, 3), dtype=np.uint8)\n",
    "    for i in range(N):\n",
    "        r, g, b = 0, 0, 0\n",
    "        id = i\n",
    "        for j in range(7):\n",
    "            str_id = uint82bin(id)\n",
    "            r = r ^ (np.uint8(str_id[-1]) << (7-j))\n",
    "            g = g ^ (np.uint8(str_id[-2]) << (7-j))\n",
    "            b = b ^ (np.uint8(str_id[-3]) << (7-j))\n",
    "            id = id >> 3\n",
    "        color_map[i, 0] = r\n",
    "        color_map[i, 1] = g\n",
    "        color_map[i, 2] = b\n",
    "    color_map = torch.from_numpy(color_map)\n",
    "    return color_map\n",
    "\n",
    "\n",
    "def dense_crf(img, output_probs):\n",
    "    \"\"\" Conditional Random Field for better segmentation\n",
    "        Refer to https://github.com/lucasb-eyer/pydensecrf for details.\n",
    "    \"\"\"\n",
    "    c = output_probs.shape[0]\n",
    "    h = output_probs.shape[1]\n",
    "    w = output_probs.shape[2]\n",
    "\n",
    "    U = utils.unary_from_softmax(output_probs)\n",
    "    U = np.ascontiguousarray(U)\n",
    "\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    d = dcrf.DenseCRF2D(w, h, c)\n",
    "    d.setUnaryEnergy(U)\n",
    "    d.addPairwiseGaussian(sxy=1, compat=3)\n",
    "    d.addPairwiseBilateral(sxy=67, srgb=3, rgbim=img, compat=4)\n",
    "\n",
    "    Q = d.inference(10)\n",
    "    Q = np.array(Q).reshape((c, h, w))\n",
    "    return Q\n",
    "\n",
    "\n",
    "def add_padding(img):\n",
    "    \"\"\" Zero-pad image(or any array-like object) to 500x500. \"\"\"\n",
    "    w, h = img.shape[-2], img.shape[-1]\n",
    "    MAX_SIZE = 500\n",
    "    IGNORE_IDX = 21\n",
    "\n",
    "    assert max(w, h) <= MAX_SIZE, f'both height and width should be less than {MAX_SIZE}'\n",
    "\n",
    "    _pad_left = (MAX_SIZE - w) // 2\n",
    "    _pad_right = (MAX_SIZE - w + 1) // 2\n",
    "    _pad_up = (MAX_SIZE - h) // 2\n",
    "    _pad_down = (MAX_SIZE - h + 1) // 2\n",
    "\n",
    "    _pad = (_pad_up, _pad_down, _pad_left, _pad_right)\n",
    "\n",
    "    padding_img = transforms.Pad(_pad)\n",
    "    padding_target = transforms.Pad(_pad, fill=IGNORE_IDX)\n",
    "\n",
    "    img = F.pad(img, pad=_pad)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7WMaIEVmoub"
   },
   "source": [
    "---\n",
    "# Define `DataLoader` for training & validation set\n",
    "\n",
    "If the cell below fails with error message \"Destination path `./cls` already exists\", try again with `download=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "do8qfYkEmtNg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.42G/1.42G [14:07<00:00, 1.67MB/s]   \n"
     ]
    }
   ],
   "source": [
    "mean = [.485, .456, .406]\n",
    "std = [.229, .224, .225]\n",
    "\n",
    "# define transform functions.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "transform_train_target = transforms.Compose([\n",
    "    toLongTensor()\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "transform_test_target = transforms.Compose([\n",
    "    toLongTensor()\n",
    "])\n",
    "\n",
    "# define dataloader.\n",
    "sbd_transform_train = StandardTransform(transform_train, transform_train_target)\n",
    "sbd_transform_test = StandardTransform(transform_test, transform_test_target)\n",
    "try:\n",
    "  train_dataset = SBDataset(root='.', image_set='train', mode='segmentation', download=True, transforms=sbd_transform_train)\n",
    "except:\n",
    "  train_dataset = SBDataset(root='.', image_set='train', mode='segmentation', download=False, transforms=sbd_transform_train)\n",
    "test_dataset = SBDataset(root='.', image_set='val', mode='segmentation', download=False, transforms=sbd_transform_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVy7Dc_rmw2A"
   },
   "source": [
    "---\n",
    "# Define networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94PR5_BogsC-"
   },
   "source": [
    "## P0. VGG16 Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P4OKiddTXYgj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ClassificationCNN(VGG):\n",
    "    def __init__(self):\n",
    "        super().__init__(make_layers(cfg['vgg16'])) \n",
    "\n",
    "print(ClassificationCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv94fT75mxac"
   },
   "source": [
    "## P1. Implement FCN32 [(Illustration)](https://docs.google.com/drawings/d/1Vlm1kdIH9MZiJU_gLFhVNO_-alFGWyxLF_zSmQxkaO0/edit?usp=sharing)\n",
    "\n",
    "### (a) Declare convolutional layers as an alternative to FC layers\n",
    "The VGG16 backbone contains two fully connected layers (fc6, fc7). In fully convolutional network, these fc layers will be convolutionized. You should define proper convolutional layers, which will be initialized from fc layers of pretrained model in section (d).\n",
    "\n",
    "### (b) Declare pixel-wise classification layer using 1x1 convolution\n",
    "This section declares 1x1 convolution layer that produces classification per pixel\n",
    "\n",
    "### (c) Declare a deconvolution layer\n",
    "Declare a learnable upsampling layer.\n",
    "\n",
    "### (d) Load pretrained weights\n",
    "As mentioned in section (a), you should convolutionize the two fc layers(fc6, fc7) of pretrained model inside `load_pretrained` method. Initialize two convolutional layers with fc6, fc7 layers of the pretrained model, respectively.\n",
    "\n",
    "### (e) Implement `forward` method\n",
    "The `forward` method should\n",
    "- Feed input through the backbone to get `pool5` (given)\n",
    "- On top of `pool5`, perform pixel-wise classification.\n",
    "- Upsample the prediction to create segmentation on the original image resolution level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NLC1mpEFm05u"
   },
   "outputs": [],
   "source": [
    "class FCN32(VGG):\n",
    "    def __init__(self):\n",
    "        super().__init__(make_layers(cfg['vgg16']))\n",
    "\n",
    "        self.numclass = 21\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "        # fc layers in vgg are all converted into conv layers.\n",
    "        #################################\n",
    "        ## P1(a). Write your code here ##\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, stride=1, padding=0)\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1, stride=1, padding=0)\n",
    "        #################################\n",
    "        # self.conv6 =\n",
    "        # self.conv7 =\n",
    "        #################################\n",
    "\n",
    "        # prediction layer with 1x1 convolution layer.\n",
    "        #################################\n",
    "        ## P1(b). Write your code here ##\n",
    "        # self.score_x32 =\n",
    "        self.score_x32 = nn.Conv2d(4096, self.numclass, kernel_size=1, stride=1, padding=0)\n",
    "        #################################\n",
    "\n",
    "        # learnable upsampling layers in FCN model.\n",
    "        #################################\n",
    "        ## P1(c). Write your code here ##\n",
    "        # self.deconv1 =\n",
    "        self.deconv1 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=64, stride=32, padding=16, output_padding=0, bias=False)\n",
    "        #################################\n",
    "\n",
    "        # initialize\n",
    "        self._initialize_weights() # initialize upsampling weight\n",
    "\n",
    "    def load_pretrained(self, pretrained_model):\n",
    "        self.features = pretrained_model.features\n",
    "        fc6 = pretrained_model.classifier[0]\n",
    "        fc7 = pretrained_model.classifier[3]\n",
    "\n",
    "        #################################\n",
    "        ## P1(d). Write your code here ##\n",
    "        self.conv6.weight.data = fc6.weight.data.view(self.conv6.weight.shape)\n",
    "        self.conv6.bias.data = fc6.bias.data\n",
    "        self.conv7.weight.data = fc7.weight.data.view(self.conv7.weight.shape)\n",
    "        self.conv7.bias.data = fc7.bias.data\n",
    "        # self.conv6.weight.data =\n",
    "        # self.conv6.bias.data =\n",
    "        # self.conv7.weight.data =\n",
    "        # self.conv7.bias.data =\n",
    "        #################################\n",
    "\n",
    "    def vgg_layer_forward(self, x, indices):\n",
    "        output = x\n",
    "        start_idx, end_idx = indices\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            output = self.features[idx](output)\n",
    "        return output\n",
    "\n",
    "    def vgg_forward(self, x):\n",
    "        out = {}\n",
    "        layer_indices = [0, 5, 10, 17, 24, 31]\n",
    "        for layer_num in range(len(layer_indices)-1):\n",
    "            x = self.vgg_layer_forward(x, layer_indices[layer_num:layer_num+2])\n",
    "            out[f'pool{layer_num+1}'] = x\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The `forward` method should\n",
    "        - Feed input through the backbone to get `pool5` (given)\n",
    "        - On top of `pool5`, perform pixel-wise classification.\n",
    "        - Upsample the prediction to create segmentation on the original image resolution level.\n",
    "        \"\"\"\n",
    "\n",
    "        # padding for aligning to the input size\n",
    "        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n",
    "        vgg_features = self.vgg_forward(padded_x)\n",
    "        vgg_pool5 = vgg_features['pool5'].detach()\n",
    "        vgg_pool4 = vgg_features['pool4'].detach()\n",
    "        vgg_pool3 = vgg_features['pool3'].detach()\n",
    "\n",
    "        #################################\n",
    "        ## P1(e). Write your code here\n",
    "        ## Add more lines as you wish.\n",
    "        \n",
    "\n",
    "        out = self.dropout(self.relu(self.conv6(vgg_pool5)))\n",
    "        out = self.dropout(self.relu(self.conv7(out)))\n",
    "        out = self.score_x32(out)\n",
    "        out = self.deconv1(out)\n",
    "        \n",
    "        # crop\n",
    "        out = out[..., 9:9+x.shape[-2], 9:9+x.shape[-1]]\n",
    "              \n",
    "        #################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    # initialize transdeconv layer with bilinear upsampling.\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGmEKR5gN5h"
   },
   "source": [
    "## P1 Tests\n",
    "\n",
    "This section tests your solution for P1. **Please do not modify the code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN32(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (conv6): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv7): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (score_x32): Conv2d(4096, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (deconv1): ConvTranspose2d(21, 21, kernel_size=(64, 64), stride=(32, 32), padding=(16, 16), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = FCN32()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "73MHsqZKgknu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST  1/13 Passed] FCN32.__init__ executed without errors\n",
      "[TEST  2/13 Passed] FCN32.conv6 and FCN32.conv7 found\n",
      "[TEST  3/13 Passed] FCN32.conv6 and FCN32.conv7 weights and biases have correct shapes\n",
      "[TEST  4/13 Passed] FCN32.score_x32 found\n",
      "[TEST  5/13 Passed] FCN32.score_x32 weights and biases have correct shapes\n",
      "[TEST  6/13 Passed] FCN32.deconv1 found\n",
      "[TEST  7/13 Passed] FCN32.deconv1 weights and biases have correct shapes and configurations\n",
      "[TEST  8/13 Passed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) executed without errors\n",
      "[TEST  9/13 Passed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) is correct\n",
      "[TEST 10/13 Passed] FCN32.forward executed without errors\n",
      "[TEST 11/13 Failed] FCN32.forward output shape is incorrect\n",
      "[TEST 12/13 Failed] FCN32.forward output mean and std are incorrect; (2.9801127910614014, 0.4679467976093292)\n",
      "[TEST 13/13 Failed] FCN32.forward conv6.weight gradient mean and std are incorrect; (0.00033379741944372654, 0.0008317517349496484)\n",
      "\n",
      "[TEST] 10/13 tests passed.\n"
     ]
    }
   ],
   "source": [
    "def run_tests_p1():\n",
    "    n_pass, n_test = 0, 13\n",
    "\n",
    "    try:\n",
    "        net = FCN32()\n",
    "        print(f\"[TEST  1/{n_test} Passed] FCN32.__init__ executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  1/{n_test} Failed] FCN32.__init__ execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "        net = nn.Identity()\n",
    "\n",
    "    # P1(a). Declare convolutional layers as an alternative to FC layers\n",
    "    try:\n",
    "        assert hasattr(net, 'conv6') and hasattr(net, 'conv7')\n",
    "        print(f\"[TEST  2/{n_test} Passed] FCN32.conv6 and FCN32.conv7 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:\n",
    "        print(f\"[TEST  2/{n_test} Failed] FCN32.conv6 or FCN32.conv7 not found\")\n",
    "        net.conv6 = nn.Identity()\n",
    "        net.conv7 = nn.Identity()\n",
    "    try:\n",
    "        assert net.conv6.weight.data.shape == torch.Size([4096, 512, 7, 7]) and net.conv6.bias.data.shape == torch.Size([4096])\n",
    "        assert net.conv7.weight.data.shape == torch.Size([4096, 4096, 1, 1]) and net.conv7.bias.data.shape == torch.Size([4096])\n",
    "        print(f\"[TEST  3/{n_test} Passed] FCN32.conv6 and FCN32.conv7 weights and biases have correct shapes\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  3/{n_test} Failed] Some of the FCN32.conv6 and FCN32.conv7 weights and biases have incorrect shapes\")\n",
    "\n",
    "    # P1(b). Declare pixel-wise classification layer using 1x1 convolution\n",
    "    try:\n",
    "        assert hasattr(net, 'score_x32')\n",
    "        print(f\"[TEST  4/{n_test} Passed] FCN32.score_x32 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:\n",
    "        print(f\"[TEST  4/{n_test} Failed] FCN32.score_x32 not found\")\n",
    "        net.score_x32 = nn.Identity()\n",
    "    try:\n",
    "        assert net.score_x32.weight.data.shape == torch.Size([21, 4096, 1, 1]) and net.score_x32.bias.data.shape == torch.Size([21])\n",
    "        print(f\"[TEST  5/{n_test} Passed] FCN32.score_x32 weights and biases have correct shapes\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  5/{n_test} Failed] Some of the FCN32.score_x32 weights and biases have incorrect shapes\")\n",
    "\n",
    "    # P1(c). Declare a deconvolution layer\n",
    "    try:\n",
    "        assert hasattr(net, 'deconv1')\n",
    "        print(f\"[TEST  6/{n_test} Passed] FCN32.deconv1 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:\n",
    "        print(f\"[TEST  6/{n_test} Failed] FCN32.deconv1 not found\")\n",
    "        net.deconv1 = nn.Identity()\n",
    "    try:\n",
    "        assert net.deconv1.weight.data.shape == torch.Size([21, 21, 64, 64]) and net.deconv1.bias is None\n",
    "        print(f\"[TEST  7/{n_test} Passed] FCN32.deconv1 weights and biases have correct shapes and configurations\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  7/{n_test} Failed] Some of the FCN32.deconv1 weights and biases have incorrect shapes or configurations\")\n",
    "\n",
    "    # P1(d). Load pretrained weights\n",
    "    pretrained_model = VGG(make_layers(cfg['vgg16']))\n",
    "    fc6, fc7 = pretrained_model.classifier[0], pretrained_model.classifier[3]\n",
    "    try:\n",
    "        net.load_pretrained(pretrained_model)\n",
    "        print(f\"[TEST  8/{n_test} Passed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  8/{n_test} Failed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "    try:\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        x = torch.randn(16, 512, 7, 7)\n",
    "        x_conv6 = net.conv6(x)\n",
    "        x_conv7 = net.conv7(x_conv6)\n",
    "        x_fc6 = fc6(x.view(16, -1))\n",
    "        x_fc7 = fc7(x_fc6)\n",
    "        assert (x_conv6 - x_fc6.view(16, 4096, 1, 1)).abs().max() <= 1e-4 and (x_conv7 - x_fc7.view(16, 4096, 1, 1)).abs().max() <= 1e-4\n",
    "        print(f\"[TEST  9/{n_test} Passed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) is correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  9/{n_test} Failed] FCN32.load_pretrained convolutionization (conv6 <- fc6 and conv7 <- fc7) is incorrect\")\n",
    "\n",
    "    # P1(e). Implement forward method\n",
    "    net.eval()\n",
    "    x = train_dataset[100][0].unsqueeze(0)\n",
    "    try:\n",
    "        pred = net(x)\n",
    "        print(f\"[TEST 10/{n_test} Passed] FCN32.forward executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 10/{n_test} Failed] FCN32.forward execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "    try:\n",
    "        pred = net(x)\n",
    "        assert pred.shape == torch.Size([1, 21, x.shape[-2], x.shape[-1]])\n",
    "        print(f\"[TEST 11/{n_test} Passed] FCN32.forward output shape is correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 11/{n_test} Failed] FCN32.forward output shape is incorrect\")\n",
    "    try:\n",
    "        net.score_x32.weight.data.fill_(0.01)\n",
    "        net.score_x32.bias.data.fill_(0.01)\n",
    "        pred = net(x)\n",
    "        assert (pred.abs().mean() - (2.5568814277648926)).abs() <= 1e-5 and (pred.abs().std() - 2.5568814277648926) <= 1e-5\n",
    "        print(f\"[TEST 12/{n_test} Passed] FCN32.forward output mean and std are correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 12/{n_test} Failed] FCN32.forward output mean and std are incorrect; {(pred.abs().mean().item(), pred.abs().std().item())}\")\n",
    "    try:\n",
    "        net.score_x32.weight.data.fill_(0.01)\n",
    "        net.score_x32.bias.data.fill_(0.01)\n",
    "        pred = net(x)\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        rand_loss = (pred - torch.randn_like(pred)).mean()\n",
    "        net.zero_grad()\n",
    "        rand_loss.backward()\n",
    "        assert (net.conv6.weight.grad.abs().mean() - (0.0002854169288184494)).abs() <= 1e-8 and (net.conv6.weight.grad.abs().std() - 0.0006842449656687677) <= 1e-8\n",
    "        print(f\"[TEST 13/{n_test} Passed] FCN32.forward conv6.weight gradient mean and std are correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 13/{n_test} Failed] FCN32.forward conv6.weight gradient mean and std are incorrect; {(net.conv6.weight.grad.abs().mean().item(), net.conv6.weight.grad.abs().std().item())}\")\n",
    "\n",
    "    if n_pass == n_test:\n",
    "        print(f\"\\n[TEST] 🎉🎉🥳 All {n_pass}/{n_test} tests passed!\")\n",
    "    else:\n",
    "        print(f\"\\n[TEST] {n_pass}/{n_test} tests passed.\")\n",
    "\n",
    "\n",
    "run_tests_p1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNaw-IdOm0be"
   },
   "source": [
    "## P2. Implement FCN8 [(Illustration)](https://docs.google.com/drawings/d/1C5bbCgm6Wm_FEw_colmNvzI-feSZBffqtCcXP4hgRj0/edit?usp=sharing)\n",
    "This section **builds on top of** P1. Make sure you finished your implementation of FCN32 first.\n",
    "\n",
    "### (a) Declare convolutional layers as an alternative to FC layers\n",
    "The VGG16 backbone contains two fully connected layers (fc6, fc7). In fully convolutional network, these fc layers will be convolutionized. You should define proper convolutional layers.\n",
    "**Note**: If a layer inherited from `FCN32` is used the same way in `FCN8`, then there's no need to re-declare those layers.\n",
    "\n",
    "### (b) Declare pixel-wise classification layer using 1x1 convolution\n",
    "This section declares **three** 1x1 convolution layers that produce classification per pixel on different resolutions. Details are provided in illustration.\n",
    "\n",
    "### (c) Declare deconvolution layers\n",
    "Create **three** learnable upsampling layers. Details are provided in illustration.\n",
    "\n",
    "### (d) Implement skip connection and `forward` method\n",
    "The `forward` method should\n",
    "- Feed input through the backbone to get `pool5` (given)\n",
    "- perform pixel-wise classification on three levels (`vgg_pool[3~5]`).\n",
    "- Upsample the prediction of `pool5`, add a skip connection from the prediction of `pool4`.\n",
    "- Upsample the prediction of `pool4` and `pool5` combined, add a skip connection from the prediction of `pool3`.\n",
    "- Upsample the prediction of `pool[3~5]` combined to create segmentation on the input image resolution level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "15ONw36JnG1j"
   },
   "outputs": [],
   "source": [
    "class FCN8(FCN32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.numclass = 21\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "        # fc layers in vgg are all converted into conv layers.\n",
    "        #################################\n",
    "        ## P2(a). Write your code here ##\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, stride=1, padding=0)\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1, stride=1, padding=0)\n",
    "        #################################\n",
    "        # self.conv6 =\n",
    "        # self.conv7 =\n",
    "        #################################\n",
    "\n",
    "        # prediction layers with 1x1 convolution layers.\n",
    "        #################################\n",
    "        ## P2(b). Write your code here ##\n",
    "        self.score_x32 = nn.Conv2d(4096, self.numclass, kernel_size=1, stride=1, padding=0)\n",
    "        self.score_x16 = nn.Conv2d(512, self.numclass, kernel_size=1, stride=1, padding=0)\n",
    "        self.score_x8 = nn.Conv2d(256, self.numclass, kernel_size=1, stride=1, padding=0)\n",
    "        # self.score_x32 =\n",
    "        # self.score_x16 =\n",
    "        # self.score_x8 =\n",
    "        #################################\n",
    "\n",
    "        # learnable upsampling layers in FCN model.\n",
    "        #################################\n",
    "        ## P2(c). Write your code here ##\n",
    "        self.deconv1 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=64, stride=32, padding=16, output_padding=0, bias=False)\n",
    "        self.deconv2 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=32, stride=16, padding=8, output_padding=0, bias=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=16, stride=8, padding=4, output_padding=0, bias=False)\n",
    "        # self.deconv1 =\n",
    "        # self.deconv2 =\n",
    "        # self.deconv3 =\n",
    "        #################################\n",
    "\n",
    "        # initialize\n",
    "        self._initialize_weights() # initialize upsampling weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Padding for aligning to the input size\n",
    "        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n",
    "        vgg_features = self.vgg_forward(padded_x)\n",
    "        vgg_pool5 = vgg_features['pool5'].detach()\n",
    "        vgg_pool4 = vgg_features['pool4'].detach()\n",
    "        vgg_pool3 = vgg_features['pool3'].detach()\n",
    "\n",
    "        #################################\n",
    "        ## P2(d). Write your code here ##\n",
    "        ## Add more lines as you wish. ##\n",
    "        # out =\n",
    "        out = self.dropout(self.relu(self.conv6(vgg_pool5)))\n",
    "        out = self.dropout(self.relu(self.conv7(out)))\n",
    "        out = self.score_x32(out)\n",
    "        out = self.deconv1(out)\n",
    "        out = out + self.score_x16(vgg_pool4)\n",
    "        out = self.deconv2(out)\n",
    "        out = out + self.score_x8(vgg_pool3)\n",
    "        out = self.deconv3(out)\n",
    "        out = out[..., 9:9+x.shape[-2], 9:9+x.shape[-1]]\n",
    "        \n",
    "        #################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MJ6SN3cQEzR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST  1/11 Passed] FCN8.__init__ executed without errors\n",
      "[TEST  2/11 Passed] FCN8.conv6 and FCN8.conv7 found\n",
      "[TEST  3/11 Passed] FCN8.conv6 and FCN8.conv7 weights and biases have correct shapes\n",
      "[TEST  4/11 Passed] FCN8.score_x32, x16, x8 found\n",
      "[TEST  5/11 Passed] FCN8.score_x32, x16, x8 weights and biases have correct shapes\n",
      "[TEST  6/11 Passed] FCN8.deconv1, deconv2, deconv3 found\n",
      "[TEST  7/11 Failed] Some of the FCN8.deconv weights and biases have incorrect shapes or configurations\n",
      "[TEST  8/11 Failed] FCN8.forward execution error; please see the traceback below\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_871191/278826620.py\", line 82, in run_tests_p2\n",
      "    pred = net(x)\n",
      "  File \"/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_871191/1772161199.py\", line 61, in forward\n",
      "    out = out + self.score_x16(vgg_pool4)\n",
      "RuntimeError: The size of tensor a (480) must match the size of tensor b (43) at non-singleton dimension 3\n",
      "\n",
      "[TEST  9/11 Failed] FCN8.forward output shape is incorrect\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pred' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 98\u001b[0m, in \u001b[0;36mrun_tests_p2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m net\u001b[38;5;241m.\u001b[39mscore_x32\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (pred\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.855009913444519\u001b[39m))\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (pred\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.297334223985672\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n",
      "File \u001b[0;32m/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 61\u001b[0m, in \u001b[0;36mFCN8.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv1(out)\n\u001b[0;32m---> 61\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_x16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg_pool4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv2(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (480) must match the size of tensor b (43) at non-singleton dimension 3",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 125\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[TEST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_pass\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tests passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mrun_tests_p2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 103\u001b[0m, in \u001b[0;36mrun_tests_p2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     n_pass \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[TEST 10/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Failed] FCN8.forward output mean and std are incorrect; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[43mpred\u001b[49m\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;250m \u001b[39mpred\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mitem())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     net\u001b[38;5;241m.\u001b[39mscore_x32\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pred' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def run_tests_p2():\n",
    "    n_pass, n_test = 0, 11\n",
    "\n",
    "    try:\n",
    "        net = FCN8()\n",
    "        print(f\"[TEST  1/{n_test} Passed] FCN8.__init__ executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  1/{n_test} Failed] FCN8.__init__ execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "        net = nn.Identity()\n",
    "\n",
    "    # P2(a). Declare convolutional layers as an alternative to FC layers\n",
    "    try:\n",
    "        assert hasattr(net, 'conv6') and hasattr(net, 'conv7')\n",
    "        print(f\"[TEST  2/{n_test} Passed] FCN8.conv6 and FCN8.conv7 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:  \n",
    "        print(f\"[TEST  2/{n_test} Failed] FCN8.conv6 or FCN8.conv7 not found\")\n",
    "        net.conv6 = nn.Identity()\n",
    "        net.conv7 = nn.Identity()\n",
    "    try:\n",
    "        assert net.conv6.weight.data.shape == torch.Size([4096, 512, 7, 7]) and net.conv6.bias.data.shape == torch.Size([4096])\n",
    "        assert net.conv7.weight.data.shape == torch.Size([4096, 4096, 1, 1]) and net.conv7.bias.data.shape == torch.Size([4096])\n",
    "        print(f\"[TEST  3/{n_test} Passed] FCN8.conv6 and FCN8.conv7 weights and biases have correct shapes\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  3/{n_test} Failed] Some of the FCN8.conv6 and FCN8.conv7 weights and biases have incorrect shapes\")\n",
    "\n",
    "    # P2(b). Declare pixel-wise classification layer using 1x1 convolution\n",
    "    try:\n",
    "        assert hasattr(net, 'score_x32')\n",
    "        assert hasattr(net, 'score_x16')\n",
    "        assert hasattr(net, 'score_x8')\n",
    "        print(f\"[TEST  4/{n_test} Passed] FCN8.score_x32, x16, x8 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:\n",
    "        print(f\"[TEST  4/{n_test} Failed] FCN8.score_x32 or x16 or x8 not found\")\n",
    "        net.score_x32 = nn.Identity()\n",
    "        net.score_x16 = nn.Identity()\n",
    "        net.score_x8 = nn.Identity()\n",
    "    try:\n",
    "        assert net.score_x32.weight.data.shape == torch.Size([21, 4096, 1, 1]) and net.score_x32.bias.data.shape == torch.Size([21])\n",
    "        assert net.score_x16.weight.data.shape == torch.Size([21, 512, 1, 1]) and net.score_x32.bias.data.shape == torch.Size([21])\n",
    "        assert net.score_x8.weight.data.shape == torch.Size([21, 256, 1, 1]) and net.score_x32.bias.data.shape == torch.Size([21])\n",
    "        print(f\"[TEST  5/{n_test} Passed] FCN8.score_x32, x16, x8 weights and biases have correct shapes\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  5/{n_test} Failed] Some of the FCN8.score_x32, x16, x8 weights and biases have incorrect shapes\")\n",
    "\n",
    "    # P2(c). Declare a deconvolution layer\n",
    "    try:\n",
    "        assert hasattr(net, 'deconv1')\n",
    "        assert hasattr(net, 'deconv2')\n",
    "        assert hasattr(net, 'deconv3')\n",
    "        print(f\"[TEST  6/{n_test} Passed] FCN8.deconv1, deconv2, deconv3 found\")\n",
    "        n_pass += 1\n",
    "    except AssertionError:\n",
    "        if not hasattr(net, 'deconv1'):\n",
    "          print(f\"[TEST  6/{n_test} Failed] FCN8.deconv1 not found\")\n",
    "          net.deconv1 = nn.Identity()\n",
    "        if not hasattr(net, 'deconv2'):\n",
    "          print(f\"[TEST  6/{n_test} Failed] FCN8.deconv2 not found\")\n",
    "          net.deconv2 = nn.Identity()\n",
    "        if not hasattr(net, 'deconv3'):\n",
    "          print(f\"[TEST  6/{n_test} Failed] FCN8.deconv3 not found\")\n",
    "          net.deconv3 = nn.Identity()\n",
    "    try:\n",
    "        assert net.deconv1.weight.data.shape == torch.Size([21, 21, 4, 4]) and net.deconv1.bias is None\n",
    "        assert net.deconv2.weight.data.shape == torch.Size([21, 21, 4, 4]) and net.deconv2.bias is None\n",
    "        assert net.deconv3.weight.data.shape == torch.Size([21, 21, 16, 16]) and net.deconv3.bias is None\n",
    "        print(f\"[TEST  7/{n_test} Passed] FCN8.deconv1 weights and biases have correct shapes and configurations\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  7/{n_test} Failed] Some of the FCN8.deconv weights and biases have incorrect shapes or configurations\")\n",
    "\n",
    "    # P2(d). Implement forward method\n",
    "    net.eval()\n",
    "    x = train_dataset[100][0].unsqueeze(0)\n",
    "    # TODO: change it to FCN8\n",
    "    try:\n",
    "        pred = net(x)\n",
    "        print(f\"[TEST  8/{n_test} Passed] FCN8.forward executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  8/{n_test} Failed] FCN8.forward execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "    try:\n",
    "        pred = net(x)\n",
    "        assert pred.shape == torch.Size([1, 21, x.shape[-2], x.shape[-1]])\n",
    "        print(f\"[TEST  9/{n_test} Passed] FCN8.forward output shape is correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  9/{n_test} Failed] FCN8.forward output shape is incorrect\")\n",
    "    try:\n",
    "        net.score_x32.weight.data.fill_(0.01)\n",
    "        net.score_x32.bias.data.fill_(0.01)\n",
    "        pred = net(x)\n",
    "        assert (pred.abs().mean() - (0.855009913444519)).abs() <= 1e-5 and (pred.abs().std() - 0.297334223985672) <= 1e-5\n",
    "        print(f\"[TEST 10/{n_test} Passed] FCN8.forward output mean and std are correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 10/{n_test} Failed] FCN8.forward output mean and std are incorrect; {(pred.abs().mean().item(), pred.abs().std().item())}\")\n",
    "    try:\n",
    "        net.score_x32.weight.data.fill_(0.01)\n",
    "        net.score_x32.bias.data.fill_(0.01)\n",
    "        pred = net(x)\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        rand_loss = (pred - torch.randn_like(pred)).mean()\n",
    "        net.zero_grad()\n",
    "        rand_loss.backward()\n",
    "        assert (net.conv6.weight.grad.abs().mean() - (0.0002523009025026113)).abs() <= 1e-8 and (net.conv6.weight.grad.abs().std() - 0.0006021885783411562) <= 1e-8\n",
    "        print(f\"[TEST 11/{n_test} Passed] FCN8.forward conv6.weight gradient mean and std are correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST 11/{n_test} Failed] FCN8.forward conv6.weight gradient mean and std are incorrect; {(net.conv6.weight.grad.abs().mean().item(), net.conv6.weight.grad.abs().std().item())}\")\n",
    "\n",
    "    if n_pass == n_test:\n",
    "        print(f\"\\n[TEST] 🎉🎉🥳 All {n_pass}/{n_test} tests passed!\")\n",
    "    else:\n",
    "        print(f\"\\n[TEST] {n_pass}/{n_test} tests passed.\")\n",
    "\n",
    "\n",
    "run_tests_p2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz3cbKImnMnh"
   },
   "source": [
    "---\n",
    "# Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKe_RQ9Zm3Ts"
   },
   "source": [
    "## P3. Implement training pipeline\n",
    "\n",
    "This section contains 2 problems.\n",
    "### (a) Forward/Backward step for training\n",
    "\n",
    "- Feed the image through the model.\n",
    "- Perform a gradient step based on the loss. Loss can be calculated using `criterion`, located at the beginning of the function.\n",
    "- Choose the highest logit per pixel as prediction.\n",
    "\n",
    "### (b) Forward step for validation\n",
    "- Feed the image through the model.\n",
    "- Calculate loss on current image.\n",
    "- Choose the highest logit per pixel as prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrGDOpkAnQMh"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def get_prediction(criterion, net, image, label):\n",
    "    #####################################################\n",
    "    ## P3. Run the model\n",
    "    ## Add more lines as you wish.\n",
    "    # output: feed foward.\n",
    "    # loss: calculate loss as scalar. (Aggregate with .mean()!)\n",
    "    # pred: choose the label with highest logit in each pixel.\n",
    "    #####################################################\n",
    "\n",
    "    # output =\n",
    "    # loss =\n",
    "    # pred =\n",
    "    return output, loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI9ys6k3NhXr"
   },
   "outputs": [],
   "source": [
    "def train_net(net, resume=False):\n",
    "    # 21 is the index for boundaries: therefore we ignore this index.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=21, reduction='none')\n",
    "    colorize = Colorize(21, get_color_map())\n",
    "    best_valid_iou = 0\n",
    "\n",
    "    if resume:\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        best_valid_iou = checkpoint['best_valid_iou']\n",
    "        print(f'Resume training from epoch {epoch+1}')\n",
    "    else:\n",
    "        epoch = 0\n",
    "\n",
    "    while epoch < args.epoch:\n",
    "        t1 = time.time()\n",
    "        saved_images, saved_labels = [], []\n",
    "\n",
    "        # start training\n",
    "        net.train()\n",
    "\n",
    "        loss_total = 0\n",
    "        ious = []\n",
    "        pixel_accs = []\n",
    "\n",
    "        for batch_idx, (image, label) in tqdm.tqdm(enumerate(train_loader)):\n",
    "            # save images for visualization.\n",
    "            if len(saved_images) < 4:\n",
    "                saved_images.append(image.cpu())\n",
    "                saved_labels.append(add_padding(label.cpu()))\n",
    "\n",
    "            # move variables to gpu.\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            #################################\n",
    "            ## P3-1. Run the model (Training)    ##\n",
    "            # Write your code here\n",
    "            # output, loss, pred =\n",
    "\n",
    "\n",
    "            #################################\n",
    "            # backward pass and update optimizer\n",
    "            # Write your code here\n",
    "\n",
    "\n",
    "            #################################\n",
    "            # update total loss.\n",
    "            # Write your code here\n",
    "            # loss_total +=\n",
    "\n",
    "\n",
    "            #################################\n",
    "            # target\n",
    "            target = label.squeeze(1).cpu().numpy()\n",
    "\n",
    "            # calculate pixel accuarcy and mean IoU\n",
    "            acc, mean_iu = label_accuracy_score(target, pred.cpu().detach().numpy(), n_class=21)\n",
    "\n",
    "            pixel_accs.append(acc)\n",
    "            ious.append(mean_iu)\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch : {epoch} || {batch_idx}/{len(train_loader)} || loss : {loss.item():.3f}, iou : {mean_iu * 100:.3f} pixel_acc : {acc * 100:.3f}')\n",
    "                writer.add_scalar('train_loss_step', loss.item(), batch_idx + epoch * len(train_loader))\n",
    "                writer.add_scalar('pixel_acc_step', acc, batch_idx + epoch * len(train_loader))\n",
    "                writer.add_scalar('mean_iou_step', mean_iu, batch_idx + epoch * len(train_loader))\n",
    "\n",
    "        # calculate average IoU\n",
    "        total_ious = np.array(ious).T\n",
    "        total_ious = np.nanmean(total_ious).mean()\n",
    "        total_pixel_acc = np.array(pixel_accs).mean()\n",
    "\n",
    "        writer.add_scalar('train_loss', loss_total / len(train_loader), epoch)\n",
    "        writer.add_scalar('pixel_acc', total_pixel_acc, epoch)\n",
    "        writer.add_scalar('mean_iou', total_ious, epoch)\n",
    "\n",
    "        # image visualization\n",
    "        un_norms, preds, outputs = [], [], []\n",
    "        for image, label in zip(saved_images, saved_labels):\n",
    "            # denormalize the image.\n",
    "            image_permuted = image.permute(1, 0, 2, 3)\n",
    "            un_norm = torch.zeros_like(image_permuted)\n",
    "            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n",
    "                un_norm[idx] = (im * s) + m\n",
    "            un_norm = un_norm.permute(1, 0, 2, 3)\n",
    "            un_norms.append(add_padding(un_norm))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = net(image.to(device))\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                preds.append(add_padding(pred))\n",
    "\n",
    "        # stitch images into a grid.\n",
    "        un_norm = make_grid(torch.cat(un_norms), nrow=2)\n",
    "        label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n",
    "        pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n",
    "\n",
    "        # write images to Tensorboard.\n",
    "        writer.add_image('img', un_norm, epoch)\n",
    "        writer.add_image('gt', label, epoch)\n",
    "        writer.add_image('pred', pred, epoch)\n",
    "\n",
    "        t = time.time() - t1\n",
    "        print(f'>> Epoch : {epoch} || AVG loss : {loss_total / len(train_loader):.3f}, iou : {total_ious * 100:.3f} pixel_acc : {total_pixel_acc * 100:.3f} {t:.3f} secs')\n",
    "\n",
    "        # evaluation\n",
    "        net.eval()\n",
    "        saved_images, saved_labels = [], []\n",
    "\n",
    "        valid_loss_total = 0\n",
    "        valid_ious = []\n",
    "        valid_pixel_accs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (image, label) in tqdm.tqdm(enumerate(test_loader)):\n",
    "                # save images for visualization.\n",
    "                if len(saved_images) < 4:\n",
    "                    saved_images.append(image.cpu())\n",
    "                    saved_labels.append(add_padding(label.cpu()))\n",
    "\n",
    "                # move variables to gpu.\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                #################################\n",
    "                ## P3-2. Run the model (Validation)  ##\n",
    "                # Write your code here\n",
    "                # output, loss, pred =\n",
    "\n",
    "                #################################\n",
    "                # update total loss.\n",
    "                # valid_loss_total +=\n",
    "\n",
    "                #################################\n",
    "\n",
    "                output = output.data.cpu().numpy()\n",
    "                target = label.squeeze(1).cpu().numpy()\n",
    "\n",
    "                acc, mean_iu = label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n",
    "\n",
    "                valid_pixel_accs.append(acc)\n",
    "                valid_ious.append(mean_iu)\n",
    "\n",
    "        # calculate average IoU\n",
    "        total_valid_ious = np.array(valid_ious).T\n",
    "        total_valid_ious = np.nanmean(total_valid_ious).mean()\n",
    "        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n",
    "\n",
    "        writer.add_scalar('valid_train_loss', valid_loss_total / len(test_loader), epoch)\n",
    "        writer.add_scalar('valid_pixel_acc', total_valid_pixel_acc, epoch)\n",
    "        writer.add_scalar('valid_mean_iou', total_valid_ious, epoch)\n",
    "\n",
    "        # image visualization + CRF\n",
    "        un_norms, preds, pred_crfs, outputs = [], [], [], []\n",
    "        for image, label in zip(saved_images, saved_labels):\n",
    "            # denormalize the image.\n",
    "            image_permuted = image.permute(1, 0, 2, 3)\n",
    "            un_norm = torch.zeros_like(image_permuted)\n",
    "            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n",
    "                un_norm[idx] = (im * s) + m\n",
    "            un_norm = un_norm.permute(1, 0, 2, 3)\n",
    "            un_norms.append(add_padding(un_norm))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = net(image.to(device))\n",
    "                outputs.append(add_padding(output))\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                preds.append(add_padding(pred))\n",
    "\n",
    "            # CRF\n",
    "            output_softmax = torch.nn.functional.softmax(output, dim=1).detach().cpu()\n",
    "            un_norm_int = (un_norm * 255).squeeze().permute(1, 2, 0).numpy().astype(np.ubyte)\n",
    "            pred_crf = dense_crf(un_norm_int, output_softmax.squeeze().numpy())\n",
    "            pred_crfs.append(add_padding(torch.argmax(torch.Tensor(pred_crf), dim=0)).unsqueeze(0))\n",
    "\n",
    "        # stitch images into a grid.\n",
    "        valid_un_norm = make_grid(torch.cat(un_norms), nrow=2)\n",
    "        valid_label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n",
    "        valid_pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n",
    "        valid_pred_crf = make_grid(colorize(torch.stack(pred_crfs)), nrow=2)\n",
    "\n",
    "        # write images to tensorboard.\n",
    "        writer.add_image('valid_img', valid_un_norm, epoch)\n",
    "        writer.add_image('valid_gt', valid_label, epoch)\n",
    "        writer.add_image('valid_pred', valid_pred, epoch)\n",
    "        writer.add_image('valid_pred_crf', valid_pred_crf, epoch)\n",
    "\n",
    "        print(f'>> Epoch : {epoch} || AVG valid loss : {valid_loss_total / len(test_loader):.3f}, iou : {total_valid_ious * 100:.3f} pixel_acc : {total_valid_pixel_acc * 100:.3f} {t:.3f} secs')\n",
    "\n",
    "        # save checkpoints every epoch.\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_valid_iou': best_valid_iou\n",
    "        }\n",
    "        torch.save(checkpoint, ckpt_path)\n",
    "\n",
    "        # save best checkpoint.\n",
    "        if total_valid_ious > best_valid_iou:\n",
    "            best_valid_iou = total_valid_ious\n",
    "            torch.save(net.state_dict(), ckpt_dir / 'best.pt')\n",
    "\n",
    "        epoch += 1\n",
    "    print(f'>> Best validation set iou: {best_valid_iou}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3GWdYW7RB2H"
   },
   "outputs": [],
   "source": [
    "def run_tests_p3():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    net = FCN32()\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=21, reduction='none')\n",
    "    image, label = next(iter(test_loader))\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    n_pass, n_test = 0, 6\n",
    "\n",
    "    try:\n",
    "        output, loss, pred = get_prediction(criterion, net, image, label)\n",
    "        print(f\"[TEST  1/{n_test} Passed] get_prediction executed without errors\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  1/{n_test} Failed] get_prediction execution error; please see the traceback below\")\n",
    "        print(f\"\\n{traceback.format_exc()}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        assert output.shape == torch.Size([1, 21, image.shape[-2], image.shape[-1]])\n",
    "        print(f\"[TEST  2/{n_test} Passed] get_prediction output shape is correct\")\n",
    "        n_pass += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST  2/{n_test} Failed] get_prediction output shape is incorrect\")\n",
    "\n",
    "    try:\n",
    "        assert (output.abs().mean() - (0.010025606490671635)).abs() <= 1e-8 and (output.abs().std() - 0.00683847488835454) <= 1e-8\n",
    "        print(f\"[TEST  3/{n_test} Passed] get_prediction output mean and std are correct\")\n",
    "        n_pass += 1\n",
    "    except:\n",
    "        print(f\"[TEST  3/{n_test} Failed] get_prediction output mean and std are incorrect; {(output.abs().mean().item(), output.abs().std().item())}\")\n",
    "\n",
    "    try:\n",
    "        assert (loss - (3.0301403999328613)).abs() <= 1e-5\n",
    "        print(f\"[TEST  4/{n_test} Passed] get_prediction loss is correct\")\n",
    "        n_pass += 1\n",
    "    except:\n",
    "        print(f\"[TEST  4/{n_test} Failed] get_prediction loss is incorrect; {loss.item()}\")\n",
    "\n",
    "    try:\n",
    "        assert pred.shape == torch.Size([1, image.shape[-2], image.shape[-1]])\n",
    "        assert pred.dtype == torch.int64\n",
    "        print(f\"[TEST  5/{n_test} Passed] get_prediction pred shape and dtype is correct\")\n",
    "        n_pass += 1\n",
    "    except:\n",
    "        print(f\"[TEST  5/{n_test} Failed] get_prediction pred shape and dtype is incorrect\")\n",
    "\n",
    "    try:\n",
    "      assert (pred.float().mean() - (9.697748184204102)).abs() <= 1e-5 and (pred.float().std() - 7.738012790679932) <= 1e-5\n",
    "      print(f\"[TEST  6/{n_test} Passed] get_prediction pred mean and std are correct\")\n",
    "      n_pass += 1\n",
    "    except:\n",
    "      print(f\"[TEST  6/{n_test} Failed] get_prediction pred mean and std are incorrect; {(pred.float().mean().item(), pred.float().std().item())}\")\n",
    "\n",
    "    if n_pass == n_test:\n",
    "        print(f\"\\n[TEST] 🎉🎉🥳 All {n_pass}/{n_test} tests passed!\")\n",
    "\n",
    "run_tests_p3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0758O0Fnbk2"
   },
   "source": [
    "---\n",
    "# Train models through the pipeline\n",
    "\n",
    "In this section, you will\n",
    "- Create/load directory.\n",
    "- Select which model to train.\n",
    "- Create model and optimizer.\n",
    "\n",
    "The training process will automatically save checkpoints to your Google drive after every epoch under `parent_dir`. Training could take up to 40 minutes per epoch. As we provide  pretrained weights to start with, you will only be training for 10 epochs on your own. Uncomment the lines after `# Select model here.` to choose which model to train.  \n",
    "**You must load the provided pretrained weights**, otherwise achieving reasonable performance will take much longer.  \n",
    "**If you would like to resume** from an existing `model.pt`, then\n",
    "- Comment out the line below `Load pretrained weights here.`,\n",
    "- Specify `parent_dir` as instructed,\n",
    "- Run the first code cell again, then run `train_net` with `resume=True` parameter.  \n",
    "\n",
    "<font color=\"red\">Do not terminate your process right after an epoch has finished.</font> Writing the saved model back to Google drive will take an extra couple of minutes, and aborting in the middle will likely ruin your checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAs5ugisnfAA"
   },
   "outputs": [],
   "source": [
    "num_trial=0\n",
    "result_dir= Path(root) / 'results'\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "\n",
    "# modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n",
    "# parent_dir = result_dir / 'trial_99'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "log_dir = parent_dir\n",
    "ckpt_dir = parent_dir\n",
    "ckpt_path = parent_dir / 'model.pt'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# load pretrained weights.\n",
    "pretrained_path = Path(root) / 'pretrained_vgg.pt'\n",
    "pretrained_VGG = vgg16().to(device)\n",
    "pretrained_VGG.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "\n",
    "# select model here.\n",
    "#####################################\n",
    "model = FCN8().to(device)\n",
    "# model = FCN32().to(device)\n",
    "#####################################\n",
    "\n",
    "# load pretrained weights here.\n",
    "model.load_pretrained(pretrained_VGG)\n",
    "\n",
    "# define optimizer.\n",
    "# according to FCN paper, we doubled the learning rate of bias compared to that of weight.\n",
    "optimizer = SGD([{'params': get_parameters(model, True), 'lr': args.lr * 2, 'weight_decay': 0},\n",
    "                 {'params': get_parameters(model, False)}],\n",
    "                lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5UErfucnhSy"
   },
   "outputs": [],
   "source": [
    "train_net(model, resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLSgFytb11DV"
   },
   "source": [
    "# Aggregating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxzyms5sZWvx"
   },
   "source": [
    "After you've trained FCN32 or FCN8, load your best models and run the following block to check validation accuracy, and compare IoU improvements made by CRF. Since the validation set contains nearly 3,000 images, this will take up to 30 minutes.\n",
    "\n",
    "You can regard that your implementation is correct if performance is in ± 2%p (pixel accuracy), 0.01 (mIoU) of the following values:\n",
    "- FCN32\n",
    "  - Without CRF: 88% pixel accuracy, 0.596 mIoU\n",
    "- FCN8\n",
    "  - Without CRF: 89% pixel accuracy, 0.614 mIoU.  \n",
    "\n",
    "The exact values are subject to change, don't worry too much if you missed the range by a small margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pbH7_yA12Qz"
   },
   "outputs": [],
   "source": [
    "# specify path to your best trained model.\n",
    "# for example if you want to FCN32 load from folder 'trial_5', modify 'trial_99' into 'trial_5'.\n",
    "FCN32_path = result_dir / 'trial_1' / 'best.pt'\n",
    "FCN8_path = result_dir / 'trial_5' / 'best.pt'\n",
    "\n",
    "# OPTIONAL: Read text below this code cell.\n",
    "use_crf = False\n",
    "\n",
    "model_list = []\n",
    "# select model here.\n",
    "#####################################\n",
    "model1 = FCN32().to(device)\n",
    "model1.load_state_dict(torch.load(FCN32_path, map_location=device))\n",
    "model_list.append(model1)\n",
    "# model2 = FCN8().to(device)\n",
    "# model2.load_state_dict(torch.load(FCN8_path, map_location=device))\n",
    "# model_list.append(model2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=21)\n",
    "colorize = Colorize(21, get_color_map())\n",
    "\n",
    "for net in model_list:\n",
    "    net.eval()\n",
    "\n",
    "    valid_loss_total = 0\n",
    "    valid_ious = []\n",
    "    valid_pixel_accs = []\n",
    "    valid_ious_crf = []\n",
    "    valid_pixel_accs_crf = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(tqdm.tqdm(test_loader, desc=f\"Testing {type(net).__name__}\")):\n",
    "            # Move variables to gpu.\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output, loss, pred = get_prediction(criterion, net, image, label)\n",
    "            # CRF for some images.\n",
    "            if use_crf:\n",
    "                image_permuted = image.cpu().permute(1, 0, 2, 3)\n",
    "                un_norm = torch.zeros_like(image_permuted)\n",
    "                for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n",
    "                    un_norm[idx] = (im * s) + m\n",
    "                un_norm = un_norm.permute(1, 0, 2, 3)\n",
    "\n",
    "                output_softmax = torch.nn.functional.softmax(output, dim=1).detach().cpu()\n",
    "                un_norm_int = (un_norm * 255).squeeze().permute(1, 2, 0).numpy().astype(np.ubyte)\n",
    "                pred_crf = dense_crf(un_norm_int, output_softmax.squeeze().numpy())\n",
    "                pred_crf = np.expand_dims(np.argmax(pred_crf, 0), 0)\n",
    "\n",
    "                target = label.squeeze(1).cpu().numpy()\n",
    "                acc_crf, mean_iu_crf = label_accuracy_score(target, pred_crf, n_class=21)\n",
    "                valid_pixel_accs_crf.append(acc_crf)\n",
    "                valid_ious_crf.append(mean_iu_crf)\n",
    "\n",
    "            target = label.squeeze(1).cpu().numpy()\n",
    "            acc, mean_iu = label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n",
    "\n",
    "            # update total loss.\n",
    "            valid_loss_total += loss.item()\n",
    "\n",
    "            valid_pixel_accs.append(acc)\n",
    "            valid_ious.append(mean_iu)\n",
    "\n",
    "\n",
    "        # calculate average IoU\n",
    "        total_valid_ious = np.array(valid_ious).T\n",
    "        total_valid_ious = np.nanmean(total_valid_ious).mean()\n",
    "        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n",
    "\n",
    "        print(f'{type(net).__name__}:')\n",
    "        print(f'Pixel accuracy: {total_valid_pixel_acc * 100:.3f}, mIoU: {total_valid_ious:.3f}')\n",
    "\n",
    "        if use_crf:\n",
    "            total_valid_ious_crf = np.array(valid_ious_crf).T\n",
    "            total_valid_ious_crf = np.nanmean(total_valid_ious_crf).mean()\n",
    "            total_valid_pixel_acc_crf = np.array(valid_pixel_accs_crf).mean()\n",
    "            print(f'CRF Pixel accuracy: {total_valid_pixel_acc_crf * 100:.3f}, CRF mIoU: {total_valid_ious_crf:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2M0KqxoWnnY"
   },
   "source": [
    "**Optional**: One way to improve the semantic segmentation is to apply Conditional Randon Field (CRF) as post-processing. In a nutshell, CRF will constrain the labeling via penalizing different labels to similar pixels. Since the CRF works in the original image, some detailed structure information lost in the encoder can be reconstructed via this process.\n",
    "\n",
    "You can practice the CRF by setting `use_crf=True` in the above code block. Feel free to try it and see how it refines the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk4xF29CKiOb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1-PrQO5aRNGtWbx5mR-LbOJ2MNbYtMLC0",
     "timestamp": 1721816056342
    },
    {
     "file_id": "1nVcgKaXTnRdOCWL6rVtxTqN3eq3qBh_E",
     "timestamp": 1721547954206
    }
   ]
  },
  "kernelspec": {
   "display_name": ".cv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

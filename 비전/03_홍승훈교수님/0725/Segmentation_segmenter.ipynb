{"cells":[{"cell_type":"markdown","metadata":{"id":"AYXIHngPoI2W"},"source":["# Image Segmentation using Segmenter\n","---\n","TA : Jaehoon Yoo (wogns98@kaist.ac.kr)\n","\n","---\n","## Instructions\n","- In this assignment, we will perform semantic segmentation on PASCAL VOC 2011 dataset which contains 20 object categories. We use the Semantic Boundaries Dataset (SBD) as it contains more segmentation labels than the original dataset.\n","- To this end, you need to implement necessary network components, load and fine-tune the pretrained network, and report segmentation performance on the validation set.\n","- Fill in the section marked **Px.x** with the appropriate code. **You can only modify inside those areas, and not the skeleton code.**\n","- To begin, you should download this ipynb file into your own Google drive clicking `make a copy(ì‚¬ë³¸ë§Œë“¤ê¸°)`. Find the copy in your drive, change their name to `Segmentation_segmenter.ipynb`, if their names were changed to e.g. `Copy of Segmentation_segmenter.ipyb` or `Segmentation_segmenter.ipynbì˜ ì‚¬ë³¸`."]},{"cell_type":"markdown","metadata":{"id":"x1ZGXqrvlc_O"},"source":["---\n","## Prerequisite: Mount your gdrive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0HEy2Tok-2u","tags":[]},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"_Tsq4xR-liMH"},"source":["---\n","## Prerequisite: Setup the `root` directory properly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHzfVbfmloDz","tags":[]},"outputs":[],"source":["# Specify the directory path where `Segmentation.ipynb` exists.\n","# For example, if you saved `Segmentation.ipynb` in `/gdrive/MyDrive/Segmentation_segmenter` directory,\n","# then set root = '/gdrive/MyDrive/Segmentation_segmenter'\n","root = '/gdrive/MyDrive/Segmentation_segmenter'"]},{"cell_type":"markdown","metadata":{"id":"9XDVC3lJl1_H"},"source":["---\n","# Basic settings"]},{"cell_type":"markdown","metadata":{"id":"3jIiyjEGlwJ8"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGlC_yM9lvue","tags":[]},"outputs":[],"source":["import os\n","import time\n","import traceback\n","import logging\n","from easydict import EasyDict as edict\n","import numpy as np\n","from pathlib import Path\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torchvision.datasets import VOCSegmentation, SBDataset\n","from torchvision.datasets.vision import StandardTransform\n","# from torchvision.models.vgg import VGG, vgg16, make_layers\n","\n","torch.backends.cudnn.benchmark = True\n","torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n","import pydensecrf.densecrf as dcrf\n","import pydensecrf.utils as utils\n","\n","!pip install einops\n","from einops import rearrange, reduce, repeat\n","\n","!pip install timm==0.9.12\n","from timm.models.vision_transformer import vit_tiny_patch16_224, vit_tiny_patch16_384"]},{"cell_type":"markdown","metadata":{"id":"W27cdKyEmJy1"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dczwhJI1l8Kl","tags":[]},"outputs":[],"source":["# Basic settings\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","args = edict()\n","args.batch_size = 1\n","args.lr = 1e-4\n","args.momentum = 0.9\n","args.weight_decay = 5e-4\n","args.epoch = 2\n","args.tensorboard = True\n","args.gpu = True\n","\n","device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n","\n","# Create directory name.\n","result_dir = Path(root) / 'results'\n","result_dir.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"5iiGFJlxmCXW"},"source":["## Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b3_oX_2mDw1","tags":[]},"outputs":[],"source":["# Setup tensorboard.\n","if args.tensorboard:\n","    %reload_ext tensorboard\n","    %tensorboard --logdir \"/gdrive/MyDrive/{str(result_dir).replace('/gdrive/MyDrive/', '')}\" --samples_per_plugin images=100\n","else:\n","    writer = None"]},{"cell_type":"markdown","metadata":{"id":"XQZvsEeDmeZt"},"source":["---\n","# Utility functions\n","\n","Here are some utility functions that we will use throughout this assignment. You don't have to modify any of these.  \n","**Conditional Random Field (CRF)** is a technique to further improve segmentation performance, mainly focusing on better localization. Details can be found in the [DeepLab](https://arxiv.org/abs/1606.00915) paper."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUOvr2A_mf4Q","tags":[]},"outputs":[],"source":["def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        trunc_normal_(m.weight, std=0.02)\n","        if isinstance(m, nn.Linear) and m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","    elif isinstance(m, nn.LayerNorm):\n","        nn.init.constant_(m.bias, 0)\n","        nn.init.constant_(m.weight, 1.0)\n","\n","\n","class toLongTensor:\n","    \"\"\" Convert a byte tensor to a long tensor \"\"\"\n","    def __call__(self, img):\n","        output = torch.from_numpy(np.array(img).astype(np.int32)).long()\n","        output[output == 255] = 21\n","        return output\n","\n","\n","def _fast_hist(label_true, label_pred, n_class):\n","    mask = (label_true >= 0) & (label_true < n_class)\n","    hist = np.bincount(\n","        n_class * label_true[mask].astype(int) +\n","        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n","    return hist\n","\n","\n","def label_accuracy_score(label_trues, label_preds, n_class):\n","    \"\"\" Returns overall accuracy and mean IoU \"\"\"\n","    hist = np.zeros((n_class, n_class))\n","    for lt, lp in zip(label_trues, label_preds):\n","        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n","    acc = np.diag(hist).sum() / hist.sum()\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        iou = np.diag(hist) / (\n","            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n","        )\n","    mean_iou = np.nanmean(iou)\n","    return acc, mean_iou\n","\n","\n","class Colorize(object):\n","    \"\"\" Colorize the segmentation labels \"\"\"\n","    def __init__(self, n=35, cmap=None):\n","        if cmap is None:\n","            raise NotImplementedError()\n","            self.cmap = labelcolormap(n)\n","        else:\n","            self.cmap = cmap\n","        self.cmap = self.cmap[:n]\n","\n","    def preprocess(self, x):\n","        if len(x.size()) > 3 and x.size(1) > 1:\n","            # if x has a shape of [B, C, H, W],\n","            # where B and C denote a batch size and the number of semantic classe\n","            # then translate it into a shape of [B, 1, H, W]\n","            x = x.argmax(dim=1, keepdim=True).float()\n","        assert (len(x.shape) == 4) and (x.size(1) == 1), 'x should have a shape of [B, 1, H, W]'\n","        return x\n","\n","    def __call__(self, x):\n","        x = self.preprocess(x)\n","        if (x.dtype == torch.float) and (x.max() < 2):\n","            x = x.mul(255).long()\n","        color_images = []\n","        gray_image_shape = x.shape[1:]\n","        for gray_image in x:\n","            color_image = torch.ByteTensor(3, *gray_image_shape[1:]).fill_(0)\n","            for label, cmap in enumerate(self.cmap):\n","                mask = (label == gray_image[0]).cpu()\n","                color_image[0][mask] = cmap[0]\n","                color_image[1][mask] = cmap[1]\n","                color_image[2][mask] = cmap[2]\n","            color_images.append(color_image)\n","        color_images = torch.stack(color_images)\n","        return color_images\n","\n","\n","def uint82bin(n, count=8):\n","    \"\"\" Returns the binary of integer n, count refers to amount of bits \"\"\"\n","    return ''.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n","\n","\n","def get_color_map():\n","    \"\"\" Returns N color map \"\"\"\n","    N=25\n","    color_map = np.zeros((N, 3), dtype=np.uint8)\n","    for i in range(N):\n","        r, g, b = 0, 0, 0\n","        id = i\n","        for j in range(7):\n","            str_id = uint82bin(id)\n","            r = r ^ (np.uint8(str_id[-1]) << (7-j))\n","            g = g ^ (np.uint8(str_id[-2]) << (7-j))\n","            b = b ^ (np.uint8(str_id[-3]) << (7-j))\n","            id = id >> 3\n","        color_map[i, 0] = r\n","        color_map[i, 1] = g\n","        color_map[i, 2] = b\n","    color_map = torch.from_numpy(color_map)\n","    return color_map\n","\n","\n","def dense_crf(img, output_probs):\n","    \"\"\" Conditional Random Field for better segmentation\n","        Refer to https://github.com/lucasb-eyer/pydensecrf for details.\n","    \"\"\"\n","    c = output_probs.shape[0]\n","    h = output_probs.shape[1]\n","    w = output_probs.shape[2]\n","\n","    U = utils.unary_from_softmax(output_probs)\n","    U = np.ascontiguousarray(U)\n","\n","    img = np.ascontiguousarray(img)\n","\n","    d = dcrf.DenseCRF2D(w, h, c)\n","    d.setUnaryEnergy(U)\n","    d.addPairwiseGaussian(sxy=1, compat=3)\n","    d.addPairwiseBilateral(sxy=67, srgb=3, rgbim=img, compat=4)\n","\n","    Q = d.inference(10)\n","    Q = np.array(Q).reshape((c, h, w))\n","    return Q\n","\n","\n","def add_padding(img):\n","    \"\"\" Zero-pad image(or any array-like object) to 500x500. \"\"\"\n","    w, h = img.shape[-2], img.shape[-1]\n","    MAX_SIZE = w + 36\n","    IGNORE_IDX = 21\n","\n","    assert max(w, h) <= MAX_SIZE, f'both height and width should be less than {MAX_SIZE}'\n","\n","    _pad_left = (MAX_SIZE - w) // 2\n","    _pad_right = (MAX_SIZE - w + 1) // 2\n","    _pad_up = (MAX_SIZE - h) // 2\n","    _pad_down = (MAX_SIZE - h + 1) // 2\n","\n","    _pad = (_pad_up, _pad_down, _pad_left, _pad_right)\n","\n","    padding_img = transforms.Pad(_pad)\n","    padding_target = transforms.Pad(_pad, fill=IGNORE_IDX)\n","\n","    img = F.pad(img, pad=_pad)\n","    return img\n"]},{"cell_type":"markdown","metadata":{"id":"r7WMaIEVmoub"},"source":["---\n","# Define `DataLoader` for training & validation set\n","\n","If the cell below fails with error message \"Destination path `./cls` already exists\", try again with `download=False`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"do8qfYkEmtNg","tags":[]},"outputs":[],"source":["mean = [.485, .456, .406]\n","std = [.229, .224, .225]\n","\n","# define transform functions.\n","im_size = 384\n","transform_train = transforms.Compose([\n","    transforms.Resize((im_size, im_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","transform_train_target = transforms.Compose([\n","    transforms.Resize((im_size, im_size), interpolation=transforms.InterpolationMode.NEAREST),\n","    toLongTensor()\n","])\n","transform_test = transforms.Compose([\n","    transforms.Resize((im_size, im_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","transform_test_target = transforms.Compose([\n","    transforms.Resize((im_size, im_size), interpolation=transforms.InterpolationMode.NEAREST),\n","    toLongTensor()\n","])\n","\n","# define dataloader.\n","sbd_transform_train = StandardTransform(transform_train, transform_train_target)\n","sbd_transform_test = StandardTransform(transform_test, transform_test_target)\n","try:\n","    train_dataset = SBDataset(root='.', image_set='train', mode='segmentation', download=True, transforms=sbd_transform_train)\n","except:\n","    train_dataset = SBDataset(root='.', image_set='train', mode='segmentation', download=False, transforms=sbd_transform_train)\n","test_dataset = SBDataset(root='.', image_set='val', mode='segmentation', download=False, transforms=sbd_transform_test)\n","train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"OVy7Dc_rmw2A"},"source":["---\n","# Define networks"]},{"cell_type":"markdown","metadata":{"id":"Yv94fT75mxac"},"source":["## P1. Implement Attention layer [(Illustration)](https://docs.google.com/drawings/d/1HOI4QoqSACBFCeW0xVTOkn3XINBoEDNbr2xbtb9LZVY)\n","### (a) Declare q, k, v projection layers.\n","### (b) Declare output projection layer.\n","### (c) Declare dropout layers.\n","### (d) Implement forward method\n","The `forward` method should\n","- Map the input to queries, keys, and values.\n","- Compute the the inner-product between queries and keys\n","  - Multiply self.scale to the inner-product for stable training\n","- Compute the attention score by applying the softmax.\n","- Dropout the attention score\n","- Aggregate values based on the attention score\n","- Apply output projection layer\n","- Apply projection dropout.\n","- return the output and attention score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLC1mpEFm05u","tags":[]},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, hidden_size, heads, dropout):\n","        super().__init__()\n","        self.heads = heads\n","        head_dim = hidden_size // heads\n","        self.scale = head_dim ** -0.5\n","        self.attn = None\n","\n","        ################################################\n","        # Write your code here\n","        # self.q =\n","        # self.k =\n","        # self.v =\n","        # self.attn_drop =\n","        # self.proj =\n","        # self.proj_drop =\n","        ################################################\n","\n","    def forward(self, x, mask=None, debug=False):\n","        B, N, C = x.shape\n","\n","        ################################################\n","        # Write your code here.\n","        # Add more lines as you wish.\n","        # q =\n","        # k =\n","        # v =\n","        # attn =\n","        # output =\n","        ################################################\n","\n","        if debug:\n","          return q, k, v, output, attn\n","        else:\n","          return output, attn\n","\n","    @property\n","    def unwrapped(self):\n","        return self"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pKQyUAPFhKx","tags":[]},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n","        super().__init__()\n","        self.fc1 = nn.Linear(dim, hidden_dim)\n","        self.act = nn.GELU()\n","        if out_dim is None:\n","            out_dim = dim\n","        self.fc2 = nn.Linear(hidden_dim, out_dim)\n","        self.drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","    @property\n","    def unwrapped(self):\n","        return self\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, heads, mlp_dim, dropout):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.attn = Attention(dim, heads, dropout)\n","        self.mlp = FeedForward(dim, mlp_dim, dropout)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        y, attn = self.attn(self.norm1(x), mask)\n","        if return_attention:\n","            return attn\n","        x = x + y\n","        x = x + self.mlp(self.norm2(x))\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"2CGmEKR5gN5h","tags":[]},"source":["## P1 Tests\n","\n","This section tests your solution for P1. **Please do not modify the code!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73MHsqZKgknu","tags":[]},"outputs":[],"source":["@torch.no_grad()\n","def run_tests_p1():\n","    n_pass, n_test = 0, 4\n","    B, N, C = 2, 10, 192\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        net = Attention(C, 3, 0.1)\n","        print(f\"[TEST  1/{n_test} Passed] Attention.__init__ executed without errors\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  1/{n_test} Failed] Attention.__init__ execution error; please see the traceback below\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","\n","        net = nn.Identity()\n","\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        x = torch.randn(B, N, C)\n","        q, k, v, output, attn = net(x, debug=True)\n","        print(f\"[TEST  2/{n_test} Passed] Attention.forward executed without errors\")\n","        n_pass += 1\n","\n","    except Exception as e:\n","        print(f\"[TEST  2/{n_test} Failed] Attention.forward execution error;\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","        return\n","\n","    try:\n","        q_shape = k_shape = v_shape = torch.Size([2, 3, 10, 64])\n","        attn_shape = torch.Size([2, 3, 10, 10])\n","        output_shape = torch.Size([2, 10, 192])\n","\n","        not_matches = []\n","        if q.shape != q_shape:\n","            not_matches.append('q')\n","        if k.shape != k_shape:\n","            not_matches.append('k')\n","        if v.shape != v_shape:\n","            not_matches.append('v')\n","        if attn.shape != attn_shape:\n","            not_matches.append('attn')\n","        if output.shape != output_shape:\n","            not_matches.append('output')\n","        assert len(not_matches) == 0\n","        print(f\"[TEST  3/{n_test} Passed] Shape of q, k, v, attn, output are matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  3/{n_test} Failed] Shape of {not_matches} are not matched\")\n","\n","    try:\n","        q_mean, q_std = q.abs().mean(), q.abs().std()\n","        k_mean, k_std = k.abs().mean(), k.abs().std()\n","        v_mean, v_std = v.abs().mean(), v.abs().std()\n","        attn_mean, attn_std = attn.abs().mean(), attn.abs().std()\n","        output_mean, output_std = output.abs().mean(), output.abs().std()\n","        not_matches = []\n","        if ((q_mean - 0.4701).abs() > 1e-4) or ((q_std - 0.3834).abs() > 1e-4):\n","            not_matches.append('q')\n","        if ((k_mean - 0.4509).abs() > 1e-4) or ((k_std - 0.3436).abs() > 1e-4):\n","            not_matches.append('k')\n","        if ((v_mean - 0.4423).abs() > 1e-4) or ((v_std - 0.3363).abs() > 1e-4):\n","            not_matches.append('v')\n","        if ((attn_mean - 0.0999).abs() > 1e-4) or ((attn_std - 0.0498).abs() > 1e-4):\n","            not_matches.append('attn')\n","        if ((output_mean - 0.1003).abs() > 1e-4) or ((output_std - 0.0870).abs() > 1e-4):\n","            not_matches.append('output')\n","        assert len(not_matches) == 0\n","        print(f\"[TEST  4/{n_test} Passed] All values are matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  4/{n_test} Failed] Value of {not_matches} are not matched\")\n","\n","    if n_pass == n_test:\n","        print(f\"\\n[TEST] ðŸŽ‰ðŸŽ‰ðŸ¥³ All {n_pass}/{n_test} tests passed!\")\n","\n","\n","run_tests_p1()"]},{"cell_type":"markdown","metadata":{"id":"JNaw-IdOm0be"},"source":["## P2. Implement MaskTransformer [(Illustration)](https://docs.google.com/drawings/d/1TQ9lDfIimTom7df7_6ThzF9cOnedqIWN0xK8cB_-cSk)\n","\n","The MaskTransformer takes the output from the ViT encoder to predict the segmentation mask. First, the visual tokens from the ViT encoder are concatenated with class embeddings and sent to the MaskTransformer. The MaskTransformer processes these tokens, and then it calculates the segmentation mask by computing the inner product between the class embeddings and the visual tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15ONw36JnG1j","tags":[]},"outputs":[],"source":["from timm.models.layers import trunc_normal_\n","class MaskTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        n_cls=21,\n","        patch_size=16,\n","        d_encoder=192,\n","        n_layers=2,\n","        n_heads=3,\n","        d_model=192,\n","        dropout=0.1,\n","    ):\n","        super().__init__()\n","        self.d_encoder = d_encoder\n","        self.patch_size = patch_size\n","        self.n_layers = n_layers\n","        self.n_cls = n_cls\n","        self.d_model = d_model\n","        self.d_ff = d_ff = d_model * 4\n","        self.scale = d_model ** -0.5\n","\n","        d_e = d_encoder\n","        d_m = d_model\n","\n","        #######################################################################\n","        # Write your code here\n","        # self.blocks =\n","        # self.cls_emb =\n","        # self.proj_dec =\n","        # self.proj_patch =\n","        # self.proj_classes =\n","        # self.decoder_norm =\n","        # self.mask_norm =\n","        #######################################################################\n","\n","        self.apply(init_weights)\n","        self.proj_patch.weight.data = self.scale * torch.randn_like(self.proj_patch.weight.data)\n","        self.proj_classes.weight.data = self.scale * torch.randn_like(self.proj_classes.weight.data)\n","        trunc_normal_(self.cls_emb, std=0.02)\n","\n","    def forward(self, x, im_size, debug=False):\n","        H, W = im_size\n","        GS = H // self.patch_size # grid_size\n","\n","        #####################################################\n","        # Write your code here\n","        # Add more lines as you wish.\n","        # block_output =\n","        # patches =\n","        # cls_seg_feat =\n","        # masks =\n","        #####################################################\n","        if debug:\n","          return block_output, patches, cls_seg_feat, masks\n","\n","        return masks\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {\"cls_emb\"}\n"]},{"cell_type":"code","source":["class DecoderLinear(nn.Module):\n","    def __init__(self, n_cls=21, patch_size=16, d_encoder=192):\n","        super().__init__()\n","\n","        self.d_encoder = d_encoder\n","        self.patch_size = patch_size\n","        self.n_cls = n_cls\n","\n","        self.head = nn.Linear(self.d_encoder, n_cls)\n","        self.apply(init_weights)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return set()\n","\n","    def forward(self, x, im_size):\n","        H, W = im_size\n","        GS = H // self.patch_size\n","        x = self.head(x)\n","        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n","\n","        return x"],"metadata":{"id":"i61swfkPiNdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROilWt3SQAgv"},"source":["### P2 Tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MJ6SN3cQEzR","tags":[]},"outputs":[],"source":["@torch.no_grad()\n","def run_tests_p2():\n","    n_pass, n_test = 0, 4\n","    B, N, C = 2, 16, 192\n","    im_size = (64, 64)\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        net = MaskTransformer()\n","        print(f\"[TEST  1/{n_test} Passed] MaskTransformer.__init__ executed without errors\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  1/{n_test} Failed] MaskTransformer.__init__ execution error; please see the traceback below\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","        return\n","\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        x = torch.randn(B, N, C)\n","        block_output, patches, cls_seg_feat, masks = net(x, im_size, debug=True)\n","        print(f\"[TEST  2/{n_test} Passed] MaskTransformer.forward executed without errors\")\n","        n_pass += 1\n","\n","    except Exception as e:\n","        print(f\"[TEST  2/{n_test} Failed] MaskTransformer.forward execution error;\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","        return\n","\n","    b_mean, b_std = block_output.abs().mean(), block_output.abs().std()\n","    p_mean, p_std = patches.abs().mean(), patches.abs().std()\n","    c_mean, c_std = cls_seg_feat.abs().mean(), cls_seg_feat.abs().std()\n","    m_mean, m_std = masks.abs().mean(), masks.abs().std()\n","\n","    try:\n","        not_matches = []\n","        if block_output.shape != torch.Size([2, 37, 192]):\n","            not_matches.append('block_output')\n","        if patches.shape != torch.Size([2, 16, 192]):\n","            not_matches.append('patches')\n","        if cls_seg_feat.shape != torch.Size([2, 21, 192]):\n","            not_matches.append('cls_seg_feat')\n","        if masks.shape != torch.Size([2, 21, 4, 4]):\n","            not_matches.append('masks')\n","        assert len(not_matches) == 0\n","        print(f\"[TEST  3/{n_test} Passed] Shape of outputs are matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  3/{n_test} Failed] Shape of {not_matches} are not matched\")\n","\n","    try:\n","        not_matches = []\n","        if ((b_mean - 0.7917).abs() > 1e-4) or ((b_std - 0.6106).abs() > 1e-4):\n","            not_matches.append('q')\n","        if ((p_mean - 0.0576).abs() > 1e-4) or ((p_std - 0.0435).abs() > 1e-4):\n","            not_matches.append('k')\n","        if ((c_mean - 0.0577).abs() > 1e-4) or ((c_std - 0.0433).abs() > 1e-4):\n","            not_matches.append('cls_seg_feat')\n","        if ((m_mean - 0.8144).abs() > 1e-4) or ((m_std - 0.5783).abs() > 1e-4):\n","            not_matches.append('masks')\n","        assert len(not_matches) == 0\n","        print(f\"[TEST  4/{n_test} Passed] All values are matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  4/{n_test} Failed] Value of {not_matches} are not matched\")\n","\n","    if n_pass == n_test:\n","        print(f\"\\n[TEST] ðŸŽ‰ðŸŽ‰ðŸ¥³ All {n_pass}/{n_test} tests passed!\")\n","\n","\n","run_tests_p2()"]},{"cell_type":"markdown","metadata":{"id":"90UYFHU_MJO9"},"source":["# P3. Implement Segmenter [(Illustration)](https://docs.google.com/drawings/d/19914B8kWbAZrIwiFJBxkRESSN9_T5MRJ43YT6NL-SCg)\n","Segmenter class connects the ViT encoder with MaskTransformer (decoder).\n","Segmenter will get images shaped as (B, 3, H, W) and return the class logit map shaped as (B, num_cls, H, W)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SanfrEKUMNFV","tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from timm.models.layers import trunc_normal_\n","\n","class Segmenter(nn.Module):\n","    def __init__(\n","        self,\n","        n_cls=21,\n","        use_tf=True,\n","    ):\n","        super().__init__()\n","        self.n_cls = n_cls\n","        encoder = vit_tiny_patch16_384(pretrained=True)\n","        decoder = MaskTransformer() if use_tf else DecoderLinear()\n","        self.patch_size = 16\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, im):\n","        H_ori, W_ori = im.size(2), im.size(3)\n","        H, W = im.size(2), im.size(3)\n","        ############################################################\n","        # Write your code here\n","        # Add more lines as you wish.\n","        # masks =\n","        ############################################################\n","        return masks\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        def append_prefix_no_weight_decay(prefix, module):\n","            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n","\n","        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n","            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n","        )\n","        return nwd_params"]},{"cell_type":"markdown","metadata":{"id":"oBRzNiH4QAgw"},"source":["### P3 Tests"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"wxceEO3SQAgw"},"outputs":[],"source":["@torch.no_grad()\n","def run_tests_p3():\n","    n_pass, n_test = 0, 4\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        net = Segmenter()\n","        print(f\"[TEST  1/{n_test} Passed] Segmenter.__init__ executed without errors\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  1/{n_test} Failed] Segmenter.__init__ execution error; please see the traceback below\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","        return\n","\n","    try:\n","        torch.manual_seed(42)\n","        torch.cuda.manual_seed_all(42)\n","        im = torch.randn(1, 3, im_size, im_size)\n","        masks = net(im)\n","        print(f\"[TEST  2/{n_test} Passed] Segmenter.forward executed without errors\")\n","        n_pass += 1\n","\n","    except Exception as e:\n","        print(f\"[TEST  2/{n_test} Failed] Segmenter.forward execution error;\")\n","        print(f\"\\n{traceback.format_exc()}\")\n","        return\n","\n","    m_mean, m_std = masks.abs().mean(), masks.abs().std()\n","    try:\n","        assert masks.shape == torch.Size([1, 21, im_size, im_size])\n","        print(f\"[TEST  3/{n_test} Passed] Shape of mask is matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  3/{n_test} Failed] Shape of mask is not matched\")\n","\n","    try:\n","        assert ((m_mean - 0.7249).abs() <= 1e-4) and ((m_std - 0.4966).abs() <= 1e-4)\n","        print(f\"[TEST  4/{n_test} Passed] Mask logits are matched\")\n","        n_pass += 1\n","    except Exception as e:\n","        print(f\"[TEST  4/{n_test} Failed] Mask logits are not matched\")\n","\n","    if n_pass == n_test:\n","        print(f\"\\n[TEST] ðŸŽ‰ðŸŽ‰ðŸ¥³ All {n_pass}/{n_test} tests passed!\")\n","\n","run_tests_p3()"]},{"cell_type":"markdown","metadata":{"id":"Fz3cbKImnMnh"},"source":["---\n","# Training function"]},{"cell_type":"markdown","metadata":{"id":"ZKe_RQ9Zm3Ts"},"source":["## Training pipeline\n","\n","### (a) Forward/Backward step for training\n","- Feed the image through the model.\n","- Perform a gradient step based on the loss. Loss can be calculated using `criterion`, located at the beginning of the function.\n","- Choose the highest logit per pixel as prediction.\n","\n","### (b) Forward step for validation\n","- Feed the image through the model.\n","- Calculate loss on current image.\n","- Choose the highest logit per pixel as prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrGDOpkAnQMh","tags":[]},"outputs":[],"source":["import tqdm\n","\n","def get_prediction(criterion, net, image, label):\n","    output = net(image)\n","    loss = criterion(output, label).mean()\n","    pred = torch.argmax(output, dim=1)\n","    return output, loss, pred\n","\n","def train_net(net, resume=False):\n","    # 21 is the index for boundaries: therefore we ignore this index.\n","    criterion = nn.CrossEntropyLoss(ignore_index=21, reduction='none')\n","    colorize = Colorize(21, get_color_map())\n","    best_valid_iou = 0\n","\n","    if resume:\n","        checkpoint = torch.load(ckpt_path)\n","        net.load_state_dict(checkpoint['state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        epoch = checkpoint['epoch']\n","        best_valid_iou = checkpoint['best_valid_iou']\n","        print(f'Resume training from epoch {epoch+1}')\n","    else:\n","        epoch = 0\n","\n","    while epoch < args.epoch:\n","        t1 = time.time()\n","        saved_images, saved_labels = [], []\n","\n","        # start training\n","        net.train()\n","\n","        loss_total = 0\n","        ious = []\n","        pixel_accs = []\n","\n","        for batch_idx, (image, label) in tqdm.tqdm(enumerate(train_loader)):\n","            # save images for visualization.\n","            if len(saved_images) < 4:\n","                saved_images.append(image.cpu())\n","                saved_labels.append(add_padding(label.cpu()))\n","\n","            # move variables to gpu.\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            output, loss, pred = get_prediction(criterion, net, image, label)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # update total loss.\n","            loss_total += loss.item()\n","\n","            # target\n","            target = label.squeeze(1).cpu().numpy()\n","\n","            # calculate pixel accuarcy and mean IoU\n","            acc, mean_iu = label_accuracy_score(target, pred.cpu().detach().numpy(), n_class=21)\n","\n","            pixel_accs.append(acc)\n","            ious.append(mean_iu)\n","\n","            if batch_idx % 50 == 0:\n","                print(f'Epoch : {epoch} || {batch_idx}/{len(train_loader)} || loss : {loss.item():.3f}, iou : {mean_iu * 100:.3f} pixel_acc : {acc * 100:.3f}')\n","                writer.add_scalar('train_loss_step', loss.item(), batch_idx + epoch * len(train_loader))\n","                writer.add_scalar('pixel_acc_step', acc, batch_idx + epoch * len(train_loader))\n","                writer.add_scalar('mean_iou_step', mean_iu, batch_idx + epoch * len(train_loader))\n","\n","\n","        # calculate average IoU\n","        total_ious = np.array(ious).T\n","        total_ious = np.nanmean(total_ious).mean()\n","        total_pixel_acc = np.array(pixel_accs).mean()\n","\n","        writer.add_scalar('train_loss', loss_total / len(train_loader), epoch)\n","        writer.add_scalar('pixel_acc', total_pixel_acc, epoch)\n","        writer.add_scalar('mean_iou', total_ious, epoch)\n","\n","        # image visualization\n","        un_norms, preds, outputs = [], [], []\n","        for image, label in zip(saved_images, saved_labels):\n","            # denormalize the image.\n","            image_permuted = image.permute(1, 0, 2, 3)\n","            un_norm = torch.zeros_like(image_permuted)\n","            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n","                un_norm[idx] = (im * s) + m\n","            un_norm = un_norm.permute(1, 0, 2, 3)\n","            un_norms.append(add_padding(un_norm))\n","\n","            with torch.no_grad():\n","                output = net(image.to(device))\n","                pred = torch.argmax(output, dim=1)\n","                preds.append(add_padding(pred))\n","\n","        # stitch images into a grid.\n","        un_norm = make_grid(torch.cat(un_norms), nrow=2)\n","        label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n","        pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n","\n","        # write images to Tensorboard.\n","        writer.add_image('img', un_norm, epoch)\n","        writer.add_image('gt', label, epoch)\n","        writer.add_image('pred', pred, epoch)\n","\n","        t = time.time() - t1\n","        print(f'>> Epoch : {epoch} || AVG loss : {loss_total / len(train_loader):.3f}, iou : {total_ious * 100:.3f} pixel_acc : {total_pixel_acc * 100:.3f} {t:.3f} secs')\n","\n","        # evaluation\n","        net.eval()\n","        saved_images, saved_labels = [], []\n","\n","        valid_loss_total = 0\n","        valid_ious = []\n","        valid_pixel_accs = []\n","\n","        with torch.no_grad():\n","            for batch_idx, (image, label) in tqdm.tqdm(enumerate(test_loader)):\n","                # save images for visualization.\n","                if len(saved_images) < 4:\n","                    saved_images.append(image.cpu())\n","                    saved_labels.append(add_padding(label.cpu()))\n","\n","                # move variables to gpu.\n","                image = image.to(device)\n","                label = label.to(device)\n","\n","                output, loss, pred = get_prediction(criterion, net, image, label)\n","\n","                # update total loss.\n","                valid_loss_total += loss.item()\n","\n","                output = output.data.cpu().numpy()\n","                target = label.squeeze(1).cpu().numpy()\n","\n","                acc, mean_iu = label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n","\n","                valid_pixel_accs.append(acc)\n","                valid_ious.append(mean_iu)\n","\n","        # calculate average IoU\n","        total_valid_ious = np.array(valid_ious).T\n","        total_valid_ious = np.nanmean(total_valid_ious).mean()\n","        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n","\n","        writer.add_scalar('valid_train_loss', valid_loss_total / len(test_loader), epoch)\n","        writer.add_scalar('valid_pixel_acc', total_valid_pixel_acc, epoch)\n","        writer.add_scalar('valid_mean_iou', total_valid_ious, epoch)\n","\n","        # image visualization + CRF\n","        un_norms, preds, pred_crfs, outputs = [], [], [], []\n","        for image, label in zip(saved_images, saved_labels):\n","            # denormalize the image.\n","            image_permuted = image.permute(1, 0, 2, 3)\n","            un_norm = torch.zeros_like(image_permuted)\n","            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n","                un_norm[idx] = (im * s) + m\n","            un_norm = un_norm.permute(1, 0, 2, 3)\n","            un_norms.append(add_padding(un_norm))\n","\n","            with torch.no_grad():\n","                output = net(image.to(device))\n","                outputs.append(add_padding(output))\n","                pred = torch.argmax(output, dim=1)\n","                preds.append(add_padding(pred))\n","\n","            # CRF\n","            output_softmax = torch.nn.functional.softmax(output, dim=1).detach().cpu()\n","            un_norm_int = (un_norm * 255).squeeze().permute(1, 2, 0).numpy().astype(np.ubyte)\n","            pred_crf = dense_crf(un_norm_int, output_softmax.squeeze().numpy())\n","            pred_crfs.append(add_padding(torch.argmax(torch.Tensor(pred_crf), dim=0)).unsqueeze(0))\n","\n","        # stitch images into a grid.\n","        valid_un_norm = make_grid(torch.cat(un_norms), nrow=2)\n","        valid_label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n","        valid_pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n","        valid_pred_crf = make_grid(colorize(torch.stack(pred_crfs)), nrow=2)\n","\n","        # write images to tensorboard.\n","        writer.add_image('valid_img', valid_un_norm, epoch)\n","        writer.add_image('valid_gt', valid_label, epoch)\n","        writer.add_image('valid_pred', valid_pred, epoch)\n","        writer.add_image('valid_pred_crf', valid_pred_crf, epoch)\n","\n","        print(f'>> Epoch : {epoch} || AVG valid loss : {valid_loss_total / len(test_loader):.3f}, iou : {total_valid_ious * 100:.3f} pixel_acc : {total_valid_pixel_acc * 100:.3f} {t:.3f} secs')\n","\n","        # save checkpoints every epoch.\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'state_dict': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'best_valid_iou': best_valid_iou\n","        }\n","        torch.save(checkpoint, ckpt_path)\n","\n","        # save best checkpoint.\n","        if total_valid_ious > best_valid_iou:\n","            best_valid_iou = total_valid_ious\n","            torch.save(net.state_dict(), ckpt_dir / 'best.pt')\n","\n","        epoch += 1\n","    print(f'>> Best validation set iou: {best_valid_iou}')"]},{"cell_type":"markdown","metadata":{"id":"k0758O0Fnbk2"},"source":["---\n","# Train models through the pipeline\n","\n","In this section, you will\n","- Create/load directory.\n","- Select which model to train.\n","- Create model and optimizer.\n","\n","The training process will automatically save checkpoints to your Google drive after every epoch under `parent_dir`. Training could take up to 40 minutes per epoch. As we provide  pretrained weights to start with, you will only be training for 2 epochs on your own. Uncomment the lines after `# Select model here.` to choose which model to train.  \n","**You must load the provided pretrained weights**, otherwise achieving reasonable performance will take much longer.  \n","**If you would like to resume** from an existing `model.pt`, then\n","- Comment out the line below `Load pretrained weights here.`,\n","- Specify `parent_dir` as instructed,\n","- Run the first code cell again, then run `train_net` with `resume=True` parameter.  \n","\n","<font color=\"red\">Do not terminate your process right after an epoch has finished.</font> Writing the saved model back to Google drive will take an extra couple of minutes, and aborting in the middle will likely ruin your checkpoint file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAs5ugisnfAA","tags":[]},"outputs":[],"source":["num_trial=0\n","result_dir= Path(root) / 'results'\n","parent_dir = result_dir / f'trial_{num_trial}'\n","while parent_dir.is_dir():\n","    num_trial = int(parent_dir.name.replace('trial_',''))\n","    parent_dir = result_dir / f'trial_{num_trial+1}'\n","\n","# modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n","# parent_dir = result_dir / 'trial_99'\n","print(f'Logs and ckpts will be saved in : {parent_dir}')\n","\n","log_dir = parent_dir\n","ckpt_dir = parent_dir\n","ckpt_path = parent_dir / 'model.pt'\n","writer = SummaryWriter(log_dir)\n","\n","# select model here.\n","model = Segmenter(use_tf=True).cuda()\n","# model = Segmenter(use_tf=False).cuda()\n","\n","# define optimizer.\n","params = list(model.parameters())\n","nwd_names = list(model.no_weight_decay())\n","wd_params = []\n","nwd_params = []\n","for n, p in model.named_parameters():\n","    ignore = False\n","    for ign in nwd_names:\n","        if n.startswith(ign):\n","            nwd_params.append(p)\n","            ignore=True\n","            continue\n","    if not ignore:\n","        wd_params.append(p)\n","optimizer = SGD([{'params': nwd_params, 'weight_decay': 0},\n","                 {'params': wd_params}],\n","                lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5UErfucnhSy","tags":[]},"outputs":[],"source":["torch.use_deterministic_algorithms(False)\n","torch.backends.cudnn.deterministic = True\n","train_net(model, resume=False)"]},{"cell_type":"markdown","metadata":{"id":"yLSgFytb11DV"},"source":["# Aggregating Results"]},{"cell_type":"markdown","metadata":{"id":"hxzyms5sZWvx"},"source":["After you've trained Segmenter, load your best models and run the following block to check validation accuracy, and compare IoU improvements made by CRF. Since the validation set contains nearly 3,000 images, this will take up to 30 minutes.\n","\n","You can regard that your implementation is correct if performance is in Â± 2%p (pixel accuracy), 0.01 (mIoU) of the following values:\n","- Segmenter(use_tf=True)\n","  - After Epoch 0\n","    - AVG valid iou : 60.29 pixel_acc : 89.83\n","  - After Epoch 1\n","    - AVG valid iou : 65.19 pixel_acc : 90.11\n","\n","- Segmenter(use_tf=False)\n","  - After Epoch 0\n","    - AVG valid iou : 60.13 pixel_acc : 89.30\n","  - After Epoch 1\n","    - AVG valid iou : 62.21 pixel_acc : 90.51\n","\n","\n","The exact values are subject to change, don't worry too much if you missed the range by a small margin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pbH7_yA12Qz","tags":[]},"outputs":[],"source":["# specify path to your best trained model.\n","# for example if you want to load Segmenter from folder 'trial_5', modify 'trial_99' into 'trial_5'.\n","segmenter_path = result_dir / 'trial_2' / 'best.pt'\n","\n","# OPTIONAL: Read text below this code cell.\n","use_crf = False\n","\n","net = Segmenter().to(device)\n","net.load_state_dict(torch.load(segmenter_path, map_location=device))\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=21, reduction='none')\n","colorize = Colorize(21, get_color_map())\n","\n","net.eval()\n","\n","valid_loss_total = 0\n","valid_ious = []\n","valid_pixel_accs = []\n","valid_ious_crf = []\n","valid_pixel_accs_crf = []\n","\n","with torch.no_grad():\n","    for batch_idx, (image, label) in enumerate(test_loader):\n","        # Move variables to gpu.\n","        image = image.to(device)\n","        label = label.to(device)\n","\n","        output, loss, pred = get_prediction(criterion, net, image, label)\n","        # CRF for some images.\n","        if use_crf:\n","            image_permuted = image.cpu().permute(1, 0, 2, 3)\n","            un_norm = torch.zeros_like(image_permuted)\n","            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n","                un_norm[idx] = (im * s) + m\n","            un_norm = un_norm.permute(1, 0, 2, 3)\n","\n","            output_softmax = torch.nn.functional.softmax(output, dim=1).detach().cpu()\n","            un_norm_int = (un_norm * 255).squeeze().permute(1, 2, 0).numpy().astype(np.ubyte)\n","            pred_crf = dense_crf(un_norm_int, output_softmax.squeeze().numpy())\n","            pred_crf = np.expand_dims(np.argmax(pred_crf, 0), 0)\n","\n","            target = label.squeeze(1).cpu().numpy()\n","            acc_crf, mean_iu_crf = label_accuracy_score(target, pred_crf, n_class=21)\n","            valid_pixel_accs_crf.append(acc_crf)\n","            valid_ious_crf.append(mean_iu_crf)\n","\n","        target = label.squeeze(1).cpu().numpy()\n","        acc, mean_iu = label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n","\n","        # update total loss.\n","        valid_loss_total += loss.item()\n","\n","        valid_pixel_accs.append(acc)\n","        valid_ious.append(mean_iu)\n","\n","        # # this is only for testing\n","        # if batch_idx > 50:\n","        #   break\n","\n","    # calculate average IoU\n","    total_valid_ious = np.array(valid_ious).T\n","    total_valid_ious = np.nanmean(total_valid_ious).mean()\n","    total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n","\n","    print(f'{type(net).__name__}:')\n","    print(f'Pixel accuracy: {total_valid_pixel_acc * 100:.3f}, mIoU: {total_valid_ious:.3f}')\n","\n","    if use_crf:\n","        total_valid_ious_crf = np.array(valid_ious_crf).T\n","        total_valid_ious_crf = np.nanmean(total_valid_ious_crf).mean()\n","        total_valid_pixel_acc_crf = np.array(valid_pixel_accs_crf).mean()\n","        print(f'CRF Pixel accuracy: {total_valid_pixel_acc_crf * 100:.3f}, CRF mIoU: {total_valid_ious_crf:.3f}')\n"]},{"cell_type":"markdown","metadata":{"id":"D2M0KqxoWnnY"},"source":["**Optional**: One way to improve the semantic segmentation is to apply Conditional Randon Field (CRF) as post-processing. In a nutshell, CRF will constrain the labeling via penalizing different labels to similar pixels. Since the CRF works in the original image, some detailed structure information lost in the encoder can be reconstructed via this process.\n","\n","You can practice the CRF by setting `use_crf=True` in the above code block. Feel free to try it and see how it refines the labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk4xF29CKiOb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hy-e/2025-ai-expert/blob/main/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%EC%84%9C%ED%99%8D%EC%84%9D%20%EA%B5%90%EC%88%98%EB%8B%98/VQA_with_quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5qYZXURpy4M"
      },
      "source": [
        "#Code walkthrough of the pytorch implementation of the paper VQA: Visual Question Answering(ICCV 2015)\n",
        "\n",
        "![](assets/transformer1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4VPcmIyEcH"
      },
      "source": [
        "##Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RSLO3w3jyHb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import easydict\n",
        "from torch.optim import lr_scheduler\n",
        "import gc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkHTdjtSqWY7"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wC8FXg6ppvxB",
        "outputId": "9df1b8f0-6cca-4546-8e8d-c9d35bdfb30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-08-25 04:35:06--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.91.190, 16.182.37.240, 16.15.186.176, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.91.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21708861 (21M) [application/zip]\n",
            "Saving to: ‘./datasets/Annotations/v2_Annotations_Train_mscoco.zip’\n",
            "\n",
            "./datasets/Annotati 100%[===================>]  20.70M  4.50MB/s    in 4.8s    \n",
            "\n",
            "2025-08-25 04:35:12 (4.32 MB/s) - ‘./datasets/Annotations/v2_Annotations_Train_mscoco.zip’ saved [21708861/21708861]\n",
            "\n",
            "--2025-08-25 04:35:12--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.15.203.127, 16.15.179.234, 54.231.192.200, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.15.203.127|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10518930 (10M) [application/zip]\n",
            "Saving to: ‘./datasets/Annotations/v2_Annotations_Val_mscoco.zip’\n",
            "\n",
            "./datasets/Annotati 100%[===================>]  10.03M  4.17MB/s    in 2.4s    \n",
            "\n",
            "2025-08-25 04:35:15 (4.17 MB/s) - ‘./datasets/Annotations/v2_Annotations_Val_mscoco.zip’ saved [10518930/10518930]\n",
            "\n",
            "--2025-08-25 04:35:15--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.71.64, 52.216.26.150, 54.231.129.136, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.71.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7239401 (6.9M) [application/zip]\n",
            "Saving to: ‘./datasets/Questions/v2_Questions_Train_mscoco.zip’\n",
            "\n",
            "./datasets/Question 100%[===================>]   6.90M  2.94MB/s    in 2.4s    \n",
            "\n",
            "2025-08-25 04:35:19 (2.94 MB/s) - ‘./datasets/Questions/v2_Questions_Train_mscoco.zip’ saved [7239401/7239401]\n",
            "\n",
            "--2025-08-25 04:35:19--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.71.64, 54.231.129.136, 52.216.9.45, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.71.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3494929 (3.3M) [application/zip]\n",
            "Saving to: ‘./datasets/Questions/v2_Questions_Val_mscoco.zip’\n",
            "\n",
            "./datasets/Question 100%[===================>]   3.33M   695KB/s    in 5.5s    \n",
            "\n",
            "2025-08-25 04:35:25 (620 KB/s) - ‘./datasets/Questions/v2_Questions_Val_mscoco.zip’ saved [3494929/3494929]\n",
            "\n",
            "--2025-08-25 04:35:25--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.15.219.155, 52.217.172.80, 16.15.187.86, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.15.219.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8966943 (8.6M) [application/zip]\n",
            "Saving to: ‘./datasets/Questions/v2_Questions_Test_mscoco.zip’\n",
            "\n",
            "./datasets/Question 100%[===================>]   8.55M  3.55MB/s    in 2.4s    \n",
            "\n",
            "2025-08-25 04:35:30 (3.55 MB/s) - ‘./datasets/Questions/v2_Questions_Test_mscoco.zip’ saved [8966943/8966943]\n",
            "\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-7wzyJhs5ZaHLmHwqjR2-OLS4EeGyVih\n",
            "From (redirected): https://drive.google.com/uc?id=1-7wzyJhs5ZaHLmHwqjR2-OLS4EeGyVih&confirm=t&uuid=754b2799-528f-4fb0-8fed-1b98ee2c5ab4\n",
            "To: /2025-ai-expert/알고리즘/서홍석/total.zip\n",
            "100%|██████████████████████████████████████| 2.29G/2.29G [01:59<00:00, 19.2MB/s]\n",
            "Archive:  ./datasets/Annotations/v2_Annotations_Train_mscoco.zip\n",
            "  inflating: ./datasets/Annotations/v2_mscoco_train2014_annotations.json  \n",
            "Archive:  ./datasets/Annotations/v2_Annotations_Val_mscoco.zip\n",
            "  inflating: ./datasets/Annotations/v2_mscoco_val2014_annotations.json  \n",
            "Archive:  ./datasets/Questions/v2_Questions_Train_mscoco.zip\n",
            "  inflating: ./datasets/Questions/v2_OpenEnded_mscoco_train2014_questions.json  \n",
            "Archive:  ./datasets/Questions/v2_Questions_Val_mscoco.zip\n",
            "  inflating: ./datasets/Questions/v2_OpenEnded_mscoco_val2014_questions.json  \n",
            "Archive:  ./datasets/Questions/v2_Questions_Test_mscoco.zip\n",
            "  inflating: ./datasets/Questions/v2_OpenEnded_mscoco_test2015_questions.json  \n",
            "  inflating: ./datasets/Questions/v2_OpenEnded_mscoco_test-dev2015_questions.json  \n"
          ]
        }
      ],
      "source": [
        "#Make Folders\n",
        "######################################################################\n",
        "DATASETS_DIR = \"./datasets\"\n",
        "\n",
        "ANNOTATIONS_DIR = os.path.join(DATASETS_DIR,'Annotations')\n",
        "QUESTIONS_DIR = os.path.join(DATASETS_DIR,'Questions')\n",
        "IMAGES_DIR = os.path.join(DATASETS_DIR,'Images')\n",
        "\n",
        "!mkdir $DATASETS_DIR\n",
        "!mkdir $ANNOTATIONS_DIR\n",
        "!mkdir $QUESTIONS_DIR\n",
        "!mkdir $IMAGES_DIR\n",
        "######################################################################\n",
        "\n",
        "# Download datasets from VQA official url: https://visualqa.org/download.html\n",
        "\n",
        "#VQA Annotations\n",
        "!wget -O $ANNOTATIONS_DIR/v2_Annotations_Train_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\"\n",
        "!wget -O $ANNOTATIONS_DIR/v2_Annotations_Val_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\"\n",
        "\n",
        "# VQA Input Questions\n",
        "!wget -O $QUESTIONS_DIR/v2_Questions_Train_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\"\n",
        "!wget -O $QUESTIONS_DIR/v2_Questions_Val_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\"\n",
        "!wget -O $QUESTIONS_DIR/v2_Questions_Test_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\"\n",
        "\n",
        "# VQA Input Images (COCO)\n",
        "!gdown https://drive.google.com/uc?id=1-7wzyJhs5ZaHLmHwqjR2-OLS4EeGyVih\n",
        "\n",
        "######################################################################\n",
        "\n",
        "!unzip $ANNOTATIONS_DIR/v2_Annotations_Train_mscoco.zip -d $ANNOTATIONS_DIR\n",
        "!unzip $ANNOTATIONS_DIR/v2_Annotations_Val_mscoco.zip -d $ANNOTATIONS_DIR\n",
        "\n",
        "!rm $ANNOTATIONS_DIR/v2_Annotations_Train_mscoco.zip\n",
        "!rm $ANNOTATIONS_DIR/v2_Annotations_Val_mscoco.zip\n",
        "\n",
        "!unzip $QUESTIONS_DIR/v2_Questions_Train_mscoco.zip -d $QUESTIONS_DIR\n",
        "!unzip $QUESTIONS_DIR/v2_Questions_Val_mscoco.zip -d $QUESTIONS_DIR\n",
        "!unzip $QUESTIONS_DIR/v2_Questions_Test_mscoco.zip -d $QUESTIONS_DIR\n",
        "\n",
        "!rm $QUESTIONS_DIR/v2_Questions_Train_mscoco.zip\n",
        "!rm $QUESTIONS_DIR/v2_Questions_Val_mscoco.zip\n",
        "!rm $QUESTIONS_DIR/v2_Questions_Test_mscoco.zip\n",
        "\n",
        "# !unzip total.zip\n",
        "# !mv /content/content/datasets/ResizedImages /content/datasets/Images/Resized_Images\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL3FYkbt5lut"
      },
      "source": [
        "##Preprocess Input Data\n",
        "Preprocess input images, questions and answers.<br>\n",
        "  1. Resize Images\n",
        "  2. Make vocabs for questions and answers\n",
        "  3. Build VQA Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T7CTdgO6ZAs"
      },
      "source": [
        "### Resize Images\n",
        "Resize the image from 640x480 -> 224x224."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_qHuSPRl6Cnk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Use the code below if you are resizing the images directly downladed from the official dataset.\\ndef resize_image(image, size):\\n    \"\"\"Resize an image to the given size.\"\"\"\\n    return image.resize((size,size), Image.ANTIALIAS) # Option to prevent aliasing after resizing image.\\n\\ndef resize_images(input_dir, output_dir, size):\\n    \"\"\"Resize the images in \\'input_dir\\' and save into \\'output_dir\\'.\"\"\"\\n    for idir in os.scandir(input_dir):\\n        import pdb; pdb.set_trace()\\n        if not idir.is_dir():\\n            continue\\n        if not os.path.exists(output_dir+\\'/\\'+idir.name):\\n            os.makedirs(output_dir+\\'/\\'+idir.name)\\n        images = os.listdir(idir.path)\\n        n_images = len(images)\\n        for iimage, image in tqdm(enumerate(images)):\\n            if iimage >= 1000:\\n              break\\n            try:\\n                with open(os.path.join(idir.path, image), \\'r+b\\') as f:\\n                    with Image.open(f) as img:\\n                        img = resize_image(img, size)\\n                        img.save(os.path.join(output_dir+\\'/\\'+idir.name, image), img.format)\\n                        original_img_path = os.path.join(idir.path,image)\\n            except(IOError, SyntaxError) as e:\\n                pass\\n            if (iimage+1) % 1000 == 0:\\n                print(\"[{}/{}] Resized the images and saved into \\'{}\\'.\"\\n                      .format(iimage+1, n_images, output_dir+\\'/\\'+idir.name))\\n\\ninput_dir = \"./datasets/Images\"\\noutput_dir = \"./datasets/ResizedImages\"\\nimg_size = 224\\n\\nresize_images(input_dir,output_dir,img_size) # Resize Images'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Use the code below if you are resizing the images directly downladed from the official dataset.\n",
        "def resize_image(image, size):\n",
        "    \"\"\"Resize an image to the given size.\"\"\"\n",
        "    return image.resize((size,size), Image.ANTIALIAS) # Option to prevent aliasing after resizing image.\n",
        "\n",
        "def resize_images(input_dir, output_dir, size):\n",
        "    \"\"\"Resize the images in 'input_dir' and save into 'output_dir'.\"\"\"\n",
        "    for idir in os.scandir(input_dir):\n",
        "        import pdb; pdb.set_trace()\n",
        "        if not idir.is_dir():\n",
        "            continue\n",
        "        if not os.path.exists(output_dir+'/'+idir.name):\n",
        "            os.makedirs(output_dir+'/'+idir.name)\n",
        "        images = os.listdir(idir.path)\n",
        "        n_images = len(images)\n",
        "        for iimage, image in tqdm(enumerate(images)):\n",
        "            if iimage >= 1000:\n",
        "              break\n",
        "            try:\n",
        "                with open(os.path.join(idir.path, image), 'r+b') as f:\n",
        "                    with Image.open(f) as img:\n",
        "                        img = resize_image(img, size)\n",
        "                        img.save(os.path.join(output_dir+'/'+idir.name, image), img.format)\n",
        "                        original_img_path = os.path.join(idir.path,image)\n",
        "            except(IOError, SyntaxError) as e:\n",
        "                pass\n",
        "            if (iimage+1) % 1000 == 0:\n",
        "                print(\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                      .format(iimage+1, n_images, output_dir+'/'+idir.name))\n",
        "\n",
        "input_dir = \"./datasets/Images\"\n",
        "output_dir = \"./datasets/ResizedImages\"\n",
        "img_size = 224\n",
        "\n",
        "resize_images(input_dir,output_dir,img_size) # Resize Images'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qVlB-DMQxo"
      },
      "source": [
        "### Make Vocabs for Questions and answers.\n",
        "Make a dictionary for questions and answers and save them into a txt file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeua0cMfdhdG"
      },
      "source": [
        "## 1. Build the dictionary for questions.\n",
        "After running the code, the words in the dictionary can be observed in the vocab_questions.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP2Wl6uYOncV",
        "outputId": "64904768-cf11-490d-d94c-13b7405471c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['v2_OpenEnded_mscoco_train2014_questions.json', 'v2_OpenEnded_mscoco_val2014_questions.json', 'v2_OpenEnded_mscoco_test2015_questions.json', 'v2_OpenEnded_mscoco_test-dev2015_questions.json']\n",
            "Make vocabulary for questions\n",
            "The number of total words of questions: 17854\n",
            "Maximum length of question: 26\n"
          ]
        }
      ],
      "source": [
        "def make_vocab_questions(input_dir):\n",
        "    \"\"\"Make dictionary for questions and save them into text file.\"\"\"\n",
        "    vocab_set = set()\n",
        "    SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "    question_length = []\n",
        "    datasets = os.listdir(input_dir)\n",
        "    print(datasets)\n",
        "    for dataset in datasets:\n",
        "        if 'json' in dataset:\n",
        "          with open(input_dir+'/'+dataset) as f:\n",
        "              questions = json.load(f)['questions']\n",
        "          set_question_length = [None]*len(questions)\n",
        "          for iquestion, question in enumerate(questions):\n",
        "              words = SENTENCE_SPLIT_REGEX.split(question['question'].lower())\n",
        "              words = [w.strip() for w in words if len(w.strip()) > 0]\n",
        "              vocab_set.update(words)\n",
        "              set_question_length[iquestion] = len(words)\n",
        "          question_length += set_question_length\n",
        "\n",
        "    vocab_list = list(vocab_set)\n",
        "    vocab_list.sort()\n",
        "    vocab_list.insert(0, '<pad>')\n",
        "    vocab_list.insert(1, '<unk>')\n",
        "\n",
        "    with open('./datasets/vocab_questions.txt', 'w') as f:\n",
        "        f.writelines([w+'\\n' for w in vocab_list])\n",
        "\n",
        "    print('Make vocabulary for questions')\n",
        "    print('The number of total words of questions: %d' % len(vocab_set))\n",
        "    print('Maximum length of question: %d' % np.max(question_length))\n",
        "\n",
        "input_dir = \"./datasets/\"\n",
        "make_vocab_questions(input_dir+'Questions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyhxSCXPgJUF"
      },
      "source": [
        "## 2. Build the dictionary for answers.\n",
        "Note that the number of words for the answers are cut to n_answers.<br>\n",
        "After running the code, the words in the dictionary can be observed in the vocab_answers.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYmQmeAWdo8p",
        "outputId": "e5f4034a-0750-4490-ad8e-a083aa2fb8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make vocabulary for answers\n",
            "The number of total words of answers: 181102\n",
            "Keep top 1000 answers into vocab\n"
          ]
        }
      ],
      "source": [
        "def make_vocab_answers(input_dir, n_answers):\n",
        "    \"\"\"Make dictionary for top n answers and save them into text file.\"\"\"\n",
        "    answers = defaultdict(lambda: 0)\n",
        "    datasets = os.listdir(input_dir)\n",
        "    for dataset in datasets:\n",
        "      if 'json' in dataset:\n",
        "          with open(input_dir+'/'+dataset) as f:\n",
        "              annotations = json.load(f)['annotations']\n",
        "          for annotation in annotations:\n",
        "              for answer in annotation['answers']:\n",
        "                  word = answer['answer']\n",
        "                  if re.search(r\"[^\\w\\s]\", word):\n",
        "                      continue\n",
        "                  answers[word] += 1\n",
        "\n",
        "    answers = sorted(answers, key=answers.get, reverse=True)\n",
        "    assert('<unk>' not in answers)\n",
        "    top_answers = ['<unk>'] + answers[:n_answers-1] # '-1' is due to '<unk>'. We restrict the number of words to n_answers.\n",
        "\n",
        "    with open('./datasets/vocab_answers.txt', 'w') as f:\n",
        "        f.writelines([w+'\\n' for w in top_answers])\n",
        "\n",
        "    print('Make vocabulary for answers')\n",
        "    print('The number of total words of answers: %d' % len(answers))\n",
        "    print('Keep top %d answers into vocab' % n_answers)\n",
        "\n",
        "input_dir = \"./datasets/\"\n",
        "# answer의 개수는 1000개로 제한 (가장 빈도수가 높은 1000개의 답변만 사용)\n",
        "n_answers = 1000\n",
        "\n",
        "\n",
        "make_vocab_answers(input_dir+'Annotations',n_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hheNDJdPHpO"
      },
      "source": [
        "### Build VQA Inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnwS_SVqPtDO"
      },
      "source": [
        "Define functions and classes that helps building inputs easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "uCxdE3dXP10U",
        "outputId": "4fe21b6b-3535-4cd5-efa3-e298242c410c"
      },
      "outputs": [],
      "source": [
        "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "\n",
        "\n",
        "def tokenize(sentence):\n",
        "    '''Question 1'''\n",
        "    '''Use the SENTENCE_SPLIT_REGEX to tokenize the input sentence'''\n",
        "    # tokens = []\n",
        "    # tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
        "    tokens = [w.strip() for w in SENTENCE_SPLIT_REGEX.split(sentence.lower()) if len(w.strip()) > 0]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "def load_str_list(fname):\n",
        "    with open(fname) as f:\n",
        "        lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BkT5WWKqiRgw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The world is changing.\n",
            "['the', 'world', 'is', 'changing', '.']\n"
          ]
        }
      ],
      "source": [
        "input = 'The world is changing.'\n",
        "print(input)\n",
        "print(tokenize(input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhD-haWfluf9"
      },
      "source": [
        "## Question 2 & 3\n",
        "### Q2. Define the __init__ function of the class VocabDict that takes a vocab_file as input and then translates the word to index and index to word.\n",
        "\n",
        "### The vocab_file is the .txt file that was the output of the functions make_vocab_questions, make_vocab_answers. Try giving them as input to fill in the blanks.\n",
        "\n",
        "### Q3. Define the tokenize_and_index function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BEaaKoVylrYd"
      },
      "outputs": [],
      "source": [
        "class VocabDict:\n",
        "\n",
        "    def __init__(self, vocab_file):\n",
        "        self.word_list = load_str_list(vocab_file)\n",
        "        '''Question 2'''\n",
        "        '''Fill in the blanks below'''\n",
        "\n",
        "        self.word2idx_dict = {w: i for i, w in enumerate(self.word_list)}\n",
        "        self.vocab_size = len(self.word_list)\n",
        "        self.unk2idx = self.word2idx_dict.get('<unk>', None)\n",
        "\n",
        "    def idx2word(self, n_w):\n",
        "\n",
        "        return self.word_list[n_w]\n",
        "\n",
        "    def word2idx(self, w):\n",
        "        if w in self.word2idx_dict:\n",
        "            return self.word2idx_dict[w]\n",
        "        elif self.unk2idx is not None:\n",
        "            return self.unk2idx\n",
        "        else:\n",
        "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
        "\n",
        "    def tokenize_and_index(self, sentence):\n",
        "      '''Question 3'''\n",
        "      '''This function takes a sentence as input and returns the indices of the tokenized words.\n",
        "         Use the tokenize function you implemented above.'''\n",
        "\n",
        "      inds = [self.word2idx(token) for token in tokenize(sentence)]\n",
        "      return inds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kpfziAlcnAHY"
      },
      "outputs": [],
      "source": [
        "answer_vocab = VocabDict('./datasets/vocab_answers.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gezH3ze_nGiS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer_vocab.word2idx_dict['corn']\n",
        "answer_vocab.unk2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer_vocab.word2idx_dict['yes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKlw74KntC0k"
      },
      "source": [
        "## Understanding the VocabDict class.\n",
        "### Try playing with the VocabDict class with the text files from make_vocab_questions, make_vocab_answers.\n",
        "\n",
        "### Try finding a sentence with all known vocabulary in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q6ZYkzO6tXZ9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 1]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_dict = VocabDict('./datasets/vocab_questions.txt')\n",
        "answer_dict = VocabDict('./datasets/vocab_answers.txt')\n",
        "\n",
        "answer_dict.tokenize_and_index('Yes No')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lkpj0DoSPZYY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "building vqa train2014 dataset\n",
            "processing 10000 / 443757\n",
            "processing 20000 / 443757\n",
            "processing 30000 / 443757\n",
            "processing 40000 / 443757\n",
            "processing 50000 / 443757\n",
            "processing 60000 / 443757\n",
            "processing 70000 / 443757\n",
            "processing 80000 / 443757\n",
            "processing 90000 / 443757\n",
            "processing 100000 / 443757\n",
            "processing 110000 / 443757\n",
            "processing 120000 / 443757\n",
            "processing 130000 / 443757\n",
            "processing 140000 / 443757\n",
            "processing 150000 / 443757\n",
            "processing 160000 / 443757\n",
            "processing 170000 / 443757\n",
            "processing 180000 / 443757\n",
            "processing 190000 / 443757\n",
            "processing 200000 / 443757\n",
            "processing 210000 / 443757\n",
            "processing 220000 / 443757\n",
            "processing 230000 / 443757\n",
            "processing 240000 / 443757\n",
            "processing 250000 / 443757\n",
            "processing 260000 / 443757\n",
            "processing 270000 / 443757\n",
            "processing 280000 / 443757\n",
            "processing 290000 / 443757\n",
            "processing 300000 / 443757\n",
            "processing 310000 / 443757\n",
            "processing 320000 / 443757\n",
            "processing 330000 / 443757\n",
            "processing 340000 / 443757\n",
            "processing 350000 / 443757\n",
            "processing 360000 / 443757\n",
            "processing 370000 / 443757\n",
            "processing 380000 / 443757\n",
            "processing 390000 / 443757\n",
            "processing 400000 / 443757\n",
            "processing 410000 / 443757\n",
            "processing 420000 / 443757\n",
            "processing 430000 / 443757\n",
            "processing 440000 / 443757\n",
            "total 21164 out of 443757 answers are <unk>\n",
            "building vqa val2014 dataset\n",
            "processing 10000 / 214354\n",
            "processing 20000 / 214354\n",
            "processing 30000 / 214354\n",
            "processing 40000 / 214354\n",
            "processing 50000 / 214354\n",
            "processing 60000 / 214354\n",
            "processing 70000 / 214354\n",
            "processing 80000 / 214354\n",
            "processing 90000 / 214354\n",
            "processing 100000 / 214354\n",
            "processing 110000 / 214354\n",
            "processing 120000 / 214354\n",
            "processing 130000 / 214354\n",
            "processing 140000 / 214354\n",
            "processing 150000 / 214354\n",
            "processing 160000 / 214354\n",
            "processing 170000 / 214354\n",
            "processing 180000 / 214354\n",
            "processing 190000 / 214354\n",
            "processing 200000 / 214354\n",
            "processing 210000 / 214354\n",
            "total 10342 out of 214354 answers are <unk>\n",
            "building vqa test2015 dataset\n",
            "processing 10000 / 447793\n",
            "processing 20000 / 447793\n",
            "processing 30000 / 447793\n",
            "processing 40000 / 447793\n",
            "processing 50000 / 447793\n",
            "processing 60000 / 447793\n",
            "processing 70000 / 447793\n",
            "processing 80000 / 447793\n",
            "processing 90000 / 447793\n",
            "processing 100000 / 447793\n",
            "processing 110000 / 447793\n",
            "processing 120000 / 447793\n",
            "processing 130000 / 447793\n",
            "processing 140000 / 447793\n",
            "processing 150000 / 447793\n",
            "processing 160000 / 447793\n",
            "processing 170000 / 447793\n",
            "processing 180000 / 447793\n",
            "processing 190000 / 447793\n",
            "processing 200000 / 447793\n",
            "processing 210000 / 447793\n",
            "processing 220000 / 447793\n",
            "processing 230000 / 447793\n",
            "processing 240000 / 447793\n",
            "processing 250000 / 447793\n",
            "processing 260000 / 447793\n",
            "processing 270000 / 447793\n",
            "processing 280000 / 447793\n",
            "processing 290000 / 447793\n",
            "processing 300000 / 447793\n",
            "processing 310000 / 447793\n",
            "processing 320000 / 447793\n",
            "processing 330000 / 447793\n",
            "processing 340000 / 447793\n",
            "processing 350000 / 447793\n",
            "processing 360000 / 447793\n",
            "processing 370000 / 447793\n",
            "processing 380000 / 447793\n",
            "processing 390000 / 447793\n",
            "processing 400000 / 447793\n",
            "processing 410000 / 447793\n",
            "processing 420000 / 447793\n",
            "processing 430000 / 447793\n",
            "processing 440000 / 447793\n",
            "total 0 out of 447793 answers are <unk>\n",
            "building vqa test-dev2015 dataset\n",
            "processing 10000 / 107394\n",
            "processing 20000 / 107394\n",
            "processing 30000 / 107394\n",
            "processing 40000 / 107394\n",
            "processing 50000 / 107394\n",
            "processing 60000 / 107394\n",
            "processing 70000 / 107394\n",
            "processing 80000 / 107394\n",
            "processing 90000 / 107394\n",
            "processing 100000 / 107394\n",
            "total 0 out of 107394 answers are <unk>\n"
          ]
        }
      ],
      "source": [
        "def extract_answers(q_answers, valid_answer_set):\n",
        "    all_answers = [answer[\"answer\"] for answer in q_answers]\n",
        "    valid_answers = [a for a in all_answers if a in valid_answer_set]\n",
        "    return all_answers, valid_answers\n",
        "\n",
        "\n",
        "def vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, image_set):\n",
        "    print('building vqa %s dataset' % image_set)\n",
        "    if image_set in ['train2014', 'val2014']:\n",
        "        load_answer = True\n",
        "        with open(annotation_file % image_set) as f:\n",
        "            annotations = json.load(f)['annotations']\n",
        "            qid2ann_dict = {ann['question_id']: ann for ann in annotations}\n",
        "    else:\n",
        "        load_answer = False\n",
        "    with open(question_file % image_set) as f:\n",
        "        questions = json.load(f)['questions']\n",
        "    coco_set_name = image_set.replace('-dev', '')\n",
        "    abs_image_dir = os.path.abspath(image_dir % coco_set_name)\n",
        "    image_name_template = 'COCO_'+coco_set_name+'_%012d'\n",
        "    dataset = [None]*len(questions)\n",
        "\n",
        "    unk_ans_count = 0\n",
        "    for n_q, q in enumerate(questions):\n",
        "        if (n_q+1) % 10000 == 0:\n",
        "            print('processing %d / %d' % (n_q+1, len(questions)))\n",
        "        image_id = q['image_id']\n",
        "        question_id = q['question_id']\n",
        "        image_name = image_name_template % image_id\n",
        "        image_path = os.path.join(abs_image_dir, image_name+'.jpg')\n",
        "        question_str = q['question']\n",
        "        question_tokens = tokenize(question_str)\n",
        "\n",
        "        iminfo = dict(image_name=image_name,\n",
        "                      image_path=image_path,\n",
        "                      question_id=question_id,\n",
        "                      question_str=question_str,\n",
        "                      question_tokens=question_tokens)\n",
        "\n",
        "        if load_answer:\n",
        "            ann = qid2ann_dict[question_id]\n",
        "            all_answers, valid_answers = extract_answers(ann['answers'], valid_answer_set)\n",
        "            if len(valid_answers) == 0:\n",
        "                valid_answers = ['<unk>']\n",
        "                unk_ans_count += 1\n",
        "            iminfo['all_answers'] = all_answers\n",
        "            iminfo['valid_answers'] = valid_answers\n",
        "\n",
        "        dataset[n_q] = iminfo\n",
        "    print('total %d out of %d answers are <unk>' % (unk_ans_count, len(questions)))\n",
        "    return dataset\n",
        "\n",
        "#############################################################################################################\n",
        "input_dir = \"./datasets/\"\n",
        "output_dir = \"./datasets/\"\n",
        "\n",
        "image_dir = input_dir+'/Images/Resized_Images/%s/'\n",
        "annotation_file = input_dir+'/Annotations/v2_mscoco_%s_annotations.json'\n",
        "question_file = input_dir+'/Questions/v2_OpenEnded_mscoco_%s_questions.json'\n",
        "\n",
        "vocab_answer_file = output_dir+'/vocab_answers.txt'\n",
        "answer_dict = VocabDict(vocab_answer_file)\n",
        "valid_answer_set = set(answer_dict.word_list)\n",
        "\n",
        "train = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'train2014')\n",
        "valid = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'val2014')\n",
        "test = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'test2015')\n",
        "test_dev = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'test-dev2015')\n",
        "\n",
        "np.save(output_dir+'/train.npy', np.array(train))\n",
        "np.save(output_dir+'/valid.npy', np.array(valid))\n",
        "np.save(output_dir+'/train_valid.npy', np.array(train+valid))\n",
        "np.save(output_dir+'/test.npy', np.array(test))\n",
        "np.save(output_dir+'/test-dev.npy', np.array(test_dev))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wHkjN72F7F"
      },
      "source": [
        "## Try testing the train, validation and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uisDn78R2JLR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image_name': 'COCO_train2014_000000458752',\n",
              " 'image_path': '/2025-ai-expert/알고리즘/서홍석/datasets/Images/Resized_Images/train2014/COCO_train2014_000000458752.jpg',\n",
              " 'question_id': 458752000,\n",
              " 'question_str': 'What is this photo taken looking through?',\n",
              " 'question_tokens': ['what',\n",
              "  'is',\n",
              "  'this',\n",
              "  'photo',\n",
              "  'taken',\n",
              "  'looking',\n",
              "  'through',\n",
              "  '?'],\n",
              " 'all_answers': ['net',\n",
              "  'net',\n",
              "  'net',\n",
              "  'netting',\n",
              "  'net',\n",
              "  'net',\n",
              "  'mesh',\n",
              "  'net',\n",
              "  'net',\n",
              "  'net'],\n",
              " 'valid_answers': ['net', 'net', 'net', 'net', 'net', 'net', 'net', 'net']}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5gqh76JVzRm"
      },
      "source": [
        "#DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dqIjhnbkV16q"
      },
      "outputs": [],
      "source": [
        "class VqaDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, input_vqa, max_qst_length=30, max_num_ans=10, transform=None):\n",
        "        self.input_dir = input_dir\n",
        "        self.vqa = np.load(input_dir+'/'+input_vqa,allow_pickle=True)\n",
        "        self.qst_vocab = VocabDict(input_dir+'/vocab_questions.txt')\n",
        "        self.ans_vocab = VocabDict(input_dir+'/vocab_answers.txt')\n",
        "        self.max_qst_length = max_qst_length\n",
        "        self.max_num_ans = max_num_ans\n",
        "        self.load_ans = ('valid_answers' in self.vqa[0]) and (self.vqa[0]['valid_answers'] is not None)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        vqa = self.vqa\n",
        "        qst_vocab = self.qst_vocab\n",
        "        ans_vocab = self.ans_vocab\n",
        "        max_qst_length = self.max_qst_length\n",
        "        max_num_ans = self.max_num_ans\n",
        "        transform = self.transform\n",
        "        load_ans = self.load_ans\n",
        "\n",
        "        image = vqa[idx]['image_path']\n",
        "        image = Image.open(image).convert('RGB')\n",
        "        qst2idc = np.array([qst_vocab.word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
        "        qst2idc[:len(vqa[idx]['question_tokens'])] = [qst_vocab.word2idx(w) for w in vqa[idx]['question_tokens']]\n",
        "        sample = {'image': image, 'question': qst2idc}\n",
        "\n",
        "        if load_ans:\n",
        "            ans2idc = [ans_vocab.word2idx(w) for w in vqa[idx]['valid_answers']]\n",
        "            ans2idx = np.random.choice(ans2idc)\n",
        "            sample['answer_label'] = ans2idx         # for training\n",
        "\n",
        "            mul2idc = list([-1] * max_num_ans)       # padded with -1 (no meaning) not used in 'ans_vocab'\n",
        "            mul2idc[:len(ans2idc)] = ans2idc         # our model should not predict -1\n",
        "            sample['answer_multi_choice'] = mul2idc  # for evaluation metric of 'multiple choice'\n",
        "\n",
        "        if transform:\n",
        "            sample['image'] = transform(sample['image'])\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.vqa)\n",
        "\n",
        "\n",
        "def get_loader(input_dir, input_vqa_train, input_vqa_valid, max_qst_length, max_num_ans, batch_size, num_workers):\n",
        "\n",
        "    transform = {\n",
        "        phase: transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                                        (0.229, 0.224, 0.225))])\n",
        "        for phase in ['train', 'valid']}\n",
        "\n",
        "    vqa_dataset = {\n",
        "        'train': VqaDataset(\n",
        "            input_dir=input_dir,\n",
        "            input_vqa=input_vqa_train,\n",
        "            max_qst_length=max_qst_length,\n",
        "            max_num_ans=max_num_ans,\n",
        "            transform=transform['train']),\n",
        "        'valid': VqaDataset(\n",
        "            input_dir=input_dir,\n",
        "            input_vqa=input_vqa_valid,\n",
        "            max_qst_length=max_qst_length,\n",
        "            max_num_ans=max_num_ans,\n",
        "            transform=transform['valid'])}\n",
        "\n",
        "    data_loader = {\n",
        "        phase: torch.utils.data.DataLoader(\n",
        "            dataset=vqa_dataset[phase],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers)\n",
        "        for phase in ['train', 'valid']}\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlxqwtgzQ4Ke"
      },
      "source": [
        "## Model\n",
        "Our model is consisted of 3 big parts.\n",
        "1. Image Encoder : Returns the image features.\n",
        "2. Question Encoder : Encodes the questions with the defined dictionary.\n",
        "3. FC Layer: Estimate the class of the combined features from both encoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDcCunAdStCr"
      },
      "source": [
        "### 1. Image Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-QYlWFd4yk6E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:19<00:00, 29.5MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg19(pretrained=True)\n",
        "print(model.classifier[-1].in_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gdm7hb4H0_am"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=25088, out_features=4096, bias=True) ReLU(inplace=True) Dropout(p=0.5, inplace=False) Linear(in_features=4096, out_features=4096, bias=True) ReLU(inplace=True) Dropout(p=0.5, inplace=False) Linear(in_features=4096, out_features=1000, bias=True)\n"
          ]
        }
      ],
      "source": [
        "print(*list(model.classifier.children()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sGjLFPbgQ_sr"
      },
      "outputs": [],
      "source": [
        "class ImgEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"(1) Load the pretrained model as you want.\n",
        "               cf) one needs to check structure of model using 'print(model)'\n",
        "                   to remove last fc layer from the model.\n",
        "           (2) Replace final fc layer (score values from the ImageNet)\n",
        "               with new fc layer (image feature).\n",
        "           (3) Normalize feature vector.\n",
        "        \"\"\"\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        '''Question 4'''\n",
        "        '''Fill in the blanks so that it matches (1) and (2) in the function description.'''\n",
        "        # input size of feature vector\n",
        "        in_features = model.classifier[-1].in_features\n",
        "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])  # remove last fc layer\n",
        "    \n",
        "        # loaded model without last fc layer\n",
        "        self.model = model\n",
        "        # feature vector of image\n",
        "        self.fc = nn.Linear(in_features, embed_size)\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"Extract feature vector from image vector.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.model(image)                  # [batch_size, vgg16(19)_fc=4096]\n",
        "        img_feature = self.fc(img_feature)                   # [batch_size, embed_size]\n",
        "\n",
        "        '''Question 5'''\n",
        "        '''L2 Normalize the img_features from self.fc()'''\n",
        "        l2_norm = torch.norm(img_feature, p=2, dim=1, keepdim=True)\n",
        "        # l2-normalized feature vector\n",
        "        img_feature = img_feature / l2_norm\n",
        "\n",
        "        return img_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wD2zECTh5W4U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1024])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(20,3,224,224)\n",
        "test = ImgEncoder(1024)\n",
        "out = test(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoB8xiLjSyUD"
      },
      "source": [
        "### 2. Question Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rnrVtXhzS2rU"
      },
      "outputs": [],
      "source": [
        "class QstEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(QstEncoder, self).__init__()\n",
        "        '''Question 6'''\n",
        "        '''Define the following functions using the nn library.'''\n",
        "        '''Please look at the documents of the functions nn.Embedding, nn.Tanh, nn.LSTM, nn.Linear to match the dimensions.'''\n",
        "        '''word2vec : A learnable layer that takes the vocabulary id and return a embedding vector '''\n",
        "\n",
        "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size, padding_idx=0)  # 0 for padding_idx\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.lstm = nn.LSTM(input_size=word_embed_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n",
        "\n",
        "        # self.word2vec = nn.Embedding()  # 0 for padding_idx\n",
        "        # self.tanh = nn.Tanh()\n",
        "        # self.lstm = nn.LSTM()\n",
        "        # self.fc = nn.Linear()     # 2 for hidden and cell states\n",
        "\n",
        "    def forward(self, question):\n",
        "\n",
        "        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n",
        "        qst_vec = self.tanh(qst_vec)\n",
        "        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n",
        "        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n",
        "        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n",
        "        qst_feature = self.tanh(qst_feature)\n",
        "        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n",
        "\n",
        "        return qst_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wsy6PayS6ds"
      },
      "source": [
        "## VQA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lS2fGrvJS7u0"
      },
      "outputs": [],
      "source": [
        "class VqaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(VqaModel, self).__init__()\n",
        "        self.img_encoder = ImgEncoder(embed_size)\n",
        "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n",
        "        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "\n",
        "        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n",
        "        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  # [batch_size, embed_size]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "\n",
        "        return combined_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5gn-LHCWHh0"
      },
      "source": [
        "#Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "eS9utPUrW7B8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_dir': './datasets', 'log_dir': './logs', 'model_dir': './models', 'max_qst_length': 30, 'max_num_ans': 10, 'embed_size': 1024, 'word_embed_size': 300, 'num_layers': 2, 'hidden_size': 512, 'learning_rate': 0.001, 'step_size': 10, 'gamma': 0.1, 'num_epochs': 1, 'batch_size': 256, 'num_workers': 0, 'save_step': 1}\n"
          ]
        }
      ],
      "source": [
        "args = easydict.EasyDict()\n",
        "\n",
        "## Parameters for training\n",
        "args.input_dir = './datasets' # Input Directory for VQA\n",
        "args.log_dir = './logs' # Directory for logs\n",
        "args.model_dir = './models' # Directory for saved models\n",
        "args.max_qst_length = 30 # Maximum length of quesiton.\n",
        "args.max_num_ans = 10 # Maximum number of answers.\n",
        "args.embed_size = 1024 # Embedding size of feature vectors for both image and question.\n",
        "# args.embedding_dim = 1024 # Dimension of joint embedding space.\n",
        "args.word_embed_size = 300 # Embedding size of word used for the input in the LSTM.\n",
        "args.num_layers = 2 # Number os Layers of the RNN(LSTM).\n",
        "args.hidden_size = 512 # Hidden size in the LSTM.\n",
        "args.learning_rate = 0.001 # Learning rate for training.\n",
        "args.step_size = 10 # Period of Learning Rate Decay.\n",
        "args.gamma = 0.1 # Multiplicative factor of learning rate decay.\n",
        "args.num_epochs = 1 # Number of epochs.\n",
        "args.batch_size = 256 # Batch size.\n",
        "args.num_workers = 0 # Number of processes working on cpu.\n",
        "args.save_step = 1 # Save step of model.\n",
        "\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "r2ZLnWVj_McR"
      },
      "outputs": [],
      "source": [
        "'''Question 7'''\n",
        "'''Define the loss function'''\n",
        "def custom_ce(pred,label):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  loss = criterion(pred, label)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EvnkuQuLCaar"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3172)\n",
            "tensor(2.3172)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "x = torch.rand((20,10))\n",
        "y = torch.randint(low=0,high=9,size=(20,)).to(torch.int64)\n",
        "real = nn.CrossEntropyLoss()\n",
        "print(custom_ce(x,y))\n",
        "print(real(x,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nNiQulb_WKsF"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train(args):\n",
        "\n",
        "    os.makedirs(args.log_dir, exist_ok=True)\n",
        "    os.makedirs(args.model_dir, exist_ok=True)\n",
        "\n",
        "    data_loader = get_loader(\n",
        "        input_dir=args.input_dir,\n",
        "        input_vqa_train='train.npy',\n",
        "        input_vqa_valid='valid.npy',\n",
        "        max_qst_length=args.max_qst_length,\n",
        "        max_num_ans=args.max_num_ans,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "\n",
        "    qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n",
        "    ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size\n",
        "    ans_unk_idx = data_loader['train'].dataset.ans_vocab.unk2idx\n",
        "\n",
        "    model = VqaModel(\n",
        "        embed_size=args.embed_size,\n",
        "        qst_vocab_size=qst_vocab_size,\n",
        "        ans_vocab_size=ans_vocab_size,\n",
        "        word_embed_size=args.word_embed_size,\n",
        "        num_layers=args.num_layers,\n",
        "        hidden_size=args.hidden_size).to(device)\n",
        "\n",
        "    criterion = custom_ce\n",
        "\n",
        "    params = list(model.img_encoder.fc.parameters()) \\\n",
        "        + list(model.qst_encoder.parameters()) \\\n",
        "        + list(model.fc1.parameters()) \\\n",
        "        + list(model.fc2.parameters())\n",
        "\n",
        "    optimizer = optim.Adam(params, lr=args.learning_rate)\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
        "    phase = 'train'\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "      running_loss = 0.0\n",
        "      running_corr_exp1 = 0\n",
        "      batch_step_size = len(data_loader[phase].dataset) / args.batch_size\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
        "        image = batch_sample['image'].to(device)\n",
        "        question = batch_sample['question'].to(device)\n",
        "        label = batch_sample['answer_label'].to(device)\n",
        "        multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            output = model(image, question)      # [batch_size, ans_vocab_size=1000]\n",
        "            _, pred_exp1 = torch.max(output, 1)  # [batch_size]\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Evaluation metric of 'multiple choice'\n",
        "        # Exp1: our model prediction to '<unk>' IS accepted as the answer.\n",
        "        # Exp2: our model prediction to '<unk>' is NOT accepted as the answer.\n",
        "        running_loss += loss.item()\n",
        "        running_corr_exp1 += torch.stack([(ans == pred_exp1.cpu()) for ans in multi_choice]).any(dim=0).sum()\n",
        "\n",
        "        # Print the average loss in a mini-batch.\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}'\n",
        "                  .format(phase.upper(), epoch+1, args.num_epochs, batch_idx, int(batch_step_size), loss.item()))\n",
        "        # Save the model check points.\n",
        "        if (batch_idx) == 500:\n",
        "            torch.save({'epoch': epoch+1, 'state_dict': model.state_dict()},\n",
        "                        os.path.join(args.model_dir, 'model-epoch-{:02d}.ckpt'.format(epoch+1)))\n",
        "            break\n",
        "\n",
        "      if (epoch + 1) % args.save_step == 0:\n",
        "        torch.save({'epoch': epoch+1, 'state_dict': model.state_dict()},\n",
        "                        os.path.join(args.model_dir, 'model-epoch-{:02d}.ckpt'.format(epoch+1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-bIz0FOpXZJj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/2025-ai-expert/.cv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| TRAIN SET | Epoch [01/01], Step [0000/1733], Loss: 6.9206\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[50], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     56\u001b[0m _, pred_exp1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/2025-ai-expert/.cv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkpbUVdgVU67"
      },
      "source": [
        "# Evaluate our model with the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxWKcLbfSYuG"
      },
      "outputs": [],
      "source": [
        "import gc; gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfK_FI7WSale"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=17C8uZTm6WHW0c2q4pi5v-gHzqbegA6LV\n",
        "!mv model-epoch-25.ckpt models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p5RGyiqVol9"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "phase = 'valid'\n",
        "answer_dict = VocabDict('./datasets/vocab_answers.txt')\n",
        "question_dict = VocabDict('./datasets/vocab_questions.txt')\n",
        "tensor2img = transforms.ToPILImage()\n",
        "\n",
        "def eval(args):\n",
        "\n",
        "    os.makedirs(args.log_dir, exist_ok=True)\n",
        "    os.makedirs(args.model_dir, exist_ok=True)\n",
        "\n",
        "    data_loader = get_loader(\n",
        "        input_dir=args.input_dir,\n",
        "        input_vqa_train='train.npy',\n",
        "        input_vqa_valid='valid.npy',\n",
        "        max_qst_length=args.max_qst_length,\n",
        "        max_num_ans=args.max_num_ans,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "\n",
        "    qst_vocab_size = data_loader['valid'].dataset.qst_vocab.vocab_size\n",
        "    ans_vocab_size = data_loader['valid'].dataset.ans_vocab.vocab_size\n",
        "    ans_unk_idx = data_loader['valid'].dataset.ans_vocab.unk2idx\n",
        "\n",
        "    model = VqaModel(\n",
        "        embed_size=args.embed_size,\n",
        "        qst_vocab_size=qst_vocab_size,\n",
        "        ans_vocab_size=ans_vocab_size,\n",
        "        word_embed_size=args.word_embed_size,\n",
        "        num_layers=args.num_layers,\n",
        "        hidden_size=args.hidden_size)\n",
        "\n",
        "    # path = './models/model-epoch-01.ckpt'\n",
        "    path = './models/model-epoch-25.ckpt'\n",
        "    model.load_state_dict(torch.load(path),strict=False)\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
        "      image = batch_sample['image'].to(device)\n",
        "      question = batch_sample['question'].to(device)\n",
        "      label = batch_sample['answer_label'].to(device)\n",
        "      multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n",
        "\n",
        "      output = model(image,question)\n",
        "      _, pred = torch.max(output,0)\n",
        "\n",
        "      for i in range(args.batch_size):\n",
        "        import pdb; pdb.set_trace()\n",
        "        trans_img = image[i].transpose(0,1).transpose(1,2).cpu()\n",
        "        q = question[i]\n",
        "        q_ques = []\n",
        "        plt.imshow(trans_img)\n",
        "        for q_q in q.tolist():\n",
        "          q_ques.append(question_dict.idx2word(q_q))\n",
        "        l = label[i]\n",
        "        p = pred[i]\n",
        "        print(\"The question is : \")\n",
        "        print(q_ques)\n",
        "        print(\"The answer is : \" + answer_dict.idx2word(l))\n",
        "        print(\"The output of our model is :\" + answer_dict.idx2word(p))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49pOG_-95Ttx"
      },
      "outputs": [],
      "source": [
        "eval(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LL3FYkbt5lut"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".cv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"M1atRyIIkd7X"},"source":["**Please follow the following steps in order to get required data and checkpoints**:\n","\n","1. Click [this link](https://drive.google.com/drive/folders/1kogf2w-wmwVz4vK6sh0fel6KLAPvNCC1?usp=sharing).  \n","2. In the top-left, click the folder name **`SAMSUNG_AIExpert`** → **`Organize`**(**정리**) → **`Add shortcut to Drive`**(**바로가기에 추가**) → choose your desired ❗location❗ to add the folder.\n","\n","3. In the cell right below, run `drive.mount('/content/drive')` and set `FOLDER_PATH` to the ❗location❗ where you added the folder in step 2."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Aug 12 05:12:42 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce RTX 3090        On  |   00000000:3D:00.0 Off |                  N/A |\n","| 30%   27C    P8             24W /  350W |       2MiB /  24576MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tK6plEUSlHNw"},"outputs":[],"source":["# from google.colab import drive\n","from pathlib import Path\n","import os, json, re, shutil, zipfile\n","\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NgMuGq2qkquO"},"outputs":[],"source":["FOLDER_PATH = f\"./\" ## 👈 Edit here and set FOLDER_PATH to the ❗location❗\n","FOLDER_PATH = Path(FOLDER_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["PosixPath('.')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["FOLDER_PATH"]},{"cell_type":"markdown","metadata":{"id":"HJ4_82t4uBBV"},"source":["We will implement and train a DDPM model to generate small 128 x 128 images conditioned on text prompts. First, we will implement the forward noising process based on Eq. (4) of the paper [1]. Then we will build a UNet model that takes $x_t$ and $t$ as inputs (optionally with other conditioning like text-prompt) and outputs a tensor of the same shape as $x_t$. Finally, we will implement the denoising objective and train our DDPM model.\n","\n","We use the text encoder from a pretrained CLIP[2] model to encode input text into a 512-dimensional vector. To speed up training, we've already pre-encoded the text data from the training set.\n","\n","[1] Denoising Diffusion Probabilistic Models. Jonathan Ho, Ajay Jain, Pieter Abbeel. [Link](https://arxiv.org/pdf/2006.11239)\n","[2] Learning transferable visual models from natural language supervision. Radford et. al. [Link](https://github.com/openai/CLIP)"]},{"cell_type":"markdown","metadata":{"id":"hgxXUK3d6zuO"},"source":["# Set up & Install Packages\n","Run the cells below in order."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Jp-AGBwNjj4p"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'AIExpert_Samsung'...\n","remote: Enumerating objects: 27, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 27 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (27/27), 13.46 KiB | 4.49 MiB/s, done.\n","Resolving deltas: 100% (7/7), done.\n"]}],"source":["!git clone https://github.com/rlawldud53/AIExpert_Samsung.git"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"L4yh9uPtZTFl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_jjcz_ed\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_jjcz_ed\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: packaging in /2025-ai-expert/.venv/lib/python3.10/site-packages (from clip==1.0) (25.0)\n","Collecting regex (from clip==1.0)\n","  Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n","Collecting tqdm (from clip==1.0)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","Requirement already satisfied: torch in /2025-ai-expert/.venv/lib/python3.10/site-packages (from clip==1.0) (2.8.0)\n","Requirement already satisfied: torchvision in /2025-ai-expert/.venv/lib/python3.10/site-packages (from clip==1.0) (0.23.0)\n","Requirement already satisfied: wcwidth in /2025-ai-expert/.venv/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.14.0)\n","Requirement already satisfied: networkx in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2025.7.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1.3)\n","Requirement already satisfied: triton==3.4.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from triton==3.4.0->torch->clip==1.0) (80.9.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Requirement already satisfied: numpy in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.2.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (11.3.0)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.8/789.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369549 sha256=bcbeef4a74a5406b93936121320f5af4e60165ed9d1795a657002f6ad2f32710\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_xwbg3qd/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: tqdm, regex, ftfy, clip\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [clip]\n","\u001b[1A\u001b[2KSuccessfully installed clip-1.0 ftfy-6.3.1 regex-2025.7.34 tqdm-4.67.1\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting gdown\n","  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting beautifulsoup4 (from gdown)\n","  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: filelock in /2025-ai-expert/.venv/lib/python3.10/site-packages (from gdown) (3.18.0)\n","Collecting requests[socks] (from gdown)\n","  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: tqdm in /2025-ai-expert/.venv/lib/python3.10/site-packages (from gdown) (4.67.1)\n","Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n","  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /2025-ai-expert/.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (4.14.1)\n","Collecting charset_normalizer<4,>=2 (from requests[socks]->gdown)\n","  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n","Collecting idna<4,>=2.5 (from requests[socks]->gdown)\n","  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests[socks]->gdown)\n","  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting certifi>=2017.4.17 (from requests[socks]->gdown)\n","  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n","Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n","  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n","Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n","Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n","Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n","Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n","Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n","Downloading idna-3.10-py3-none-any.whl (70 kB)\n","Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n","Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n","Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n","Installing collected packages: urllib3, soupsieve, PySocks, idna, charset_normalizer, certifi, requests, beautifulsoup4, gdown\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [gdown]32m7/9\u001b[0m [beautifulsoup4]\n","\u001b[1A\u001b[2KSuccessfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.3 gdown-5.2.0 idna-3.10 requests-2.32.4 soupsieve-2.7 urllib3-2.5.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git\n","!pip install gdown"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Lz1aQ0UIudIj"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]},{"name":"stderr","output_type":"stream","text":["/2025-ai-expert/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import numpy as np\n","import torch\n","import random\n","import matplotlib.pyplot as plt\n","import torchvision.utils as tv_utils\n","from tqdm.auto import tqdm\n","import os\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error.\"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-10, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"id":"lObDiZ-wZlHv"},"source":["# Download Dataset\n","\n","We will be using the **CelebA-Dialog** dataset.  \n","CelebA-Dialog is an extension of the CelebA dataset, where each facial image is paired with a **textual description** (caption) that provides information about the person's facial attributes, appearance, or other visual characteristics.  \n","\n","- **Images**: High-quality celebrity face images.  \n","- **Captions**: Human-written descriptions that include various attributes such as hair color, hairstyle, facial expressions, accessories, and more.  "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"claGWZRy7IXT"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/images.zip'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_extract\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     12\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(tmp_extract)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_zip_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[1;32m     14\u001b[0m     members \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m z\u001b[38;5;241m.\u001b[39minfolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m m\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tqdm(members, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnzipping\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1254\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[1;32m   1255\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/images.zip'"]}],"source":["####### Dataset Download Path #######\n","ROOT = Path(\"./data/celeba_dialog\")\n","RAW        = FOLDER_PATH / \"data\"\n","\n","tmp_extract =  ROOT / \"_unzipped\"\n","IMAGES_DIR = ROOT / \"_unzipped\" / \"image\"\n","TEXT_DIR   = ROOT / \"text\"\n","cap_json_path = RAW / \"captions.json\"\n","img_zip_path  = RAW / \"images.zip\"\n","\n","if tmp_extract.exists():\n","    shutil.rmtree(tmp_extract)\n","with zipfile.ZipFile(img_zip_path) as z:\n","    members = [m for m in z.infolist() if not m.is_dir()]\n","    for m in tqdm(members, desc=\"Unzipping\", unit=\"file\"):\n","        z.extract(m, tmp_extract)\n","files = [p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\") for p in tmp_extract.rglob(ext)]\n","\n","moved = 0\n","for src in tqdm(files, desc=\"Organizing images\", unit=\"img\"):\n","    dst = IMAGES_DIR / src.name\n","    if not dst.exists():\n","        shutil.copy2(src, dst)\n","        moved += 1\n","\n","print(f\"[OK] images organized: {moved} files\")\n","\n","jsonl_path = ROOT / \"celeba_dialog.jsonl\"\n","\n","if not jsonl_path.exists():\n","    with open(cap_json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    imgset = {p.name for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\") for p in IMAGES_DIR.glob(ext)}\n","    final = []\n","    for img, value_dict in data.items():\n","        if not img or img not in imgset: continue\n","\n","        if isinstance(value_dict.get(\"overall_caption\"), str):\n","            caps = [value_dict[\"overall_caption\"].strip()]\n","        else:\n","            caps = [\"a portrait photo\"]\n","        for c in caps:\n","            final.append({\"image\": img, \"caption\": c})\n","\n","    if not final:\n","        final = [{\"image\": img, \"caption\": \"a portrait photo\"} for img in sorted(imgset)]\n","\n","    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(json.dumps(r, ensure_ascii=False) for r in final))\n","    print(f\"[OK] JSONL written: {len(final)} → {jsonl_path}\")\n","\n","else:\n","    print(f\"[SKIP] JSONL exists: {jsonl_path}\")"]},{"cell_type":"markdown","metadata":{"id":"-WSecDvXZxn7"},"source":["# Look into Dataset\n","\n","- Load and inspect a few samples from the CelebADialog dataset.\n","- Verify that image–caption pairs are correctly aligned."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"48eymY5s-RSw"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|███████████████████████████████████████| 338M/338M [00:12<00:00, 28.2MiB/s]\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './data/celeba_dialog/celeba_dialog.jsonl'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     14\u001b[0m JSONL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/celeba_dialog/celeba_dialog.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCelebADialogDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGES_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJSONL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_embedder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_embedder\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/2025-ai-expert/알고리즘/02_김승룡교수님/ai_expert/celebadialog_dataset.py:41\u001b[0m, in \u001b[0;36mCelebADialogDataset.__init__\u001b[0;34m(self, img_dir, ann_jsonl, image_size, normalize, clip_embedder, num_text_emb_pca)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39massert\u001b[39;00m clip_embedder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mPass a ClipEmbed instance\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dir \u001b[39m=\u001b[39m Path(img_dir)\n\u001b[0;32m---> 41\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(ann_jsonl, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m f \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39mstrip()]\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m get_transform(image_size, normalize)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/celeba_dialog/celeba_dialog.jsonl'"]}],"source":["import json, re\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import torchvision.transforms as T\n","from pathlib import Path\n","from ai_expert.celebadialog_dataset import ClipEmbed, CelebADialogDataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","clip_embedder = ClipEmbed(device)\n","\n","image_size = 128\n","JSONL_PATH = f\"./data/celeba_dialog/celeba_dialog.jsonl\"\n","dataset = CelebADialogDataset(IMAGES_DIR, JSONL_PATH, image_size=image_size,\n","                                  normalize=False, clip_embedder=clip_embedder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9j0GNaPAj01"},"outputs":[],"source":["def visualize_samples(dataset, num_samples=16, grid_size=(4, 4), max_text_len=50):\n","    # Randomly sample indices\n","    indices = random.sample(range(len(dataset)), num_samples)\n","    samples = [dataset[i] for i in indices]\n","\n","    # Inspect one sample\n","    img_shape = list(samples[0][0].shape)\n","    emb_shape = list(samples[0][1][\"text_emb\"].shape)\n","    print(f\"One sample: (image: {img_shape}, {{ \\\"text_emb\\\": {emb_shape}, \\\"text\\\": string }})\")\n","\n","    # Extract images and texts\n","    images = torch.stack([sample[0] for sample in samples])  # Stack images\n","    texts = [sample[1][\"text\"] for sample in samples]  # Extract text descriptions\n","\n","    # Create a grid of images\n","    grid_img = tv_utils.make_grid(images, nrow=grid_size[1], padding=2)\n","\n","    # Convert to numpy for plotting\n","    grid_img = grid_img.permute(1, 2, 0).numpy()\n","\n","    # Plot the images\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    ax.imshow(grid_img)\n","    ax.axis(\"off\")\n","\n","    # Add text annotations\n","    grid_w, grid_h = grid_size\n","    img_w, img_h = grid_img.shape[1] // grid_w, grid_img.shape[0] // grid_h\n","\n","    for i, text in enumerate(texts):\n","        row, col = divmod(i, grid_w)\n","        x, y = col * img_w, row * img_h\n","\n","        # Wrap text\n","        wrapped_text = \"\"\n","        words = text.split()\n","        current_line = \"\"\n","        for word in words:\n","            if len(current_line) + len(word) + 1 <= max_text_len:\n","                if current_line:\n","                    current_line += \" \"\n","                current_line += word\n","            else:\n","                wrapped_text += current_line + \"\\n\"\n","                current_line = word\n","        wrapped_text += current_line\n","\n","        ax.text(x+5, y+5, wrapped_text, fontsize=6, color='white', bbox=dict(facecolor='black', alpha=0.5))\n","\n","    plt.show()\n","\n","visualize_samples(dataset)"]},{"cell_type":"markdown","metadata":{"id":"xp9hJg0hHA4E"},"source":["# Implement Gaussian Diffusion (Q1, Q2, Q4, Q5, Q6)\n","\n","From this point on, you will implement several methods in the `GaussianDiffusion` class.  \n","Fill in the sections marked with `# TODO` comments and run the cells\n","If the implementation is incorrect, you will not meet the requirements in the following cells.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7Naf55pHCv_"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from tqdm.auto import tqdm\n","import math\n","\n","\n","class GaussianDiffusion(nn.Module):\n","    def __init__(\n","        self,\n","        model,\n","        *,\n","        image_size,\n","        timesteps=1000,\n","        objective=\"pred_noise\",\n","        beta_schedule=\"sigmoid\",\n","    ):\n","        super().__init__()\n","\n","        self.model = model\n","        self.channels = 3\n","        self.image_size = image_size\n","        self.objective = objective\n","        assert objective in {\n","            \"pred_noise\",\n","            \"pred_x_start\",\n","        }, \"objective must be either pred_noise (predict noise) or pred_x_start (predict image start)\"\n","\n","        # A helper function to register some constants as buffers to ensure that\n","        # they are on the same device as model parameters.\n","        # See https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","        # Each buffer can be accessed as `self.name`\n","        register_buffer = lambda name, val: self.register_buffer(name, val.float())\n","\n","        #############################################################################\n","        # Noise schedule beta and alpha values\n","        #############################################################################\n","        betas = get_beta_schedule(beta_schedule, timesteps)\n","        self.num_timesteps = int(betas.shape[0])\n","        alphas = 1.0 - betas\n","        alphas_cumprod = torch.cumprod(alphas, dim=0)  # alpha_bar_t\n","        register_buffer(\"betas\", betas)  # can be accessed as self.betas\n","        register_buffer(\"alphas\", alphas)  # can be accessed as self.alphas\n","        register_buffer(\"alphas_cumprod\", alphas_cumprod)  # self.alphas_cumprod\n","\n","        #############################################################################\n","        # Other coefficients needed to transform between x_t, x_0, and noise\n","        # Note that according to Eq. (4) and its reparameterization in Eq. (14),\n","        # x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n","        # where noise is sampled from N(0, 1)\n","        #############################################################################\n","        register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n","        register_buffer(\n","            \"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod)\n","        )\n","        # register_buffer(\"sqrt_recip_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod))\n","        # register_buffer(\n","        #     \"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod - 1)\n","        # )\n","\n","        #############################################################################\n","        # For posterior q(x_{t-1} | x_t, x_0) according to Eq. (6) and (7) of the paper.\n","        #############################################################################\n","        # alpha_bar_{t-1}\n","        alphas_cumprod_prev = nn.functional.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n","        register_buffer(\n","            \"posterior_mean_coef1\",\n","            betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),\n","        )\n","        register_buffer(\n","            \"posterior_mean_coef2\",\n","            (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod),\n","        )\n","        posterior_var = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n","        posterior_std = torch.sqrt(posterior_var.clamp(min=1e-20))\n","        register_buffer(\"posterior_std\", posterior_std)\n","\n","        #################################################################\n","        # loss weight\n","        #################################################################\n","        snr = alphas_cumprod / (1 - alphas_cumprod)\n","        loss_weight = torch.ones_like(snr) if objective == \"pred_noise\" else snr\n","        register_buffer(\"loss_weight\", loss_weight)\n","\n","    def normalize(self, img):\n","        return img * 2 - 1\n","\n","    def unnormalize(self, img):\n","        return (img + 1) * 0.5\n","\n","    def predict_start_from_noise(self, x_t, t, noise):\n","        \"\"\"Get x_start from x_t and noise according to Eq. (14) of the paper.\n","        Args:\n","            x_t: (b, *) tensor. Noisy image.\n","            t: (b,) tensor. Time step.\n","            noise: (b, *) tensor. Noise from N(0, 1).\n","        Returns:\n","            x_start: (b, *) tensor. Starting image.\n","        \"\"\"\n","        x_start = None\n","        ####################################################################\n","        # Q2.TODO:\n","        # Transform x_t and noise to get x_start according to Eq.(4) and Eq.(14).\n","        # Look at the coeffs in `__init__` method and use the `extract` function.\n","        ####################################################################\n","\n","        # YOUR CODE FROM HERE\n","\n","\n","        ####################################################################\n","        return x_start\n","\n","    def predict_prev_from_noise_ddim(self ,x_t, t, t_prev, noise, eta : float = 0.0):\n","        a_t = extract(self.alphas_cumprod, t, x_t.shape)\n","        a_prev = extract(self.alphas_cumprod, t_prev, x_t.shape)\n","        z = torch.randn_like(x_t) if (eta > 0) else torch.zeros_like(x_t) # random noise\n","        x_prev = None\n","        ####################################################################\n","        # Q6. TODO:\n","        # Compute x_{t-1} from (x_t, predicted noise) via DDIM update (recover x0, then apply DDIM formula).\n","        ####################################################################\n","        # YOUR CODE FROM HERE\n","        ####################################################################\n","        return x_prev\n","\n","    def predict_noise_from_start(self, x_t, t, x_start):\n","        \"\"\"Get noise from x_t and x_start according to Eq. (14) of the paper.\n","        Args:\n","            x_t: (b, *) tensor. Noisy image.\n","            t: (b,) tensor. Time step.\n","            x_start: (b, *) tensor. Starting image.\n","        Returns:\n","            pred_noise: (b, *) tensor. Predicted noise.\n","        \"\"\"\n","        pred_noise = None\n","        ####################################################################\n","        # Q2.TODO:\n","        # Transform x_t and noise to get x_start according to Eq.(4) and Eq.(14).\n","        # Look at the coeffs in `__init__` method and use the `extract` function.\n","        ####################################################################\n","\n","        # YOUR CODE FROM HERE\n","        \n","\n","        ####################################################################\n","        return pred_noise\n","\n","    def predict_prev_from_start_ddim(self, x_t, t, t_prev, x_start, eta : float = 0.0 ):\n","        a_t = extract(self.alphas_cumprod, t, x_t.shape)\n","        a_prev = extract(self.alphas_cumprod, t_prev, x_t.shape)\n","        z = torch.randn_like(x_t) if (eta > 0) else torch.zeros_like(x_t)\n","        x_prev = None\n","        ####################################################################\n","        # Q6.TODO: Compute x_{t-1} from (x_t, predicted x0) via DDIM update.\n","        ####################################################################\n","        # YOUR CODE FROM HERE\n","        ####################################################################\n","\n","        return x_prev\n","\n","\n","    def q_posterior(self, x_start, x_t, t):\n","        \"\"\"Get the posterior q(x_{t-1} | x_t, x_0) according to Eq. (6) and (7) of the paper.\n","        Args:\n","            x_start: (b, *) tensor. Predicted start image.\n","            x_t: (b, *) tensor. Noisy image.\n","            t: (b,) tensor. Time step.\n","        Returns:\n","            posterior_mean: (b, *) tensor. Mean of the posterior.\n","            posterior_std: (b, *) tensor. Std of the posterior.\n","        \"\"\"\n","        posterior_mean = None\n","        posterior_std = None\n","        ####################################################################\n","        # We have already implemented this method for you.\n","        c1 = extract(self.posterior_mean_coef1, t, x_t.shape)\n","        c2 = extract(self.posterior_mean_coef2, t, x_t.shape)\n","        posterior_mean = c1 * x_start + c2 * x_t\n","        posterior_std = extract(self.posterior_std, t, x_t.shape)\n","        ####################################################################\n","        return posterior_mean, posterior_std\n","\n","\n","    @torch.no_grad()\n","    def p_sample(self, x_t, t: int, model_kwargs={}):\n","        \"\"\"Sample from p(x_{t-1} | x_t) according to Eq. (6) of the paper. Used only during inference.\n","        Args:\n","            x_t: (b, *) tensor. Noisy image.\n","            t: int. Sampling time step.\n","            model_kwargs: additional arguments for the model.\n","        Returns:\n","            x_tm1: (b, *) tensor. Sampled image.\n","        \"\"\"\n","        t = torch.full((x_t.shape[0],), t, device=x_t.device, dtype=torch.long)  # (b,)\n","        x_tm1 = None  # sample x_{t-1} from p(x_{t-1} | x_t)\n","\n","        ##################################################################\n","        # Q5. TODO: Implement the sampling step p(x_{t-1} | x_t) according to Eq. (6):\n","        #\n","        # - Steps:\n","        #   1. Get the model prediction by calling self.model with appropriate args.\n","        #   2. The model output can be either noise or x_start depending on self.objective.\n","        #      You can recover the other by calling self.predict_start_from_noise or\n","        #      self.predict_noise_from_start as needed.\n","        #   3. Clamp predicted x_start to the valid range [-1, 1]. This ensures the\n","        #      generation remains stable during denoising iterations.\n","        #   4. Get the mean and std for q(x_{t-1} | x_t, x_0) using self.q_posterior,\n","        #      and sample x_{t-1}.\n","        ##################################################################\n","\n","         # YOUR CODE FROM HERE\n","\n","        # Call model to predict x_start (or noise)\n","\n","        if self.objective == \"pred_noise\":\n","            pass\n","            # Model predicted noise, not x_start; we need to convert\n","\n","        # Get the posterior mean and standard deviation, sample backwards\n","\n","        ##################################################################\n","        return x_tm1\n","\n","    @torch.no_grad()\n","    def p_sample_ddim(self, x_t, t:int, t_prev:int, eta:float=0.0, model_kwargs={}):\n","        t = torch.full((x_t.shape[0],), t, device=x_t.device, dtype=torch.long)\n","        t_prev = torch.full((x_t.shape[0],), t_prev, device=x_t.device, dtype=torch.long)\n","        x_tm1 = None # sample x_{t-1} from p(x_{t-1} | x_t)\n","        ####################################################################\n","        # Q6. TODO:\n","        # # Predict noise or x0 from x_t, then compute x_{t-1} using DDIM.\n","        ####################################################################\n","\n","        # YOUR CODE FROM HERE\n","\n","        ####################################################################\n","\n","        return x_tm1\n","\n","\n","    @torch.no_grad()\n","    def sample(self, batch_size=16, return_all_timesteps=False, model_kwargs={}):\n","\n","        shape = (batch_size, self.channels, self.image_size, self.image_size)\n","        img = torch.randn(shape, device=self.betas.device)\n","        imgs = [img]\n","\n","        for t in tqdm(\n","            reversed(range(0, self.num_timesteps)),\n","            desc=\"sampling ddpm loop time step\",\n","            total=self.num_timesteps,\n","        ):\n","            img = self.p_sample(img, t, model_kwargs=model_kwargs)\n","            imgs.append(img)\n","\n","        res = img if not return_all_timesteps else torch.stack(imgs, dim=1)\n","        res = self.unnormalize(res)\n","        return res\n","\n","    @torch.no_grad()\n","    def sample_ddim(self, batch_size=16, eta:float=0.0, return_all_timesteps=False, steps=None, model_kwargs={}):\n","\n","        if steps == None:\n","            steps=self.num_timesteps\n","\n","        assert 1 <= steps <= self.num_timesteps, \"steps must be in [1, num_timesteps]\"\n","\n","        shape = (batch_size, self.channels, self.image_size, self.image_size)\n","        img = torch.randn(shape, device=self.betas.device)\n","\n","        t_seq = torch.linspace(self.num_timesteps - 1, 0, steps+1, device=img.device)\n","        t_seq = torch.round(t_seq).long().tolist()\n","\n","        imgs = [img]\n","        for i in tqdm(\n","            range(len(t_seq) - 1),\n","            desc=\"sampling ddim loop time step\",\n","            total=steps\n","        ):\n","            t = int(t_seq[i])\n","            t_prev = int(t_seq[i+1])\n","            img = self.p_sample_ddim(img, t, t_prev, eta=eta, model_kwargs=model_kwargs)\n","            imgs.append(img)\n","\n","        res = img if not return_all_timesteps else torch.stack(imgs,dim=1)\n","        res = self.unnormalize(res)\n","\n","        return res\n","\n","    def q_sample(self, x_start, t, noise):\n","        \"\"\"Sample from q(x_t | x_0) according to Eq. (4) of the paper.\n","\n","        Args:\n","            x_start: (b, *) tensor. Starting image.\n","            t: (b,) tensor. Time step.\n","            noise: (b, *) tensor. Noise from N(0, 1).\n","        Returns:\n","            x_t: (b, *) tensor. Noisy image.\n","        \"\"\"\n","\n","        x_t = None\n","        ####################################################################\n","        # Q1. TODO:\n","        # Implement sampling from q(x_t | x_0) according to Eq. (4) of the paper.\n","        # Hints: (1) Look at the `__init__` method to see precomputed coefficients.\n","        # (2) Use the `extract` function defined above to extract the coefficients\n","        # for the given time step `t`. (3) Recall that sampling from N(mu, sigma^2)\n","        # can be done as: x_t = mu + sigma * noise where noise is sampled from N(0, 1).\n","        # Approximately 3 lines of code.\n","        ###################################################################\n","        alpha_sqrt = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n","        sigma = extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n","        x_t = alpha_sqrt\n","\n","        \n","\n","        ####################################################################\n","        return x_t\n","\n","    def p_losses(self, x_start, model_kwargs={}):\n","        b, nts = x_start.shape[0], self.num_timesteps\n","        t = torch.randint(0, nts, (b,), device=x_start.device).long()  # (b,)\n","        x_start = self.normalize(x_start)  # (b, *)\n","        noise = torch.randn_like(x_start)  # (b, *)\n","        target = noise if self.objective == \"pred_noise\" else x_start  # (b, *)\n","        loss_weight = extract(self.loss_weight, t, target.shape)  # (b, *)\n","        loss = None\n","\n","        ####################################################################\n","        # Q4. TODO:\n","        # Implement the loss function according to Eq. (14) of the paper.\n","        # First, sample x_t from q(x_t | x_0) using the `q_sample` function.\n","        # Then, get model predictions by calling self.model with appropriate args.\n","        # Finally, compute the weighted MSE loss.\n","        # Approximately 3-4 lines of code.\n","        ####################################################################\n","\n","        # YOUR CODE FROM HERE\n","\n","        ####################################################################\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMbt8DIFHgT9"},"outputs":[],"source":["def extract(a, t, x_shape):\n","    \"\"\"\n","    Extracts the appropriate coefficient values based on the given timesteps.\n","\n","    This function gathers the values from the coefficient tensor `a` according to\n","    the given timesteps `t` and reshapes them to match the required shape such that\n","    it supports broadcasting with the tensor of given shape `x_shape`.\n","\n","    Args:\n","        a (torch.Tensor): A tensor of shape (T,), containing coefficient values for all timesteps.\n","        t (torch.Tensor): A tensor of shape (b,), representing the timesteps for each sample in the batch.\n","        x_shape (tuple): The shape of the input image tensor, usually (b, c, h, w).\n","\n","    Returns:\n","        torch.Tensor: A tensor of shape (b, 1, 1, 1), containing the extracted coefficient values\n","                      from a for corresponding timestep of each batch element, reshaped accordingly.\n","    \"\"\"\n","    b, *_ = t.shape  # Extract batch size from the timestep tensor\n","    out = a.gather(-1, t)  # Gather the coefficient values from `a` based on `t`\n","    out = out.reshape(\n","        b, *((1,) * (len(x_shape) - 1))\n","    )  # Reshape to (b, 1, 1, 1) for broadcasting\n","    return out\n","\n","\n","def linear_beta_schedule(timesteps):\n","    \"\"\"\n","    linear schedule, proposed in original ddpm paper\n","    \"\"\"\n","    scale = 1000 / timesteps\n","    beta_start = scale * 0.0001\n","    beta_end = scale * 0.02\n","    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n","\n","\n","def cosine_beta_schedule(timesteps, s=0.008):\n","    \"\"\"\n","    cosine schedule\n","    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n","    \"\"\"\n","    steps = timesteps + 1\n","    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n","    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n","    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n","    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n","    return torch.clip(betas, 0, 0.999)\n","\n","\n","def sigmoid_beta_schedule(timesteps, start=-3, end=3, tau=1, clamp_min=1e-5):\n","    \"\"\"\n","    sigmoid schedule\n","    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n","    better for images > 64x64, when used during training\n","    \"\"\"\n","    steps = timesteps + 1\n","    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n","    v_start = torch.tensor(start / tau).sigmoid()\n","    v_end = torch.tensor(end / tau).sigmoid()\n","    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (\n","        v_end - v_start\n","    )\n","    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n","    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n","    return torch.clip(betas, 0, 0.999)\n","\n","\n","def get_beta_schedule(beta_schedule, timesteps):\n","    if beta_schedule == \"linear\":\n","        beta_schedule_fn = linear_beta_schedule\n","    elif beta_schedule == \"cosine\":\n","        beta_schedule_fn = cosine_beta_schedule\n","    elif beta_schedule == \"sigmoid\":\n","        beta_schedule_fn = sigmoid_beta_schedule\n","    else:\n","        raise ValueError(f\"unknown beta schedule {beta_schedule}\")\n","\n","    betas = beta_schedule_fn(timesteps)\n","    return betas"]},{"cell_type":"markdown","metadata":{"id":"Px8xT4J2GT3s"},"source":["## **Q1. q_sample**\n","\n","Now we will define the forward noising process. Consult the original DDPM paper[1] for the equations. Implement `q_sample` method and test it below. You should see **zero** relative error.\n","\n","$$\n","q(x_t \\mid x_0) = \\mathcal{N} \\left( x_t \\; ; \\; \\sqrt{\\bar{\\alpha}_t} \\, x_0, \\; (1 - \\bar{\\alpha}_t) \\, \\mathbf{I} \\right)\n","$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MErXLgGJBMqs"},"outputs":[],"source":["# Test GaussianDiffusion.q_sample method\n","sz = 2\n","b = 3 # batch size\n","\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=sz,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n",")\n","\n","t = torch.tensor([0, 300, 999]).long()\n","x_start = torch.linspace(-0.9, 0.6, b*3*sz*sz).view(b, 3, sz, sz)\n","noise = torch.linspace(-0.7, 0.8, b*3*sz*sz).view(b, 3, sz, sz)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","\n","expected_x_t = np.array([\n","    [\n","        [[-0.9119949, -0.86840147], [-0.8248081, -0.7812148]],\n","        [[-0.7376214, -0.694028], [-0.65043473, -0.6068413]],\n","        [[-0.563248, -0.51965463], [-0.47606122, -0.43246788]],\n","    ],\n","    [\n","        [[-0.42800453, -0.37039882], [-0.31279305, -0.2551873]],\n","        [[-0.19758154, -0.1399758], [-0.08237009, -0.024764337]],\n","        [[0.032841414, 0.090447165], [0.14805292, 0.20565866]],\n","    ],\n","    [\n","        [[0.32864183, 0.37152246], [0.41440308, 0.45728368]],\n","        [[0.50016433, 0.5430449], [0.5859255, 0.6288062]],\n","        [[0.67168677, 0.7145674], [0.757448, 0.8003287]],\n","    ],\n","]).astype(np.float32)\n","\n","# Should see zero relative error\n","error = rel_error(x_t.numpy(), expected_x_t)\n","print(\"x_t error: \", rel_error(x_t.numpy(), expected_x_t))\n","\n","if error == 0.0 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SkIto96GacN"},"outputs":[],"source":["# Let's visualize the noisy images at various timesteps.\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=image_size,\n","      timesteps=1000,\n",")\n","\n","B = 10\n","img = dataset[770][0]  # 3 x H x W\n","x_start = img[None].repeat(B, 1, 1, 1)  # B x 3 x H x W\n","noise = torch.randn_like(x_start)  # B x 3 x H x W\n","t = torch.linspace(0, 1000-1, B).long()\n","\n","x_start = diffusion.normalize(x_start)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","x_t = diffusion.unnormalize(x_t).clamp(0, 1)\n","grid_img = tv_utils.make_grid(x_t, nrow=5, padding=2)\n","grid_img = grid_img.permute(1, 2, 0).cpu().numpy()\n","fig, ax = plt.subplots(figsize=(10, 10))\n","ax.imshow(grid_img)\n","ax.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NqZIi8lIevGc"},"source":["# **Q2. Predict Noise/Clean Image**\n","A diffusion model can be trained to predict either the clean image or the noise, as one can be derived from the other.\n","\n","Forward noising process can be written as:\n","\n","$$\n","x_t = \\sqrt{\\bar{\\alpha}_t} \\; x_0 + \\sigma_t \\; \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n","$$\n","\n","Implement `predict_start_from_noise` and `predict_noise_from_start` methods and test them below. You should see relative error less than **1e-5**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pf5e1rh-de4T"},"outputs":[],"source":["# Test `predict_noise_from_start` and `predict_start_from_noise`\n","sz = 2\n","b = 3\n","\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=sz,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n",")\n","\n","t = torch.tensor([1, 300, 998]).long()\n","x_start = torch.linspace(-0.91, 0.67, b*3*sz*sz).view(b, 3, sz, sz)\n","noise = torch.linspace(-0.73, 0.81, b*3*sz*sz).view(b, 3, sz, sz)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","\n","pred_noise = diffusion.predict_noise_from_start(x_t, t, x_start)\n","pred_x_start = diffusion.predict_start_from_noise(x_t, t, noise)\n","\n","# Should relative errors around 1e-5 or less\n","noise_error = rel_error(pred_noise.numpy(), noise.numpy())\n","print(\"noise error: \", noise_error)\n","\n","if noise_error < 1e-5 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")\n","\n","x_start_error = rel_error(pred_x_start.numpy(), x_start.numpy())\n","print(\"x_start error: \", x_start_error )\n","\n","if x_start_error < 1e-5 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")"]},{"cell_type":"markdown","metadata":{"id":"3uUpzI6whyTI"},"source":["# **Q3. UNet for Denoising**\n","\n","So far, we have focused on the **`forward process`**, which gradually adds noise to the clean image.\n","Now, we turn to the **`reverse process`**—progressively removing noise to recover the original image.\n","We will use a **UNet** model for denoising the input image during this reverse diffusion process.\n","\n","**What is UNet?**  \n","UNet is a neural network architecture originally designed for image-to-image tasks such as segmentation, style transfer, and inpainting.  \n","It consists of:  \n","- **Encoder (Downsampling path):** progressively reduces the spatial resolution while increasing the number of feature channels to extract high-level representations.  \n","- **Decoder (Upsampling path):** progressively restores the spatial resolution, mirroring the encoder’s structure.  \n","- **Skip connections:** directly connect encoder and decoder layers at the same scale, allowing the decoder to recover fine-grained details without relying solely on the bottleneck features.\n","\n","**Why UNet here?**  \n","\n","Here's 4th question!\n","> 💡 **Question:** Why do you think UNet is a good choice for this denoising task?\n","\n","**Answer** : WRITE YOUR OWN ANSWER HERE\n","\n","In a later lecture, we will explore **DiT** (Diffusion Transformer), which replaces UNet with a pure transformer-based architecture for image generation.\n"]},{"cell_type":"markdown","metadata":{"id":"2vqAxcZb7PQ9"},"source":["# **Q4. p_losses**\n","\n","Now that we have model implementation done, let's write the DDPM's denoising training step. As mentioned before, optimizing the denoising loss is equivalent to minimizing the expected negative log likelihood of the dataset. Complete the `GaussianDiffusion.p_losses` method and test it below. You should see relative error less than 1e-6 .\n","\n","$$\n","\\mathcal{L} = \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}}\n","\\left[ \\left\\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta \\left( \\mathbf{x}_t, t \\right) \\right\\|_2^2 \\right]\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgh9UhC8esQj"},"outputs":[],"source":["from ai_expert.unet import Unet, ResnetBlock, Downsample, Upsample\n","\n","np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2, 4)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","h = w = 4\n","b = 3\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n","      objective=\"pred_x_start\",\n",")\n","\n","inp_x = torch.randn(b, 3, h, w)\n","inp_model_kwargs = {\"text_emb\": torch.randn(b, condition_dim)}\n","out = diffusion.p_losses(inp_x, inp_model_kwargs)\n","expected_out = 30.0732689\n","\n","forward_error = rel_error(out.item(), expected_out)\n","print(\"forward error: \", forward_error)\n","\n","if forward_error < 1e-6 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")"]},{"cell_type":"markdown","metadata":{"id":"RZTFsGpK7wiV"},"source":["## **Q5. p_sample**\n","\n","There is one final ingredient remaining now. DDPM generates samples by iteratively performing the reverse process. Each iteration of this reverse process involves sampling from $p(x_{t-1}|x_t)$. Implement `GaussianDiffusion.p_sample` method by following Equation (6) from the paper. This equation describes sampling from the posterior of the forward process, conditioned on $x_t$ and $x_0$, where $x_0$ can be derived from the denoising model's output. We have already implemented `sample` method that iteratively calls `p_sample` to generate images from input texts.\n","\n","Test your implementation of `p_sample` below, you should see relative errors less than 1e-6.\n","\n","$$\n","p_\\theta(x_{t-1} \\mid x_t) =\n","\\mathcal{N}\\left(\n","x_{t-1} \\; ; \\;\n","\\mu_\\theta(x_t, t), \\; \\sigma_t^2 \\mathbf{I}\n","\\right)\n","$$\n","\n","> 🔎 **Reference:** Consult the original DDPM paper[1] around **Equation (6)** (posterior mean/variance)\n","> and **Equation (11)** (reparameterization using the model prediction)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWFo6kkffPAD"},"outputs":[],"source":["np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2,)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","h = w = 4\n","b = 1\n","inp_x_t = torch.randn(b, 3, h, w)\n","inp_model_kwargs = {\"text_emb\": torch.randn(b, condition_dim)}\n","t = 231\n","\n","# test 1\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n","      objective=\"pred_x_start\",\n",")\n","out = diffusion.p_sample(inp_x_t, t, inp_model_kwargs).detach().numpy()\n","expected_out = np.array(\n","    [[[[ 1.1339471 ,  0.12097352, -0.7175048 ,  1.3196243 ],\n","         [-0.27657282,  0.4899886 ,  1.0170169 , -0.8242867 ],\n","         [-0.18946372,  0.9899801 ,  0.01498353,  0.39722288],\n","         [-0.97995025, -0.5947938 , -0.07796463, -0.07311387]],\n","\n","        [[ 0.0739838 , -1.5537696 ,  0.43128064, -0.7395982 ],\n","         [-1.0517508 , -1.7030833 ,  0.79073197, -1.217138  ],\n","         [-0.5314434 ,  0.9862699 ,  0.6568664 , -0.4559122 ],\n","         [-0.17322278,  0.51251256, -0.75741345, -0.3967054 ]],\n","\n","        [[ 0.8546979 ,  1.6186953 ,  1.9930652 ,  0.57347   ],\n","         [ 0.20219846,  0.5374655 , -0.81597316,  1.9089762 ],\n","         [ 0.7327057 ,  1.19275   ,  1.8593936 , -1.4582647 ],\n","         [ 0.68447256, -0.9056745 ,  0.7863245 ,  0.14455058]]]])\n","forward_error = rel_error(out, expected_out)\n","print(\"forward error: \", forward_error)\n","if forward_error < 1e-6 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")\n","\n","# test 2\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"cosine\",\n","      objective=\"pred_noise\",\n",")\n","out = diffusion.p_sample(inp_x_t, t, inp_model_kwargs).detach().numpy()\n","expected_out = np.array(\n","    [[[[ 1.1036711 ,  0.08143333, -0.6856102 ,  1.3826138 ],\n","         [-0.25455472,  0.514572  ,  1.104592  , -0.75972646],\n","         [-0.22729763,  0.9837706 ,  0.05891411,  0.52049375],\n","         [-1.0331786 , -0.5416254 , -0.01623197, -0.04838388]],\n","\n","        [[ 0.08324978, -1.545468  ,  0.41357145, -0.63511896],\n","         [-1.1362139 , -1.7128816 ,  0.8694859 , -1.2297069 ],\n","         [-0.49168122,  1.0043695 ,  0.6759953 , -0.5297671 ],\n","         [-0.10931232,  0.52347076, -0.80946106, -0.5015002 ]],\n","\n","        [[ 0.7437265 ,  1.590004  ,  1.9481117 ,  0.5656144 ],\n","         [ 0.22895451,  0.5289113 , -0.8511001 ,  1.8864397 ],\n","         [ 0.72863096,  1.2271638 ,  1.892699  , -1.5199479 ],\n","         [ 0.64346373, -0.86913294,  0.7869012 ,  0.12637165]]]])\n","\n","forward_error = rel_error(out, expected_out)\n","print(\"forward error: \", forward_error)\n","if forward_error < 1e-6 : print(\"Passed! You did it\")\n","else : print(\"Failed! Try again\")"]},{"cell_type":"markdown","metadata":{"id":"kfUs5An5jW_P"},"source":["# **Training**\n","\n","We have all ingredients needed for DDPM training and we can train the model on our CelebADialog dataset. You don't have to code anything here but we encourage you to look at the training code at `ai_expert/ddpm_trainer.py` in [ this link](https://github.com/rlawldud53/AIExpert_Samsung/tree/main/ai_expert).\n","\n","For the rest of the notebook, we will use a pretrained model which is already trained for many iterations on this dataset. However, you are free to train your own model on colab GPU (make sure to change the `results_folder`). Note that it may take more than 12 hours on T4 GPU before you start seeing a reasonable generation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dzEyKnH6VSL"},"outputs":[],"source":["from ai_expert.ddpm_trainer import Trainer\n","\n","dim = 48\n","image_size = 128\n","results_folder = FOLDER_PATH / \"pretrained_model\"\n","condition_dim = 512\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = Unet(\n","    dim=dim,\n","    dim_mults=(1, 2, 4, 8),\n","    condition_dim=condition_dim,\n",")\n","print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n","\n","diffusion = GaussianDiffusion(\n","    model,\n","    image_size=image_size,\n","    timesteps=100,  # number of diffusion steps\n","    objective=\"pred_noise\",  # \"pred_x_start\" or \"pred_noise\"\n",")\n","\n","dataset = CelebADialogDataset(  image_size=image_size,\n","                                    img_dir = \"./data/celeba_dialog/images\",\n","                                    ann_jsonl=\"./data/celeba_dialog/celeba_dialog.jsonl\",\n","                                    clip_embedder=clip_embedder)\n","\n","trainer = Trainer(\n","    diffusion,\n","    dataset,\n","    device,\n","    train_batch_size=256,\n","    weight_decay=0.0,\n","    train_lr=1e-3,\n","    train_num_steps=50000,\n","    results_folder=results_folder,\n",")\n","\n","# trainer.train() # If you want to train your own model from scratch, uncomment this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTW0oLsPKuoY"},"outputs":[],"source":["trainer.load(7000)"]},{"cell_type":"markdown","metadata":{"id":"SKD1a4Uqof_T"},"source":["# **Q6. DDIM Sampling**  \n","\n","In this section, you will implement **DDIM sampling**, an alternative to DDPM sampling that allows for **fewer denoising steps** while maintaining high sample quality.  \n","\n","Unlike DDPM, where we sample from the stochastic posterior $p(x_{t-1} \\mid x_t)$ with variance from the forward process, DDIM introduces a deterministic path when $\\eta = 0$, and controlled stochasticity when $\\eta > 0$.\n","\n","\n","Your task is to implement the **`TODO`** parts in the `GaussianDiffusion`:  \n","\n","- **`predict_prev_from_noise_ddim`**:  \n","Computes $x_{t-1}$ given $x_t$, timestep $t$, noise prediction, and $\\eta$, following the DDIM update rule.  \n","\n","\n","- **`predict_prev_from_start_ddim`**:  \n","Computes $x_{t-1}$ given $x_t$, timestep $t$, predicted $x_0$, and $\\eta$.  \n","\n","\n","- **`p_sample_ddim`**:  \n","  Calls the model to obtain predictions, then chooses the correct function above depending on whether the model predicts **noise** or **\\(x_0\\)**.\n","\n","> 💡 **Hint:**  \n","> - When $\\eta = 0$, DDIM becomes deterministic (no additional random noise is added).  \n","> - When $\\eta > 0$, noise is reintroduced proportionally to $\\eta$.  \n","> - You can reuse your implementations of `predict_start_from_noise` and `predict_noise_from_start` here.  \n","\n","\n","**Expected Outcome:**  \n","When implemented correctly, DDIM sampling should generate coherent images **with fewer steps** compared to DDPM.  \n","\n","**DDIM Update Equation:**  \n","\n","> 🔎 **Reference:** Consult the original DDIM paper [[2]](https://arxiv.org/abs/2010.02502) around **Equation (12)** and **Equation (16)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqoZyGpI80gR"},"outputs":[],"source":["def get_text_emb(text):\n","    return trainer.ds._encode(text)\n","\n","# Helper function to visualize generations.\n","def show_images(img):\n","    # img: B x T x 3 x H x W\n","    plt.figure(figsize=(10, 10))\n","    img2 = img.clamp(0, 1).permute(0, 3, 1, 4, 2).flatten(0, 1).flatten(1, 2).cpu().numpy()\n","    plt.imshow(img2)\n","    plt.axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NE3HCAOSb7BU"},"outputs":[],"source":["text = \"This old lady has extremely frowning face with yellow hair. There are eyeglasses on her face\"  # edit here freely!\n","text_emb = get_text_emb(text)\n","text_emb = text_emb[None].expand(5, -1).to(device)\n","\n","torch.manual_seed(0)\n"]},{"cell_type":"markdown","metadata":{"id":"ei2A0FDW9gzH"},"source":["### DDPM sampling\n","DDPM progressively denoises random Gaussian noise through all timesteps until a clean image emerges.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBoVOe4qp9hx"},"outputs":[],"source":["# Sample\n","# `return_all_timesteps=True` returns the intermediate results at each timestep,\n","ddpm_imgs = trainer.diffusion_model.sample(\n","    batch_size=5,\n","    model_kwargs={\"text_emb\": text_emb},\n","    return_all_timesteps=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-oo_Cx4ITMz"},"outputs":[],"source":["show_images(ddpm_imgs[:, ::20])"]},{"cell_type":"markdown","metadata":{"id":"9G55tRt_9zMa"},"source":["### DDIM Sampling\n","Unlike DDPM, DDIM can use fewer denoising steps (`steps=10`) while maintaining quality, thanks to its non-Markovian update rule."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oRzf_tAB-ce"},"outputs":[],"source":["# Here, `eta=0.0` makes the process **deterministic** (no additional noise is added).\n","# `steps=10` means we skip timesteps to perform sampling in only 10 reverse steps\n","# instead of the full training `timesteps` (e.g., 1000), making generation faster but potentially less accurate.\n","ddim_imgs = trainer.diffusion_model.sample_ddim(\n","    batch_size=5,\n","    steps=100, # you can edit here for faster sampling\n","    eta=0.0,\n","    model_kwargs={\"text_emb\": text_emb},\n","    return_all_timesteps=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VZISUuYCD16"},"outputs":[],"source":["show_images(ddim_imgs[:, ::5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4k1AZfGQ8wt"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.12 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"da12274c09a011afc12b834c578e2507bd3d13f6cb8ca652534137b78250785f"}}},"nbformat":4,"nbformat_minor":0}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hy-e/2025-ai-expert/blob/main/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%EC%9C%A4%EC%84%B1%EC%9D%98/1_Image_Retrieval_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgNzevTjyeNv"
      },
      "source": [
        "#AI Expert - Image Retrieval Basic Practice\n",
        "====\n",
        "진행 조교 : 김우재, 한규범, 이주민\n",
        "\n",
        "## Instruction\n",
        "> 안녕하세요. 본 실습 강의에서는 이미지 검색과 관련된 기본적인 요소들을 직접 구현하며, 딥러닝 기반의 이미지 검색 시스템에 대해 이해하는 것을 목표로 합니다. 본 실습은 이미지 검색의 전체적인 파이프라인을 따라, 아래와 같은 순서로 구성되어 있습니다.\n",
        "## Preparation\n",
        "> 우선 창 왼쪽 상단의 **파일** 탭의, **Drive에 사본 저장** 버튼을 눌러 본 Colab 파일의 사본을 만들고, 실습을 진행하시길 바랍니다.\n",
        "## Reference materials\n",
        "> 아래는 본 과제 실습에서 주로 활용하는 PyTorch, NumPy 의 documentation 입니다.\n",
        "* PyTorch \\[[Documentation](https://pytorch.org/docs/stable/index.html)\\]\n",
        "* NumPy  \\[[Documentation](https://numpy.org/doc/stable/)\\]\n",
        "\n",
        "> 아래는 이미지 검색 모델에 활용되는 기본적인 손실\\(Loss\\) 함수에 대한 이해 및 구현을 도와주는 자료입니다. 실습 진행에 어려움을 겪을 시 활용할 수 있습니다.\n",
        "* Pytorch-metric-learning \\[[Documentation](https://kevinmusgrave.github.io/pytorch-metric-learning/) / [GitHub](https://github.com/KevinMusgrave/pytorch-metric-learning)\\]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulE42TkLeVv"
      },
      "source": [
        "## Step 1: Set the enviroments\n",
        "실습 진행을 위한 기본적인 환경 설정을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdd4sGKyMzx9"
      },
      "source": [
        "### Step 1-1: Import the necessary libraries\n",
        "이미지 검색 시스템 구축에 활용할 라이브러리를 import합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk251QkJeeCc"
      },
      "outputs": [],
      "source": [
        "# Import libraries to develop the basic image retrieval system\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100 # https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QJtPmJbM_2s"
      },
      "source": [
        "### Step 1-2 Construct the dataset for image retireval\n",
        "이미지 검색에 활용할 데이터 셋을 구성합니다. 본 실습에서는 대표적인 이미지 데이터셋인 CIFAR-100을 활용합니다. 제한된 시간에서 효율적인 실습 진행을 위해 해당 데이터셋의 일부만을 활용할 예정입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTg56U6lOFCS"
      },
      "source": [
        "이미지 검색 모델은 기존에 학습하지 않았던 class 데이터에 대해서도 동작 가능합니다. 본 실습에서는 CIFAR-100의 1번\\~50번 class를 학습 데이터로, 51번\\~100번 class를 평가 데이터로 활용합니다. 아래는 CIFAR-100의 데이터 구성을 본 실습 진행에 적합하게 변경하는 작업을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZSFjZ6KOhHi"
      },
      "outputs": [],
      "source": [
        "# Download the original datasets\n",
        "original_train = CIFAR100(root=\"CIFAR100_Dataset\", train=True, transform=None, download=True)\n",
        "original_val = CIFAR100(root=\"CIFAR100_Dataset\", train=False, transform=None, download=True)\n",
        "\n",
        "\n",
        "# This will be used to create train and val sets that are class-disjoint\n",
        "class ClassDisjointCIFAR100(torch.utils.data.Dataset):\n",
        "    def __init__(self, original_train, original_val, train, transform):\n",
        "        rule = (lambda x: x < 50) if train else (lambda x: x >= 50)\n",
        "        train_filtered_idx = [\n",
        "            i for i, x in enumerate(original_train.targets) if rule(x)\n",
        "        ]\n",
        "        val_filtered_idx = [i for i, x in enumerate(original_val.targets) if rule(x)]\n",
        "        self.data = np.concatenate(\n",
        "            [\n",
        "                original_train.data[train_filtered_idx],\n",
        "                original_val.data[val_filtered_idx],\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        self.targets = np.concatenate(\n",
        "            [\n",
        "                np.array(original_train.targets)[train_filtered_idx],\n",
        "                np.array(original_val.targets)[val_filtered_idx],\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "class ClassDisjointCIFAR100Test(torch.utils.data.Dataset):\n",
        "    def __init__(self, original_val, train, transform):\n",
        "        rule = (lambda x: x < 50) if train else (lambda x: x >= 50)\n",
        "        val_filtered_idx = [i for i, x in enumerate(original_val.targets) if rule(x)]\n",
        "        self.data = original_val.data[val_filtered_idx]\n",
        "        self.targets = np.array(original_val.targets)[val_filtered_idx]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doqWLahzO69M"
      },
      "source": [
        "아래는 구축한 CIFAR-100 데이터셋에 대하여 이미지 전처리를 적용하고 해당 데이터셋을 불러오는 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcZPHCG3esbF"
      },
      "outputs": [],
      "source": [
        "# Set the image transforms\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(64),\n",
        "        transforms.RandomResizedCrop(scale=(0.16, 1), ratio=(0.75, 1.33), size=64),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Load class disjoint CIFAR-100 training and validation set\n",
        "train_dataset = ClassDisjointCIFAR100(original_train, original_val, True, train_transform)\n",
        "test_dataset = ClassDisjointCIFAR100Test(original_val, False, test_transform)  # We use only the validation dataset for testing because of the computation time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mg4CAocQbwL"
      },
      "source": [
        "### Step 1-3 Construct the dataloader for training and testing\n",
        "학습 및 평가에 활용할 데이터를 불러오는 data loader를 생성합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOYL_vpyPT0Q"
      },
      "source": [
        "이미지 검색 모델 학습에 활용되는 loss 함수들은, anchor 데이터의 positive / negative pair를 활용합니다. 여기서 postivie pair는 anchor와 같은 클래스를 나타내는 데이터이며, negative pair는 anchor와 다른 클래스를 나타내는 데이터입니다. 아래는 학습에 활용하는 mini-batch에 같은 클래스와 다른 클래스를 골고루 포함할 수 있도록 하는 mini-batch sampler에 해당하는 내용입니다. 해당 MPerClassSampler는 mini-batch 내에서 각 클래스 당 m개의 sample을 포함하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzyPbocDsTa5"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "# modified from\n",
        "# https://raw.githubusercontent.com/bnulihaixia/Deep_metric/master/utils/sampler.py\n",
        "class MPerClassSampler(Sampler):\n",
        "    \"\"\"\n",
        "    At every iteration, this will return m samples per class. For example,\n",
        "    if dataloader's batchsize is 100, and m = 5, then 20 classes with 5 samples\n",
        "    each will be returned\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, m, length_before_new_iter=100000):\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            labels = labels.numpy()\n",
        "        self.m_per_class = int(m) # 4\n",
        "        self.labels_to_indices = self.get_labels_to_indices(labels)\n",
        "        self.labels = list(self.labels_to_indices.keys()) # 50\n",
        "        self.length_of_single_pass = self.m_per_class * len(self.labels) # 200\n",
        "        self.list_size = length_before_new_iter # 30000\n",
        "\n",
        "    def get_labels_to_indices(self, labels):\n",
        "            \"\"\"\n",
        "            Creates labels_to_indices, which is a dictionary mapping each label\n",
        "            to a numpy array of indices that will be used to index into self.dataset\n",
        "            \"\"\"\n",
        "            if torch.is_tensor(labels):\n",
        "                labels = labels.cpu().numpy()\n",
        "            labels_to_indices = collections.defaultdict(list)\n",
        "            for i, label in enumerate(labels):\n",
        "                labels_to_indices[label].append(i)\n",
        "            for k, v in labels_to_indices.items():\n",
        "                labels_to_indices[k] = np.array(v, dtype=int)\n",
        "            return labels_to_indices\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.list_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        idx_list = [0] * self.list_size\n",
        "        i = 0\n",
        "        num_iters = self.calculate_num_iters()\n",
        "        for _ in range(num_iters):\n",
        "            np.random.shuffle(self.labels)\n",
        "            curr_label_set = self.labels\n",
        "            for label in curr_label_set:\n",
        "                t = self.labels_to_indices[label]\n",
        "                idx_list[i : i + self.m_per_class] = np.random.choice(t, size=self.m_per_class)\n",
        "                i += self.m_per_class\n",
        "        return iter(idx_list)\n",
        "\n",
        "    def calculate_num_iters(self):\n",
        "        divisor = (\n",
        "            self.length_of_single_pass\n",
        "        )\n",
        "        return self.list_size // divisor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpOVHMNQOPO"
      },
      "source": [
        "아래는 구현한 sampler 함수를 활용하여, 학습 데이터를 불러오는 training dataset loader에 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp0idUsgwExT"
      },
      "outputs": [],
      "source": [
        "# Create the mini-batch sampler\n",
        "sampler = MPerClassSampler(train_dataset.targets, m=4, length_before_new_iter=len(train_dataset)) # 4 samples per each class\n",
        "\n",
        "# Create the training dataloader\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=64,\n",
        "        sampler=sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=0,\n",
        "        shuffle=sampler is None,\n",
        "        pin_memory=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3rOzF4pQ4an"
      },
      "source": [
        "아래는 평가에 활용할 데이터를 불러오는 testing dataset loader에 해당하는 내용입니다. 학습 평가에는 positive / negative pair를 활용하여 loss를 계산하는 과정이 없기 때문에, sampler를 활용하지 않습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDvguTfuRF_j"
      },
      "outputs": [],
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=128,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        shuffle=False,\n",
        "        pin_memory=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RuMH4-GRYk1"
      },
      "source": [
        "## Step 2: Train a image retrieval model\n",
        "이미지 검색을 위한 여러 loss 함수를 구현하고, 모델을 학습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r08RvGpERxLd"
      },
      "source": [
        "### Step 2-1: Create a image retrieval model\n",
        "본 실습에서는 resnet18을 활용하여 간단한 이미지 검색 모델을 생성합니다. Classification, detection, segmentation 등 일반적인 모델과 다르게, 이미지 검색 모델은 input 이미지에 대하여 추후 검색에 활용할 수 있는 feature descriptor를 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83O3zkBOdaGj"
      },
      "outputs": [],
      "source": [
        "class resnet18(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(resnet18, self).__init__()\n",
        "        # imagenet pretrained resnet18\n",
        "        resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "        # cnn backbone\n",
        "        self.conv = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        f = torch.squeeze(self.avgpool(x))\n",
        "        return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3itXZ7uU1Je6"
      },
      "outputs": [],
      "source": [
        "first_batch = next(iter(test_dataloader))\n",
        "img, label = first_batch[0][:4].clone().detach(), first_batch[1][:4].clone().detach()\n",
        "model = resnet18()\n",
        "out = model(img)\n",
        "print(out.shape, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7bnpTDOSjL1"
      },
      "source": [
        "### Step 2-2: Train a model with contrastive loss\n",
        "이미지 검색 모델 학습을 위한 손실 함수 중 가장 기본적인 형태인 contrastive loss를 구현하고, 이를 통해 모델 학습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHuQZxwdZOR"
      },
      "source": [
        "아래는 contrastive loss에 대한 기본적인 설명입니다. 설명을 토대로 주어진 함수를 구현해봅니다.\n",
        "\n",
        "---\n",
        "Contrastive loss의 형태는 다음과 같다:\n",
        "\n",
        "$Loss_{contrastive}(a, b) = y \\cdot D(a, b)^2 + (1 - y) \\cdot \\max(0, m - D(a, b))^2$.\n",
        "\n",
        "$y$는 두개의 feature가 같은 클래스에 속하는 지를 나타내며, 같은 클래스의 경우 1, 다른 클래스의 경우 0의 값을 가진다.\n",
        "$D$는 두개의 feature에 대한 Euclidean distance를 나타낸다. 임의의 feature $a$, $b$에 대해 Euclidean distance는 다음과 같이 계산할 수 있다:\n",
        "\n",
        "$D(a, b) = \\sqrt{\\sum^n_{i=1}{(b_i - a_i)^2}}$,\n",
        "\n",
        "Margin $m$은 두 샘플 간의 거리(또는 유사도)를 비교할 때 사용되는 중요한 하이퍼파라미터이다. Margin은 두 샘플이 동일한 클래스(positive pair)인 경우에는 거리가 얼마나 가까워야 하는지, 서로 다른 클래스(negative pair)인 경우에는 얼마나 멀어야 하는지를 결정한다.\n",
        "\n",
        "두 샘플을 임베딩 공간에서 가깝게 배치하고 싶은 경우, Positive pair의 거리는 가능한 한 0에 가깝게 유지되어야 한다. 따라서 Positive pair의 손실은 일반적으로 거리를 사용할 때는 0에 가까워지도록 만들어진다.\n",
        "\n",
        "한편 두 샘플을 임베딩 공간에서 멀리 배치하고 싶은 경우, Negative pair의 거리는 Margin 값보다 더 멀어지도록 만들어야 한다. Margin 값보다 작은 거리를 가지면, 이러한 샘플 쌍은 Contrastive loss에 의해 큰 손실을 받게 된다. 따라서 Negative pair의 손실은 일반적으로 Margin 값보다 작은 거리에 대해서만 계산된다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuiLAu9JwVqJ"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "\tdef __init__(self, margin=0.3):\n",
        "\t\tsuper(ContrastiveLoss, self).__init__()\n",
        "\t\tself.margin = margin\n",
        "\n",
        "\tdef sample(self, labels):\n",
        "\t\tB = labels.size(0)\n",
        "\t\tmatches = (labels.expand(B, B).eq(labels.expand(B, B).t())).byte()\n",
        "\t\tmatches.fill_diagonal_(0)\n",
        "\t\treturn matches\n",
        "\n",
        "\tdef euclidean_dist(self, x, y):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute distance matrix between x and y\n",
        "\t\t:param x: [m, d]\n",
        "\t\t:param y: [n, d]\n",
        "\t\t:return:[m, n]\n",
        "\t\t\"\"\"\n",
        "\t\tm, n = x.size(0), y.size(0)\n",
        "\t\txx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
        "\t\tyy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
        "\t\tdist = xx + yy - 2 * torch.matmul(x, y.t())\n",
        "\t\tdist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "\t\treturn dist\n",
        "\n",
        "\tdef forward(self, features, labels):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\t\tfeatures (torch.Tensor): extracted features with shape (batch_size, num_channels)\n",
        "\t\t\t\tlabels (torch.Tensor): ground truth labels with shape (batch_size)\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\tloss (torch.Tensor): computed triplet loss shape (1, ) (scalar)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tQ. Write your code to compute contrastive loss.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\treturn loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ch6QWxC3lO7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwf99MeZTpIl"
      },
      "source": [
        "아래는 구현된 contrastive loss를 활용하여 모델 학습을 진행하는 trainer 클래스입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEn_p5IXwmy3"
      },
      "outputs": [],
      "source": [
        "class ContrastiveTrainer(object):\n",
        "    def __init__(self, model, device, margin=0.3):\n",
        "        super(ContrastiveTrainer, self).__init__()\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.criterion = ContrastiveLoss(margin=margin)\n",
        "\n",
        "    def train(self, train_data_loader, optimizer, epoch):\n",
        "        self.model.train()\n",
        "        for i in range(epoch):\n",
        "            progress_bar = tqdm(train_data_loader)\n",
        "            for j, data in enumerate(progress_bar):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                features = self.model(images)\n",
        "                loss = self.criterion(features, labels)  # compute contrastive loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                progress_bar.set_postfix({\"Epoch\": i, \"Iteration\": j, \"Loss\": f\"{loss.item():.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZFBb4PvUFH4"
      },
      "source": [
        "구현한 contrastive loss를 토대로, 이미지 검색 모델을 생성하고 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzhBWUr9wyMS"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "contrastive_model = resnet18()\n",
        "contrastive_model.to(device)\n",
        "epoch = 10\n",
        "lr = 1e-5\n",
        "margin = 0.7\n",
        "contrastive_optimizer = torch.optim.Adam(contrastive_model.parameters(), lr=lr, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycCM4_vhwyPV"
      },
      "outputs": [],
      "source": [
        "contrastive_trainer = ContrastiveTrainer(model=contrastive_model, device=device, margin=margin)\n",
        "contrastive_trainer.train(train_dataloader, contrastive_optimizer, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU-4RCFgUb90"
      },
      "source": [
        "### Step 2-3: Train a model with triplet loss\n",
        "이미지 검색 모델 학습을 위한 손실 함수 중 가장 활발하게 활용되는 triplet loss를 구현하고, 이를 통해 모델 학습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etC2yS3zWlIZ"
      },
      "source": [
        "아래는 triplet loss에 대한 기본적인 설명입니다. 설명을 토대로 주어진 함수를 구현해봅니다.\n",
        "\n",
        "---\n",
        "Triplet loss의 형태는 다음과 같다:\n",
        "\n",
        "$Loss_{triplet}(a, p, n) = \\max(0, D(a, p) - D(a, n) + m)^2$.\n",
        "\n",
        "$(a, p, n)$은 anchor point $a$를 기준으로 세 개의 데이터 쌍인 'triplet'을 의미한다. Positive point $p$는 anchor point $a$와 동일한 클래스/정보를 나타내는 데이터 포인트로 구성되며, negative point $n$은 anchor point $a$와 다른 정보를 나타내는 데이터 포인트로 구성된다. Margin $m$은 positive 데이터와 negative 데이터 사이의 거리 차이를 조절하는 하이퍼파라미터이다.\n",
        "\n",
        "$D$는 두 개의 feature에 대한 Euclidean distance를 나타낸다. 임의의 feature $a$, $b$에 대해 Euclidean distance는 다음과 같이 계산할 수 있다:\n",
        "\n",
        "$D(a, b) = \\sqrt{\\sum^n_{i=1}{(b_i - a_i)^2}}$.\n",
        "\n",
        "Triplet loss의 주요 목적은 비슷한 데이터끼리는 feature space에서 가깝게, 다른 데이터끼리는 멀리 떨어지게끔 학습하는 것이다. Triplet loss의 핵심 아이디어는 anchor와 positive 데이터 간의 거리를 최소화하고, anchor와 negative 데이터 간의 거리를 최대화하며, 거리를 최적화 하는 과정에서 anchor-postive / anchor-negative 거리를 상대적으로 비교하며 활용하는 것이다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J02__AkrUjbL"
      },
      "outputs": [],
      "source": [
        "class TripletMarginLoss(nn.Module):\n",
        "\tdef __init__(self, margin=0.3):\n",
        "\t\tsuper(TripletMarginLoss, self).__init__()\n",
        "\t\tself.margin = margin\n",
        "\n",
        "\tdef euclidean_dist(self, x, y):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute distance matrix between x and y\n",
        "\t\t:param x: [m, d]\n",
        "\t\t:param y: [n, d]\n",
        "\t\t:return:[m, n]\n",
        "\t\t\"\"\"\n",
        "\t\tm, n = x.size(0), y.size(0)\n",
        "\t\txx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
        "\t\tyy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
        "\t\tdist_mat = xx + yy - 2 * x @ y.t()\n",
        "\t\tdist_mat = dist_mat.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "\t\treturn dist_mat\n",
        "\n",
        "\tdef compute_triplet_indices(self, labels):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\t\tlabels (torch.Tensor): ground truth labels with shape (num_data)\n",
        "\t\tReturns:\n",
        "\t\t\t\ta_indices: anchor indices of triplets\n",
        "\t\t\t\tp_indices: postive indices of triplets\n",
        "\t\t\t\tn_indices: negative indices of triplets\n",
        "\t\t\"\"\"\n",
        "\t\tB = labels.shape[0]\n",
        "\t\tmatches = (labels.expand(B, B).eq(labels.expand(B, B).t())).float()\n",
        "\t\tdiffs = 1 - matches\n",
        "\t\tmatches.fill_diagonal_(0)\n",
        "\t\ttriplets = torch.bmm(matches.unsqueeze(2), diffs.unsqueeze(1))\n",
        "\t\ta_indices, p_indices, n_indices = torch.where(triplets)\n",
        "\t\treturn (a_indices, p_indices, n_indices)\n",
        "\n",
        "\tdef forward(self, features, labels):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\t\tfeatures (torch.Tensor): extracted features with shape (batch_size, num_channels)\n",
        "\t\t\t\tlabels (torch.Tensor): ground truth labels with shape (batch_size)\n",
        "\t\tReturns:\n",
        "\t\t\t\tloss (torch.Tensor): computed triplet loss shape (1, ) (scalar)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tQ. Write your code to compute triplet margin loss.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\treturn loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnbwl5CQW-Do"
      },
      "source": [
        "아래는 구현된 triplet loss를 활용하여 모델 학습을 진행하는 trainer 클래스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDV_21SoUjZF"
      },
      "outputs": [],
      "source": [
        "class TripletTrainer(object):\n",
        "    def __init__(self, model, device, margin=0.3):\n",
        "        super(TripletTrainer, self).__init__()\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.criterion = TripletMarginLoss(margin=margin)\n",
        "\n",
        "    def train(self, train_data_loader, optimizer, epoch):\n",
        "        self.model.train()\n",
        "\n",
        "        for i in range(epoch):\n",
        "            progress_bar = tqdm(train_data_loader)\n",
        "            for j, data in enumerate(progress_bar):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                features = self.model(images)\n",
        "                loss = self.criterion(features, labels)  # compute triplet loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                progress_bar.set_postfix({\"Epoch\": i, \"Iteration\": j, \"Loss\": f\"{loss.item():.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8wkaGmgXJEv"
      },
      "source": [
        "구현한 triplet loss를 토대로, 이미지 검색 모델을 생성하고 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOMXpIUjUjWq"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "triplet_model = resnet18()\n",
        "triplet_model.to(device)\n",
        "epoch = 10\n",
        "lr = 1e-5\n",
        "margin = 0.1\n",
        "triplet_optimizer = torch.optim.Adam(triplet_model.parameters(), lr=lr, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgDzRAHcUQmi"
      },
      "outputs": [],
      "source": [
        "triplet_trainer = TripletTrainer(model=triplet_model, device=device, margin=margin)\n",
        "triplet_trainer.train(train_dataloader, triplet_optimizer, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVS_JyYrXZ6X"
      },
      "source": [
        "## Step 3: Test a image retrieval model\n",
        "이미지 검색 모델 평가를 위한 여러 평가 지표를 구현하고, contrastive/triplet loss를 통해 학습된 모델을 평가합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK5OncipOeMY"
      },
      "source": [
        "### Step 3-1: Compute mean Average Precision (mAP)\n",
        "\n",
        "아래를 이미지 검색 성능 평가에 대표적으로 활용되는 Average Precision (AP)와 mean Average Precision (mAP)에 대한 설명입니다. 해당 평가 지표를 구현해봅시다.\n",
        "\n",
        "---\n",
        "\n",
        "$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n",
        "\n",
        "*   $n$: ranked list에서의 top n.\n",
        "* $R_n$: 상위 n번째 까지의 Recall.\n",
        "* $P_n$: 상위 n번째 까지의 Precision.\n",
        "\n",
        "\n",
        "Average Precision (AP)은 Precision-Recall 곡선의 아래 면적값으로 계산된다. 위 수식을 활용하여 이산적 검색결과에 대한 AP계산이 가능하다. 구체적으로, ranked list 크기를 늘려 가면서 Recall 값을 0부터 1까지 변화시킨다. 각 Recall에서의 Precision을 계산하고, 이것을 Recall값의 차이와 곱하여 더해준다.\n",
        "\n",
        "mean Average Precision (mAP)은 모든 쿼리에 대한 AP를 평균내어 계산한다.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_X8eqzmq-Ez"
      },
      "source": [
        "\n",
        "- Precision@K = $\\frac{\\text{Number of relevant items in K}}{\\text{Total number of items in K}}$\n",
        "\n",
        "- Recall@K = $\\frac{\\text{Number of relevant items in K}}{\\text{Total number of relevant items}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2q-vpFDIdm0"
      },
      "outputs": [],
      "source": [
        "def average_precision(ranked_list, labels, q_ind):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      ranked_list(numpy array): index-ranked list of 1D array w.r.t a specific query (num_images)\n",
        "      labels (numpy array): class labels (num_images)\n",
        "      q_ind: index of a specific query (scalar)\n",
        "\n",
        "    Returns:\n",
        "      Average Precision (AP) for q_ind  (scalar)\n",
        "    \"\"\"\n",
        "    def precision_at_k(hits, k):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hits (numpy array): correct match or not list (num_images)\n",
        "            k: top-k (scalar)\n",
        "\n",
        "        Returns:\n",
        "            precision at k over the given ranked_list (scalar)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Q. Write your code to compute precision at k.\n",
        "        \"\"\"\n",
        "\n",
        "        return precision\n",
        "\n",
        "    def recall_at_k(hits,k):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hits (numpy array): correct match or not list (num_images)\n",
        "            k: top-k (scalar)\n",
        "\n",
        "        Returns:\n",
        "            recall at k over the given ranked_list (scalar)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Q. Write your code to compute recall at k.\n",
        "        \"\"\"\n",
        "        return recall\n",
        "\n",
        "    hits = (labels[ranked_list] == labels[q_ind])\n",
        "\n",
        "    ap = 0\n",
        "    prev_recall = 0\n",
        "\n",
        "    \"\"\"\n",
        "    Q. Write your code to compute average precision.\n",
        "    \"\"\"\n",
        "\n",
        "    return ap\n",
        "\n",
        "def mean_average_precision(features, labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      features (numpy array): L2-normalized features of all images (num_images, num_channels)\n",
        "      labels (numpy array): class labels (num_images)\n",
        "\n",
        "    Returns:\n",
        "      mean Average Precision (mAP) for all queries  (scalar)\n",
        "    \"\"\"\n",
        "    sim = np.dot(features, features.T)\n",
        "    ranks = np.argsort(-sim, axis=1)\n",
        "\n",
        "    \"\"\"\n",
        "    Q. Write your code to compute mAP.\n",
        "    \"\"\"\n",
        "\n",
        "    return mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7XmJ72atz6"
      },
      "source": [
        "### Step 3-2 Compare trained models\n",
        "Imagenet-pretrained, contrastive loss trained, triplet loss trained model들의 평가하고 비교합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8zvfzOVZH09"
      },
      "source": [
        "아래는 구현된 mAP를 기반으로 모델을 평가하는 evaluator 클래스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgEFe0kNYcoz"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, model, device):\n",
        "        super(Evaluator, self).__init__()\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        self.model.eval()\n",
        "        test_features = []\n",
        "        test_labels = []\n",
        "        for i, data in enumerate(test_dataloader):\n",
        "            images, labels = data\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            features = self.model(images)\n",
        "            test_features.append(features.detach().cpu())\n",
        "            test_labels.append(labels.detach().cpu())\n",
        "\n",
        "        test_features = torch.cat(test_features).numpy()\n",
        "        test_features = test_features / np.linalg.norm(test_features, axis = 1)[:, None] # L2 normalization\n",
        "        test_labels = torch.cat(test_labels).numpy()\n",
        "        mAP = mean_average_precision(test_features, test_labels)\n",
        "        print('\\nMean AP: {:4.1%}'.format(mAP))\n",
        "        return test_features, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CLtjxSca9Qr"
      },
      "source": [
        "ImageNet-pretrained resnet-18 모델을 불러오고 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esNsdHzLIdra"
      },
      "outputs": [],
      "source": [
        "pretrained_model= resnet18()\n",
        "pretrained_model.to(device)\n",
        "\n",
        "pretrained_evaluator = Evaluator(pretrained_model, device)\n",
        "pretrained_test_features, test_labels = pretrained_evaluator.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y6YaewvbYbK"
      },
      "source": [
        "Contrastive loss로 학습한 모델을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odUg3T2AIlMa"
      },
      "outputs": [],
      "source": [
        "contrastive_evaluator = Evaluator(contrastive_model, device)\n",
        "contrastive_test_features, contrastive_test_labels = contrastive_evaluator.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAfGXKbqbyGa"
      },
      "source": [
        "Triplet loss로 학습한 모델을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sexoLiM0b15q"
      },
      "outputs": [],
      "source": [
        "triplet_evaluator = Evaluator(triplet_model, device)\n",
        "triplet_test_features, triplet_test_labels = triplet_evaluator.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWwJmkqaq-E0"
      },
      "source": [
        "### Step 3-3 Visualize retrieved features\n",
        "학습된 모델의 feature를 시각화하여 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swg4TiNTq-E1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def show_feature(ax, test_features, test_labels, title):\n",
        "    # PCA를 사용하여 test_features를 2차원으로 축소\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(test_features)\n",
        "\n",
        "    # 라벨별로 시각화\n",
        "    label_range = np.array([71, 56, 80])#np.arange(71, 73)\n",
        "\n",
        "    for label in label_range:\n",
        "        idxs = test_labels == label\n",
        "        ax.scatter(pca_result[idxs, 0], pca_result[idxs, 1], label=f'Label {label}', alpha=0.7)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('PCA Component 1')\n",
        "    ax.set_ylabel('PCA Component 2')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "# 전체 figure를 설정하고, 서브플롯을 1행 3열로 배치\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "show_feature(axs[0], pretrained_test_features, test_labels, 'Pretrained Model')\n",
        "show_feature(axs[1], contrastive_test_features, contrastive_test_labels, 'Contrastive Loss')\n",
        "show_feature(axs[2], triplet_test_features, triplet_test_labels, 'Triplet Loss')\n",
        "\n",
        "# 서브플롯 간의 간격을 조정\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF2cNAvsb8ku"
      },
      "source": [
        "### Step 3-4 Visualize retrieved results\n",
        "학습된 모델을 활용하여 검색된 결과를 시각화하여 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DTyRH2Wq-E1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(test_features, test_images, test_labels, q_ind, title):\n",
        "    # 유사도 행렬 계산 및 가장 가까운 이웃 찾기\n",
        "    sim = np.dot(test_features, test_features.T)\n",
        "    knn_matrix = np.argsort(-sim, axis=1)[:,:5] # 상위 5개의 이웃\n",
        "\n",
        "    neighbors = knn_matrix[q_ind] # [K]\n",
        "    matched_images = []\n",
        "    matched_labels = []\n",
        "    for neighbor in neighbors:\n",
        "        matched_images.append(np.asarray(test_images[neighbor]))\n",
        "        matched_labels.append(np.asarray(test_labels[neighbor]))\n",
        "\n",
        "    # 쿼리 이미지와 가장 가까운 이미지를 서브플롯으로 그리기\n",
        "    fig, axs = plt.subplots(1, 6, figsize=(9, 2))\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    # 쿼리 이미지 표시\n",
        "    axs[0].imshow(test_images[q_ind])\n",
        "    axs[0].set_title(f\"Query\\nLabel: {test_labels[q_ind]}\")\n",
        "    axs[0].axis('off')  # 축 제거\n",
        "\n",
        "    # 가까운 5개의 이미지 표시\n",
        "    for i in range(len(matched_images)):\n",
        "        axs[i + 1].imshow(matched_images[i])\n",
        "        axs[i + 1].set_title(f\"Top {i+1}\\nLabel: {matched_labels[i]}\")\n",
        "        axs[i + 1].axis('off')  # 축 제거\n",
        "\n",
        "    # 서브플롯 간의 간격을 조정\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # 제목을 위한 공간 확보\n",
        "    plt.show()\n",
        "\n",
        "test_images = test_dataset.data\n",
        "test_labels = test_dataset.targets\n",
        "q_ind = 4\n",
        "\n",
        "show_images(pretrained_test_features, test_images, test_labels, q_ind, \"Pretrained Model\")\n",
        "show_images(contrastive_test_features, test_images, test_labels, q_ind, \"Contrastive Loss\")\n",
        "show_images(triplet_test_features, test_images, test_labels, q_ind, \"Triplet Loss\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
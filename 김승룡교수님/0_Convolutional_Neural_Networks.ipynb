{"cells":[{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Exercise 1: Convolutional neural networks (CNN)"]},{"cell_type":"markdown","metadata":{"id":"UzZFFAD7ujN4"},"source":["##Import dependencies (run the following cells)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pRePt-K1_yw9","cellView":"form","executionInfo":{"status":"ok","timestamp":1753244303455,"user_tz":-540,"elapsed":12453,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}}},"outputs":[],"source":["# @title import dependencies\n","\n","from typing import Mapping, Union, Optional\n","\n","import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import plotly.graph_objects as go\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","from tqdm.notebook import tqdm\n","\n","from __future__ import print_function, division"]},{"cell_type":"markdown","metadata":{"id":"yjyYfdR7QnrN"},"source":["## CNNs in practice\n","\n","Building a CNN today is very easy thanks to modern deep learning programming frameworks. As you have seen in the last notebook, the `torch.nn` package reduces the introduction of a convolution transformation to a single call to a function  (e.g. ```nn.Conv2d```  for 2D data).\n","\n","Nevertheless, CNNs involve many different operations, including non-trivial changes in shape of the input tensor through the layers.\n","\n","Keep in mind the two basic transformations introduced by CNNs:\n","- Convolution\n","\n","![conv](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n","- Pooling (Max pooling in the example below)\n","\n","![pooling](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n","\n","*Images from Wikipedia*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sDbVInBiKs0a"},"source":["---\n","\n","**Under the hood: how does a convolution work?**\n","\n","Let's compute by hand a single output value of a convolution operation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BX6uFr2TK9dt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753235885509,"user_tz":-540,"elapsed":35,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}},"outputId":"72e2621f-18a1-47ad-e88d-64818416ca72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[[ 0.,  1.,  2.,  3.],\n","           [ 4.,  5.,  6.,  7.],\n","           [ 8.,  9., 10., 11.],\n","           [12., 13., 14., 15.]],\n"," \n","          [[16., 17., 18., 19.],\n","           [20., 21., 22., 23.],\n","           [24., 25., 26., 27.],\n","           [28., 29., 30., 31.]]]]),\n"," torch.Size([1, 2, 4, 4]))"]},"metadata":{},"execution_count":3}],"source":["# Define an arbitrary input tensor, i.e.: [batch, channels, w, h]\n","batch_size, num_channels, width, height = 1, 2, 4, 4\n","a = torch.arange(batch_size * num_channels * width * height).reshape(batch_size, num_channels, width, height).float()\n","a, a.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p89SvOCjLNix"},"outputs":[],"source":["# Define a convolution\n","c = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=2, bias=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b61JiSP0Lcq-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753235888236,"user_tz":-540,"elapsed":10,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}},"outputId":"31256b44-5797-4649-e203-298a15f61232"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Parameter containing:\n"," tensor([[[[ 0.3432,  0.2381],\n","           [-0.0373, -0.1192]],\n"," \n","          [[ 0.2985,  0.2948],\n","           [-0.0664,  0.3083]]],\n"," \n"," \n","         [[[-0.0435, -0.0830],\n","           [ 0.1895, -0.1782]],\n"," \n","          [[-0.3184,  0.3128],\n","           [ 0.2530, -0.2484]]],\n"," \n"," \n","         [[[-0.1078,  0.1343],\n","           [-0.1081,  0.3484]],\n"," \n","          [[ 0.2352, -0.3167],\n","           [ 0.2134,  0.1555]]]], requires_grad=True),\n"," torch.Size([3, 2, 2, 2]))"]},"metadata":{},"execution_count":5}],"source":["# Let's look under the hood... somewhere there must be learnable weights...\n","# They have shape [3, 2, 2, 2], i.e.: [out, in, kernel, kernel]\n","c.weight, c.weight.shape"]},{"cell_type":"markdown","source":["The weights you see above are random, so don't look for any particular meaning. What we care about is the **shape** of `c`:\n","- We asked for $2\\times 2$ kernels.\n","- `nn.Conv2d` creates $2$ such kernels: one per input channel.\n","- Convolution will be applied to each channel separately, resulting in $2$ feature maps.\n","- These features maps are [summed together](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n","- All the above is done $3$ times: once per output channel.\n","\n","As a result, the output of `nn.Conv2d` is a tensor with shape `[3, 2, 2, 2]` that will be convolved with the input."],"metadata":{"id":"g2i8ziUi1pTM"}},{"cell_type":"markdown","source":["> **EXERCISE:** What's the shape of the _output_ resulting from applying these convolutions?"],"metadata":{"id":"cEe-fZXB7Tms"}},{"cell_type":"markdown","source":["Let's do a bit of unrolling to get a deeper insight into convolution."],"metadata":{"id":"w7sC8gtX7idF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"shPYD5lQLeyV"},"outputs":[],"source":["# We define custom kernel weights for the first out channel\n","# In this way we can easily reproduce the computation\n","my_custom_2dkernel_in_channel1 = torch.tensor([[ .1, .2], [-.4, -.5, ]])\n","my_custom_2dkernel_in_channel2 = torch.tensor([[.4, .3], [-.42, -.45, ]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbwPRBb4Lr5h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753235893144,"user_tz":-540,"elapsed":14,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}},"outputId":"7ec01bed-ff25-4423-ad42-e527033b7018"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.1000,  0.2000],\n","         [-0.4000, -0.5000]]),\n"," torch.Size([2, 2]))"]},"metadata":{},"execution_count":7}],"source":["my_custom_2dkernel_in_channel1, my_custom_2dkernel_in_channel1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-X8884WUMTgr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753235894324,"user_tz":-540,"elapsed":13,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}},"outputId":"c5c3439f-a5fb-4ef2-eb03-dbfa083ddd98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.4000,  0.3000],\n","         [-0.4200, -0.4500]]),\n"," torch.Size([2, 2]))"]},"metadata":{},"execution_count":8}],"source":["my_custom_2dkernel_in_channel2, my_custom_2dkernel_in_channel2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dl9dAc8LMXDJ"},"outputs":[],"source":["# Modify the weights of the convolution\n","\n","# out-channel 0, in-channel 0\n","c.weight.data[0, 0, ...] = my_custom_2dkernel_in_channel1\n","\n","# out-channel 0, in-channel 1\n","c.weight.data[0, 1, ...] = my_custom_2dkernel_in_channel2\n","\n","# ...accessing directly the .data attribute of a tensor by-passed some safety checks!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Uqafi2mMd7x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753235897846,"user_tz":-540,"elapsed":8,"user":{"displayName":"Matthew Kwak","userId":"09328475166977512482"}},"outputId":"b6a2bf30-bd6d-4389-e8b9-571d9a779cfd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Parameter containing:\n"," tensor([[[[ 0.1000,  0.2000],\n","           [-0.4000, -0.5000]],\n"," \n","          [[ 0.4000,  0.3000],\n","           [-0.4200, -0.4500]]],\n"," \n"," \n","         [[[-0.0435, -0.0830],\n","           [ 0.1895, -0.1782]],\n"," \n","          [[-0.3184,  0.3128],\n","           [ 0.2530, -0.2484]]],\n"," \n"," \n","         [[[-0.1078,  0.1343],\n","           [-0.1081,  0.3484]],\n"," \n","          [[ 0.2352, -0.3167],\n","           [ 0.2134,  0.1555]]]], requires_grad=True),\n"," torch.Size([3, 2, 2, 2]))"]},"metadata":{},"execution_count":10}],"source":["# Let's check: the tirst out-channel is correctly set\n","c.weight, c.weight.shape"]},{"cell_type":"markdown","source":["As we mentioned, we have a $2 \\times 2$ kernel for each input channel, in this case two, and we have as many kernel pairs as we have output channels. For simplicity, we will only consider one output channel in our unrolling."],"metadata":{"id":"VOVMd17p410R"}},{"cell_type":"markdown","metadata":{"id":"4IcR2UxQWnn-"},"source":["> **EXERCISE**\n",">\n","> Stop one second. Try to apply the convolution yourself!\n",">\n","> Here is an input image with shape `torch.Size([1, 2, 4, 4])`, i.e. `[batch, channels, w, h]`:\n",">\n","> ```python\n","> a = tensor([[[[ 0.,  1.,  2.,  3.],\n",">               [ 4.,  5.,  6.,  7.],\n",">               [ 8.,  9., 10., 11.],\n",">               [12., 13., 14., 15.]],\n",">     \n",">              [[16., 17., 18., 19.],\n",">               [20., 21., 22., 23.],\n",">               [24., 25., 26., 27.],\n",">               [28., 29., 30., 31.]]]])\n","> ```\n",">\n","> This is the kernel tensor from before, having shape `torch.Size([3, 2, 2, 2])`, i.e. `[out_channels, in_channels, kernel_size, kernel_size]`:\n",">\n","> ```python\n","> c.weight = tensor([[[[ 0.1000,  0.2000],\n",">                      [-0.4000, -0.5000]],\n",">            \n",">                     [[ 0.4000,  0.3000],\n",">                      [-0.4200, -0.4500]]],\n",">            \n",">            \n",">                    [[[-0.3517,  0.2366],\n",">                      [ 0.2679,  0.1289]],\n",">            \n",">                     [[-0.2465, -0.3489],\n",">                      [-0.2871,  0.2636]]],\n",">            \n",">            \n",">                    [[[ 0.1697,  0.2975],\n",">                      [ 0.1852,  0.0895]],\n",">            \n",">                     [[-0.0035, -0.2689],\n",">                      [-0.3029, -0.3307]]]], requires_grad=True)\n","> ```\n",">\n","> **Question: What value will be in `output[0, 0, 0, 0]`, i.e. the first element of the output tensor after the convolution?**\n","\n",">**Answer:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM64GGyGRmgv"},"outputs":[],"source":["# Here's the complete result when we apply this convolution\n","o = c(a)\n","o, o.shape"]},{"cell_type":"markdown","source":["Is shape `[1, 3, 3, 3]` your answer to the previous exercise? If yes, you're on a good path!"],"metadata":{"id":"EnmVVrBR8BwR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Cek6YsiUVMr"},"outputs":[],"source":["# Let's compute the first value of the first out channel manually, i.e. this one:\n","o[0, 0, 0, 0]  # [batch, channels, w, h]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66lyzn5mW6US"},"outputs":[],"source":["# Take the first window of the same size of the kernel in the first in_channel of the input\n","f1 = a[0, 0, :2, :2]\n","f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXkLuX7XXapU"},"outputs":[],"source":["# And the second input channel\n","f2 = a[0, 1, :2, :2]\n","f2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lvpbc_7NXhhs"},"outputs":[],"source":["# Perform a point-wise multiplication along the (input) feature dimension, between the input and the kernel.\n","# In this case this can be done manually in this way:\n","\n","f = f1 * my_custom_2dkernel_in_channel1 + f2 * my_custom_2dkernel_in_channel2\n","f"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXtIrXlQXkSA"},"outputs":[],"source":["# Sum up the result\n","s = f.sum()\n","s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCeR9huCXx8F"},"outputs":[],"source":["# i.e. the first element in the first out_channel of our output tensor\n","o"]},{"cell_type":"markdown","metadata":{"id":"ZHXMHMgCdDE3"},"source":["> **EXERCISE**\n",">\n","> How many parameters does this layer have? Note that this time there is a _bias_ as well! Check the docs to see how bias is defined for 2d convolutions.\n",">\n","> ```python\n","> nn.Conv2d(in_channels=5, out_channels=10, kernel_size=2, bias=True)\n","> ```"]},{"cell_type":"markdown","source":["### Break the symmetry!\n","\n","A small interlude before we implement our first CNN. Let's address two questions.\n","\n","**_Why would I want more output channels than input channels?_**\n","\n","Having multiple output channels (i.e. _feature maps_) allows the network to simultaneously learn a variety of features from the same piece of input data. For instance, one channel might become specialized in detecting horizontal edges, while another might focus on vertical edges, and a third might detect areas of high contrast.\n","\n","**_If I initialize the kernel weights equally for all the output channels, will I get the same feature maps?_**\n","\n","If you initialize the convolutional filter weights equally for all output channels, indeed you might find that the filters learn identical features and produce the same output. This is why **random initialization** is critical and universally adopted â€” it ensures that each filter starts from a slightly different state, allowing them to explore different paths and learn to capture various features of the input data."],"metadata":{"id":"oveSkHBWCc7a"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["UzZFFAD7ujN4"],"toc_visible":true,"provenance":[{"file_id":"https://github.com/erodola/DLAI-s2-2024/blob/main/labs/06_Convolutional_Neural_Networks.ipynb","timestamp":1753235354480}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DOv_qK3oXomk"},"outputs":[],"source":["from typing import List\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorflow_hub as hub\n","from tensorflow import keras\n","from keras import backend\n","from keras import layers\n","\n","tfds.disable_progress_bar()\n","keras.utils.set_random_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nd6dC8FXqhZ"},"outputs":[],"source":["# Model\n","IMAGE_SIZE = 224\n","PATCH_SIZE = 16\n","NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","LAYER_NORM_EPS = 1e-6\n","PROJECTION_DIM = 192\n","NUM_HEADS = 3\n","NUM_LAYERS = 12\n","MLP_UNITS = [\n","    PROJECTION_DIM * 4,\n","    PROJECTION_DIM,\n","]\n","DROPOUT_RATE = 0.0\n","\n","# Training\n","NUM_EPOCHS = 10\n","BASE_LR = 0.0005\n","WEIGHT_DECAY = 0.0001\n","\n","# Data\n","BATCH_SIZE = 64\n","AUTO = tf.data.AUTOTUNE\n","NUM_CLASSES = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17668,"status":"ok","timestamp":1752213034889,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"},"user_tz":-540},"id":"IvZLZf8wXqZZ","outputId":"73de3dd4-100f-485e-dd38-1d93a8e60a09"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Variant folder /root/tensorflow_datasets/tf_flowers/3.0.1 has no dataset_info.json\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n","Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n","Number of training examples: 3303\n","Number of validation examples: 367\n"]}],"source":["def preprocess_dataset(is_training=True):\n","    def fn(image, label):\n","        if is_training:\n","            # Resize to a bigger spatial resolution and take the random crops.\n","            image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","            image = tf.image.random_crop(image, (IMAGE_SIZE, IMAGE_SIZE, 3))\n","            image = tf.image.random_flip_left_right(image)\n","            image = image / 255.0\n","        else:\n","            image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","            image = image / 255.0\n","        label = tf.one_hot(label, depth=NUM_CLASSES)\n","        return image, label\n","\n","    return fn\n","\n","\n","def prepare_dataset(dataset, is_training=True):\n","    if is_training:\n","        dataset = dataset.shuffle(BATCH_SIZE * 10)\n","    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=AUTO)\n","    return dataset.batch(BATCH_SIZE).prefetch(AUTO)\n","\n","\n","train_dataset, val_dataset = tfds.load(\n","    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\",], as_supervised=True\n",")\n","num_train = train_dataset.cardinality()\n","num_val = val_dataset.cardinality()\n","\n","print(f\"Number of training examples: {num_train}\")\n","print(f\"Number of validation examples: {num_val}\")\n","\n","\n","train_dataset = prepare_dataset(train_dataset, is_training=True)\n","val_dataset = prepare_dataset(val_dataset, is_training=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y52yB4jXvpT"},"outputs":[],"source":["def mlp(x, dropout_rate: float, hidden_units: List):\n","    \"\"\"FFN for a Transformer block.\"\"\"\n","    # Iterate over the hidden units and add Dropout.\n","    for (idx, units) in enumerate(hidden_units):\n","        x = layers.Dense(\n","            units,\n","            activation=tf.nn.gelu if idx == 0 else None,\n","        )(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","def transformer(name: str) -> keras.Model:\n","    \"\"\"Transformer block with pre-norm.\"\"\"\n","    num_patches = NUM_PATCHES + 2 if \"deit\" in MODEL_TYPE else NUM_PATCHES + 1\n","    encoded_patches = layers.Input((num_patches, PROJECTION_DIM))\n","\n","    # Implement here\n","    # Layer normalization 1 - layers.LayerNormalization(epsilon)\n","\n","    # Multi Head Self Attention layer - layers.MultiHeadAttention(num_heads, key_dim, dropout)\n","\n","    # Skip connection\n","    # layers.add()\n","\n","    # Layer normalization 2\n","\n","    # MLP layer 1.\n","\n","    # Skip connection 2.\n","\n","    return keras.Model(encoded_patches, outputs, name=name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoERTVrcXzGS"},"outputs":[],"source":["class ViTClassifier(keras.Model):\n","    \"\"\"Vision Transformer base class.\"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        # Patchify + linear projection + reshaping.\n","        self.projection = keras.Sequential(\n","            [\n","                layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n","                # Implement here\n","                # Hint: Use convolution\n","\n","                #layers.Conv2D()\n","\n","                #layers.Reshape()\n","            ],\n","            name=\"projection\",\n","        )\n","\n","        # Positional embedding.\n","        init_shape = (\n","            1,\n","            NUM_PATCHES + 1,\n","            PROJECTION_DIM,\n","        )\n","        self.positional_embedding = tf.Variable(\n","            tf.zeros(init_shape), name=\"pos_embedding\"\n","        )\n","\n","        # Transformer blocks.\n","        self.transformer_blocks = [\n","            transformer(name=f\"transformer_block_{i}\")\n","            for i in range(NUM_LAYERS)\n","        ]\n","\n","        # CLS token.\n","        initial_value = tf.zeros((1, 1, PROJECTION_DIM))\n","        self.cls_token = tf.Variable(\n","            initial_value=initial_value, trainable=True, name=\"cls\"\n","        )\n","\n","        # Other layers.\n","        self.dropout = layers.Dropout(DROPOUT_RATE)\n","        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n","        self.head = layers.Dense(\n","            NUM_CLASSES,\n","            name=\"classification_head\",\n","        )\n","\n","    def call(self, inputs, training=True):\n","        n = tf.shape(inputs)[0]\n","\n","        # Create patches and project the patches.\n","        projected_patches = self.projection(inputs)\n","\n","        # Append class token if needed.\n","        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n","        cls_token = tf.cast(cls_token, projected_patches.dtype)\n","        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n","\n","        # Add positional embeddings to the projected patches.\n","        encoded_patches = (\n","            self.positional_embedding + projected_patches\n","        )\n","        encoded_patches = self.dropout(encoded_patches)\n","\n","        # Iterate over the number of layers and stack up blocks of Transformer.\n","        for transformer_module in self.transformer_blocks:\n","            # Add a Transformer block.\n","            encoded_patches = transformer_module(encoded_patches)\n","\n","        # Final layer normalization.\n","        representation = self.layer_norm(encoded_patches)\n","\n","        # Pool representation.\n","        encoded_patches = representation[:, 0]\n","\n","        # Classification head.\n","        output = self.head(encoded_patches)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":392428,"status":"ok","timestamp":1752213432852,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"},"user_tz":-540},"id":"53xPyD1AX0WC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"95a1c16e-1558-414e-a7c0-ac2d829d4105"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 790ms/step - accuracy: 0.1618 - loss: 8.0384 - val_accuracy: 0.1526 - val_loss: 9.4425\n","Epoch 2/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 380ms/step - accuracy: 0.1471 - loss: 8.1811 - val_accuracy: 0.1526 - val_loss: 9.2668\n","Epoch 3/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 395ms/step - accuracy: 0.1386 - loss: 8.3209 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 4/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 404ms/step - accuracy: 0.1421 - loss: 8.2413 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 5/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 408ms/step - accuracy: 0.1425 - loss: 8.2497 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 6/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 408ms/step - accuracy: 0.1383 - loss: 8.0517 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 7/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 395ms/step - accuracy: 0.1458 - loss: 8.2998 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 8/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 411ms/step - accuracy: 0.1451 - loss: 8.2397 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 9/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 406ms/step - accuracy: 0.1455 - loss: 8.3378 - val_accuracy: 0.1526 - val_loss: 9.3986\n","Epoch 10/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 399ms/step - accuracy: 0.1452 - loss: 8.2117 - val_accuracy: 0.1526 - val_loss: 9.3986\n"]}],"source":["MODEL_TYPE = \"vit\"\n","vit_model = ViTClassifier()\n","\n","vit_model.compile(\n","    optimizer=keras.optimizers.AdamW(learning_rate=BASE_LR/51200 * BATCH_SIZE, weight_decay=WEIGHT_DECAY),\n","    metrics=[\"accuracy\"],\n","    loss='categorical_crossentropy',\n",")\n","\n","_ = vit_model.fit(train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS)"]},{"cell_type":"code","source":["_, vit_acc = vit_model.evaluate(val_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUOXvzOzbLit","executionInfo":{"status":"ok","timestamp":1752213434127,"user_tz":-540,"elapsed":1271,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"outputId":"307d9fe8-0526-4ad5-8439-8ad61d5bca1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.1570 - loss: 9.4360\n"]}]},{"cell_type":"markdown","source":["# MobileViT: A mobile-friendly Transformer-based model for image classification\n","\n","**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n","\n","**Description:** MobileViT for image classification with combined benefits of convolutions and Transformers."],"metadata":{"id":"WLUVb2eYGFxN"}},{"cell_type":"code","source":["# Values are from table 4.\n","patch_size = 4  # 2x2, for the Transformer blocks.\n","image_size = 256\n","expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."],"metadata":{"id":"X5CKs9-4CUBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","auto = tf.data.AUTOTUNE\n","resize_bigger = 280\n","num_classes = 5\n","\n","\n","def preprocess_dataset(is_training=True):\n","    def _pp(image, label):\n","        if is_training:\n","            # Resize to a bigger spatial resolution and take the random\n","            # crops.\n","            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n","            image = tf.image.random_crop(image, (image_size, image_size, 3))\n","            image = tf.image.random_flip_left_right(image)\n","        else:\n","            image = tf.image.resize(image, (image_size, image_size))\n","        label = tf.one_hot(label, depth=num_classes)\n","        return image, label\n","\n","    return _pp\n","\n","\n","def prepare_dataset(dataset, is_training=True):\n","    if is_training:\n","        dataset = dataset.shuffle(batch_size * 10)\n","    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)\n","    return dataset.batch(batch_size).prefetch(auto)\n","\n","train_dataset, val_dataset = tfds.load(\n","    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n",")\n","\n","num_train = train_dataset.cardinality()\n","num_val = val_dataset.cardinality()\n","print(f\"Number of training examples: {num_train}\")\n","print(f\"Number of validation examples: {num_val}\")\n","\n","train_dataset = prepare_dataset(train_dataset, is_training=True)\n","val_dataset = prepare_dataset(val_dataset, is_training=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdN-yJbtFmy5","executionInfo":{"status":"ok","timestamp":1752213922616,"user_tz":-540,"elapsed":747,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"outputId":"c278aea2-7494-49fa-e571-493adcac264c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 3303\n","Number of validation examples: 367\n"]}]},{"cell_type":"markdown","source":["## MobileViT utilities\n","\n","The MobileViT architecture is comprised of the following blocks:\n","\n","* Strided 3x3 convolutions that process the input image.\n","* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n","downsampling the resolution of the intermediate feature maps.\n","* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n","presented in the figure below (taken from the\n","[original paper](https://arxiv.org/abs/2110.02178)):\n","\n","\n","![](https://i.imgur.com/mANnhI7.png)"],"metadata":{"id":"KdNloq6BGJSN"}},{"cell_type":"code","source":["def conv_block(x, filters=16, kernel_size=3, strides=2):\n","    conv_layer = layers.Conv2D(\n","        filters,\n","        kernel_size,\n","        strides=strides,\n","        activation=keras.activations.swish,\n","        padding=\"same\",\n","    )\n","    return conv_layer(x)\n","\n","\n","# Reference: https://github.com/keras-team/keras/blob/e3858739d178fe16a0c77ce7fab88b0be6dbbdc7/keras/applications/imagenet_utils.py#L413C17-L435\n","\n","\n","def correct_pad(inputs, kernel_size):\n","    img_dim = 2 if backend.image_data_format() == \"channels_first\" else 1\n","    input_size = inputs.shape[img_dim : (img_dim + 2)]\n","    if isinstance(kernel_size, int):\n","        kernel_size = (kernel_size, kernel_size)\n","    if input_size[0] is None:\n","        adjust = (1, 1)\n","    else:\n","        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n","    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n","    return (\n","        (correct[0] - adjust[0], correct[0]),\n","        (correct[1] - adjust[1], correct[1]),\n","    )\n","\n","\n","# Reference: https://git.io/JKgtC\n","\n","\n","def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n","    # 1x1 Conv\n","    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n","    m = layers.BatchNormalization()(m)\n","    m = keras.activations.swish(m)\n","\n","    # 3x3 DepthwiseConv2D\n","    if strides == 2:\n","        m = layers.ZeroPadding2D(padding=correct_pad(m, 3))(m)\n","    m = layers.DepthwiseConv2D(\n","        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n","    )(m)\n","    m = layers.BatchNormalization()(m)\n","    m = keras.activations.swish(m)\n","\n","    # 1x1 Conv\n","    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n","    m = layers.BatchNormalization()(m)\n","\n","    # Skip Connection\n","    if keras.ops.equal(x.shape[-1], output_channels) and strides == 1:\n","        return layers.Add()([m, x])\n","    return m\n","\n","\n","# Reference:\n","# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n","\n","\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=keras.activations.swish)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, x])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(\n","            x3,\n","            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n","            dropout_rate=0.1,\n","        )\n","        # Skip connection 2.\n","        x = layers.Add()([x3, x2])\n","\n","    return x\n","\n","\n","def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n","    # Local projection with convolutions.\n","    local_features = conv_block(x, filters=projection_dim, strides=strides)\n","    local_features = conv_block(\n","        local_features, filters=projection_dim, kernel_size=1, strides=strides\n","    )\n","\n","    # Unfold into patches and then pass through Transformers.\n","    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n","    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n","        local_features\n","    )\n","    global_features = transformer_block(\n","        non_overlapping_patches, num_blocks, projection_dim\n","    )\n","\n","    # Fold into conv-like feature-maps.\n","    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n","        global_features\n","    )\n","\n","    # Apply point-wise conv -> concatenate with the input features.\n","    folded_feature_map = conv_block(\n","        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n","    )\n","    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n","\n","    # Fuse the local and global features using a convoluion layer.\n","    local_global_features = conv_block(\n","        local_global_features, filters=projection_dim, strides=strides\n","    )\n","\n","    return local_global_features\n"],"metadata":{"id":"ZYEMe-OLCWq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_mobilevit(num_classes=5):\n","    inputs = keras.Input((image_size, image_size, 3))\n","    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n","\n","    # Initial conv-stem -> MV2 block.\n","\n","\n","    # Downsampling with MV2 block.\n","\n","\n","    # First MV2 -> MobileViT block.\n","\n","\n","    # Second MV2 -> MobileViT block.\n","\n","\n","    # Third MV2 -> MobileViT block.\n","\n","\n","    # Classification head.\n","\n","    return keras.Model(inputs, outputs)\n","\n","\n","mobilevit_xxs = create_mobilevit()"],"metadata":{"id":"5w2hqfA4CWtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mobilevit_xxs.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate=0.002),\n","    metrics=[\"accuracy\"],\n","    loss='categorical_crossentropy',\n",")\n","\n","_ = mobilevit_xxs.fit(train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DesF2-WWD83W","executionInfo":{"status":"ok","timestamp":1752214518407,"user_tz":-540,"elapsed":591723,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"outputId":"c9d30ef6-5217-4057-f3ac-83ae39a29f4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 2s/step - accuracy: 0.4243 - loss: 1.3499 - val_accuracy: 0.2316 - val_loss: 1.9340\n","Epoch 2/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 421ms/step - accuracy: 0.5983 - loss: 1.0022 - val_accuracy: 0.1907 - val_loss: 1.8497\n","Epoch 3/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 419ms/step - accuracy: 0.6648 - loss: 0.8749 - val_accuracy: 0.1907 - val_loss: 1.9939\n","Epoch 4/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 423ms/step - accuracy: 0.7079 - loss: 0.7831 - val_accuracy: 0.1907 - val_loss: 1.7146\n","Epoch 5/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 423ms/step - accuracy: 0.7211 - loss: 0.7530 - val_accuracy: 0.1907 - val_loss: 2.4908\n","Epoch 6/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 423ms/step - accuracy: 0.7297 - loss: 0.6925 - val_accuracy: 0.1907 - val_loss: 4.1751\n","Epoch 7/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 424ms/step - accuracy: 0.7484 - loss: 0.6565 - val_accuracy: 0.1907 - val_loss: 2.4273\n","Epoch 8/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 409ms/step - accuracy: 0.7736 - loss: 0.6251 - val_accuracy: 0.2098 - val_loss: 2.3636\n","Epoch 9/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 410ms/step - accuracy: 0.7732 - loss: 0.5941 - val_accuracy: 0.3243 - val_loss: 1.9347\n","Epoch 10/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 416ms/step - accuracy: 0.7605 - loss: 0.6195 - val_accuracy: 0.4796 - val_loss: 1.2090\n"]}]},{"cell_type":"code","source":["_, mobvit_acc = mobilevit_xxs.evaluate(val_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SV0jeibLI5mR","executionInfo":{"status":"ok","timestamp":1752214798899,"user_tz":-540,"elapsed":1321,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"outputId":"0e09bf44-2fa5-4dad-ca61-039176da4351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.4640 - loss: 1.2604\n"]}]},{"cell_type":"markdown","source":["## DeiT"],"metadata":{"id":"YfcwjEaHdpDw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx-FKFardgxc"},"outputs":[],"source":["# Model\n","IMAGE_SIZE = 224\n","PATCH_SIZE = 16\n","NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","LAYER_NORM_EPS = 1e-6\n","PROJECTION_DIM = 192\n","NUM_HEADS = 3\n","NUM_LAYERS = 12\n","MLP_UNITS = [\n","    PROJECTION_DIM * 4,\n","    PROJECTION_DIM,\n","]\n","DROPOUT_RATE = 0.0\n","\n","# Training\n","NUM_EPOCHS = 10\n","BASE_LR = 0.0005\n","WEIGHT_DECAY = 0.0001\n","\n","# Data\n","BATCH_SIZE = 64\n","AUTO = tf.data.AUTOTUNE\n","NUM_CLASSES = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17668,"status":"ok","timestamp":1752213034889,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"},"user_tz":-540},"outputId":"73de3dd4-100f-485e-dd38-1d93a8e60a09","id":"BzUpQu3bdj8Y"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Variant folder /root/tensorflow_datasets/tf_flowers/3.0.1 has no dataset_info.json\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n","Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n","Number of training examples: 3303\n","Number of validation examples: 367\n"]}],"source":["def preprocess_dataset(is_training=True):\n","    def fn(image, label):\n","        if is_training:\n","            # Resize to a bigger spatial resolution and take the random crops.\n","            image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","            image = tf.image.random_crop(image, (IMAGE_SIZE, IMAGE_SIZE, 3))\n","            image = tf.image.random_flip_left_right(image)\n","            image = image / 255.0\n","        else:\n","            image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","            image = image / 255.0\n","        label = tf.one_hot(label, depth=NUM_CLASSES)\n","        return image, label\n","\n","    return fn\n","\n","\n","def prepare_dataset(dataset, is_training=True):\n","    if is_training:\n","        dataset = dataset.shuffle(BATCH_SIZE * 10)\n","    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=AUTO)\n","    return dataset.batch(BATCH_SIZE).prefetch(AUTO)\n","\n","\n","train_dataset, val_dataset = tfds.load(\n","    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\",], as_supervised=True\n",")\n","num_train = train_dataset.cardinality()\n","num_val = val_dataset.cardinality()\n","\n","print(f\"Number of training examples: {num_train}\")\n","print(f\"Number of validation examples: {num_val}\")\n","\n","\n","train_dataset = prepare_dataset(train_dataset, is_training=True)\n","val_dataset = prepare_dataset(val_dataset, is_training=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEuMXc_diEg7"},"outputs":[],"source":["class ViTDistilled(ViTClassifier):\n","    def __init__(self, regular_training=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.num_tokens = 2\n","        self.regular_training = regular_training\n","\n","        # CLS and distillation tokens, positional embedding.\n","        init_value = tf.zeros((1, 1, PROJECTION_DIM))\n","        self.dist_token = tf.Variable(init_value, name=\"dist_token\")\n","        self.positional_embedding = tf.Variable(\n","            tf.zeros(\n","                (\n","                    1,\n","                    NUM_PATCHES + self.num_tokens,\n","                    PROJECTION_DIM,\n","                )\n","            ),\n","            name=\"pos_embedding\",\n","        )\n","\n","        # Head layers.\n","        self.head = layers.Dense(\n","            NUM_CLASSES,\n","            name=\"classification_head\",\n","        )\n","        self.head_dist = layers.Dense(\n","            NUM_CLASSES,\n","            name=\"distillation_head\",\n","        )\n","\n","    def call(self, inputs, training=True):\n","        n = tf.shape(inputs)[0]\n","\n","        # Create patches and project the patches.\n","        projected_patches = self.projection(inputs)\n","\n","        # Append the tokens.\n","        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n","        dist_token = tf.tile(self.dist_token, (n, 1, 1))\n","        cls_token = tf.cast(cls_token, projected_patches.dtype)\n","        dist_token = tf.cast(dist_token, projected_patches.dtype)\n","        projected_patches = tf.concat(\n","            [cls_token, dist_token, projected_patches], axis=1\n","        )\n","\n","        # Add positional embeddings to the projected patches.\n","        encoded_patches = (\n","            self.positional_embedding + projected_patches\n","        )\n","        encoded_patches = self.dropout(encoded_patches)\n","\n","        # Iterate over the number of layers and stack up blocks of Transformer.\n","        for transformer_module in self.transformer_blocks:\n","            # Add a Transformer block.\n","            encoded_patches = transformer_module(encoded_patches)\n","\n","        # Final layer normalization.\n","        representation = self.layer_norm(encoded_patches)\n","\n","        # Classification heads.\n","        x, x_dist = (\n","            self.head(representation[:, 0]),\n","            self.head_dist(representation[:, 1]),\n","        )\n","\n","        if not training or self.regular_training:\n","            # During standard train / finetune, inference average the classifier predictions.\n","            return (x + x_dist) / 2\n","\n","        elif training:\n","            # Only return separate classification predictions when training in distilled mode.\n","            return x, x_dist\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02LZ7HS2iFxH"},"outputs":[],"source":["class DeiT(keras.Model):\n","    # Reference:\n","    # https://keras.io/examples/vision/knowledge_distillation/\n","    def __init__(self, student, teacher, **kwargs):\n","        super().__init__(**kwargs)\n","        self.student = student\n","        self.teacher = teacher\n","\n","        self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")\n","        self.dist_loss_tracker = keras.metrics.Mean(name=\"distillation_loss\")\n","\n","    @property\n","    def metrics(self):\n","        metrics = super().metrics\n","        metrics.append(self.student_loss_tracker)\n","        metrics.append(self.dist_loss_tracker)\n","        return metrics\n","\n","    def compile(\n","        self,\n","        optimizer,\n","        metrics,\n","        student_loss_fn,\n","        distillation_loss_fn,\n","    ):\n","        super().compile(optimizer=optimizer, metrics=metrics)\n","        self.student_loss_fn = student_loss_fn\n","        self.distillation_loss_fn = distillation_loss_fn\n","\n","    def train_step(self, data):\n","        # Unpack data.\n","        x, y = data\n","\n","        # Forward pass of teacher\n","        teacher_output = self.teacher(x)\n","\n","        if isinstance(teacher_output, dict):\n","          logits = teacher_output['dense']\n","        else:\n","          logits = teacher_output\n","\n","        teacher_probs = tf.nn.softmax(logits, -1)\n","\n","        with tf.GradientTape() as tape:\n","            # Forward pass of student.\n","            cls_predictions, dist_predictions = self.student(x, training=True)\n","\n","            # Compute losses.\n","            student_loss = self.student_loss_fn(y, cls_predictions)\n","            distillation_loss = self.distillation_loss_fn(\n","                teacher_probs, dist_predictions\n","            )\n","            loss = (student_loss + distillation_loss) / 2\n","\n","        # Compute gradients.\n","        trainable_vars = self.student.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights.\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics configured in `compile()`.\n","        student_predictions = (cls_predictions + dist_predictions) / 2\n","        self.compiled_metrics.update_state(y, student_predictions)\n","        self.dist_loss_tracker.update_state(distillation_loss)\n","        self.student_loss_tracker.update_state(student_loss)\n","\n","        # Return a dict of performance.\n","        results = {m.name: m.result() for m in self.metrics}\n","        return results\n","\n","    def test_step(self, data):\n","        # Unpack the data.\n","        x, y = data\n","\n","        # Compute predictions.\n","        y_prediction = self.student(x, training=False)\n","\n","        # Calculate the loss.\n","        student_loss = self.student_loss_fn(y, y_prediction)\n","\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_prediction)\n","        self.student_loss_tracker.update_state(student_loss)\n","\n","        # Return a dict of performance.\n","        results = {m.name: m.result() for m in self.metrics}\n","        return results\n","\n","    def call(self, inputs):\n","        return self.student(inputs, training=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bO5LTq2SiafY"},"outputs":[],"source":["!wget -q https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip\n","!unzip -q bit_teacher_flowers.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYw0SNbGieYC","executionInfo":{"status":"ok","timestamp":1751865916313,"user_tz":-540,"elapsed":1355277,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c7bada4-f123-43a9-ef90-466f3858bef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'vi_t_distilled_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:642: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.2173 - distillation_loss: 0.2560 - loss: 0.2420 - student_loss: 0.2620 - val_distillation_loss: 0.5151 - val_loss: 0.5151 - val_student_loss: 0.5256\n","Epoch 2/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.2177 - distillation_loss: 0.5080 - loss: 0.5007 - student_loss: 0.5097 - val_distillation_loss: 0.4887 - val_loss: 0.4887 - val_student_loss: 0.4987\n","Epoch 3/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.2386 - distillation_loss: 0.4874 - loss: 0.4800 - student_loss: 0.4892 - val_distillation_loss: 0.4641 - val_loss: 0.4641 - val_student_loss: 0.4750\n","Epoch 4/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.2320 - distillation_loss: 0.4594 - loss: 0.4518 - student_loss: 0.4612 - val_distillation_loss: 0.4359 - val_loss: 0.4359 - val_student_loss: 0.4462\n","Epoch 5/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.2455 - distillation_loss: 0.4235 - loss: 0.4156 - student_loss: 0.4250 - val_distillation_loss: 0.3892 - val_loss: 0.3892 - val_student_loss: 0.4003\n","Epoch 6/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.2320 - distillation_loss: 0.3922 - loss: 0.3840 - student_loss: 0.3937 - val_distillation_loss: 0.3510 - val_loss: 0.3510 - val_student_loss: 0.3618\n","Epoch 7/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.2487 - distillation_loss: 0.3510 - loss: 0.3425 - student_loss: 0.3525 - val_distillation_loss: 0.3183 - val_loss: 0.3183 - val_student_loss: 0.3296\n","Epoch 8/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.2523 - distillation_loss: 0.3133 - loss: 0.3045 - student_loss: 0.3148 - val_distillation_loss: 0.2766 - val_loss: 0.2766 - val_student_loss: 0.2879\n","Epoch 9/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.2551 - distillation_loss: 0.2706 - loss: 0.2615 - student_loss: 0.2719 - val_distillation_loss: 0.2401 - val_loss: 0.2401 - val_student_loss: 0.2516\n","Epoch 10/10\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.3494 - distillation_loss: 0.2474 - loss: 0.2381 - student_loss: 0.2477 - val_distillation_loss: 0.1781 - val_loss: 0.1781 - val_student_loss: 0.1903\n"]}],"source":["MODEL_TYPE = \"deit\"\n","# bit_teacher_flowers = keras.models.load_model(\"bit_teacher_flowers\")\n","bit_teacher_flowers = keras.layers.TFSMLayer(\"bit_teacher_flowers\", call_endpoint=\"serving_default\")\n","\n","deit_tiny = ViTDistilled()\n","deit_distiller = DeiT(student=deit_tiny, teacher=bit_teacher_flowers)\n","\n","lr_scaled = (0.1 / 512) * BATCH_SIZE\n","deit_distiller.compile(\n","    optimizer=keras.optimizers.AdamW(weight_decay=WEIGHT_DECAY, learning_rate=lr_scaled),\n","    metrics=[\"accuracy\"],\n","    student_loss_fn=keras.losses.CategoricalCrossentropy(\n","        from_logits=True, label_smoothing=0.1\n","    ),\n","    distillation_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",")\n","_ = deit_distiller.fit(train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE1nEzqU4d_7","executionInfo":{"status":"ok","timestamp":1751866337177,"user_tz":-540,"elapsed":1268,"user":{"displayName":"­정휘수 / 학생 / 컴퓨터공학부","userId":"15095837646754011039"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ededeb1-0788-43cf-dbb0-880bb4910530"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.3070 - distillation_loss: 0.1785 - loss: 0.1785 - student_loss: 0.1901\n"]}],"source":["_ = deit_distiller.evaluate(val_dataset)"]},{"cell_type":"code","source":[],"metadata":{"id":"_yMFtqDd-A_U"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}